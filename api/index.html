<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://github.com/StatMixedML/XGBoostLSS/api/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>API references - XGBoostLSS</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "API references";
        var mkdocs_page_input_path = "api.md";
        var mkdocs_page_url = "/StatMixedML/XGBoostLSS/api/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> XGBoostLSS
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../dgbm/">Distributional Modelling</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../distributions/">Available Distributions</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Examples</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/Gaussian_Regression/">Basic Walkthrough - Gaussian Regression</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/Dirichlet_Regression/">Dirichlet Regression</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/Expectile_Regression/">Expectile Regression</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/Gamma_Regression_CaliforniaHousing/">Gamma Regression (California Housing Data)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/GaussianMixture_Regression_CaliforniaHousing/">Gausssian-Mixture Regression (California Housing Data)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/How_To_Select_A_Univariate_Distribution/">How to Select a Univariate Distribution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/How_To_Select_A_Multivariate_Distribution/">How to Select a Multivariate Distribution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/MVN_Cholesky/">Multivariate Gaussian Regression (Cholesky Decomposition)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/MVN_LowRank/">Multivariate Gaussian Regression (Low-Rank Approximation)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/MVT_Cholesky/">Multivariate Student-T Regression (Cholesky Decomposition)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/SplineFlow_Regression/">Spline Flow Regression</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../examples/ZAGamma_Regression/">Zero-Adjusted Gamma Regression</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">API Docs</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">API references</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#xgboostlss">xgboostlss</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#xgboostlss.datasets">datasets</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.datasets.data_loader">data_loader</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.datasets.data_loader.load_articlake_data">load_articlake_data</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.datasets.data_loader.load_simulated_gaussian_data">load_simulated_gaussian_data</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.datasets.data_loader.load_simulated_multivariate_gaussian_data">load_simulated_multivariate_gaussian_data</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.datasets.data_loader.load_simulated_multivariate_studentT_data">load_simulated_multivariate_studentT_data</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.datasets.data_loader.load_simulated_studentT_data">load_simulated_studentT_data</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#xgboostlss.distributions">distributions</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Beta">Beta</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.Beta">Beta</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Beta.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Cauchy">Cauchy</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.Cauchy">Cauchy</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Cauchy.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet">Dirichlet</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.Dirichlet">Dirichlet</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Dirichlet.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Expectile">Expectile</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.Expectile">Expectile</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.Expectile_Torch">Expectile_Torch</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.expectile_norm">expectile_norm</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.expectile_pnorm">expectile_pnorm</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Expectile.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Gamma">Gamma</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.Gamma">Gamma</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gamma.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Gaussian">Gaussian</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.Gaussian">Gaussian</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gaussian.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Gumbel">Gumbel</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.Gumbel">Gumbel</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Gumbel.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Laplace">Laplace</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.Laplace">Laplace</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Laplace.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.LogNormal">LogNormal</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.LogNormal">LogNormal</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.LogNormal.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Logistic">Logistic</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.Logistic">Logistic</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Logistic.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.MVN">MVN</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.MVN">MVN</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa">MVN_LoRa</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.MVN_LoRa">MVN_LoRa</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVN_LoRa.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.MVT">MVT</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.MVT">MVT</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.MVT.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Mixture">Mixture</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.Mixture">Mixture</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Mixture.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial">NegativeBinomial</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.NegativeBinomial">NegativeBinomial</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.NegativeBinomial.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Poisson">Poisson</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.Poisson">Poisson</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Poisson.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.SplineFlow">SplineFlow</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.SplineFlow.SplineFlow">SplineFlow</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.StudentT">StudentT</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.StudentT">StudentT</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.StudentT.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.Weibull">Weibull</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.Weibull">Weibull</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.Weibull.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.ZABeta">ZABeta</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.ZABeta">ZABeta</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZABeta.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma">ZAGamma</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.ZAGamma">ZAGamma</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZAGamma.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.ZALN">ZALN</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.ZALN">ZALN</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZALN.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.ZINB">ZINB</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.ZINB">ZINB</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZINB.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson">ZIPoisson</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.ZIPoisson">ZIPoisson</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.exp_fn">exp_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.exp_fn_df">exp_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.gumbel_softmax_fn">gumbel_softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.identity_fn">identity_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.nan_to_num">nan_to_num</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.relu_fn">relu_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.relu_fn_df">relu_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.sigmoid_fn">sigmoid_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.softmax_fn">softmax_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.softplus_fn">softplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.softplus_fn_df">softplus_fn_df</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.squareplus_fn">squareplus_fn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.ZIPoisson.squareplus_fn_df">squareplus_fn_df</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.distribution_utils">distribution_utils</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.flow_utils">flow_utils</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.flow_utils.NormalizingFlowClass">NormalizingFlowClass</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.mixture_distribution_utils">mixture_distribution_utils</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass">MixtureDistributionClass</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.mixture_distribution_utils.get_component_distributions">get_component_distributions</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.multivariate_distribution_utils">multivariate_distribution_utils</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass">Multivariate_DistributionClass</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.distributions.zero_inflated">zero_inflated</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.zero_inflated.ZeroAdjustedBeta">ZeroAdjustedBeta</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.zero_inflated.ZeroAdjustedGamma">ZeroAdjustedGamma</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.zero_inflated.ZeroAdjustedLogNormal">ZeroAdjustedLogNormal</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.zero_inflated.ZeroInflatedNegativeBinomial">ZeroInflatedNegativeBinomial</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.distributions.zero_inflated.ZeroInflatedPoisson">ZeroInflatedPoisson</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#xgboostlss.model">model</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS">XGBoostLSS</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS--parameters">Parameters</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.adjust_labels">adjust_labels</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.cv">cv</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.expectile_plot">expectile_plot</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.hyper_opt">hyper_opt</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.load_model">load_model</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.plot">plot</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.predict">predict</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.save_model">save_model</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.set_base_margin">set_base_margin</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.set_eval_margin">set_eval_margin</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.set_params_adj">set_params_adj</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.XGBoostLSS.train">train</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.exp_fn">exp_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.exp_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.exp_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.exp_fn_df">exp_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.exp_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.exp_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.gumbel_softmax_fn">gumbel_softmax_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.gumbel_softmax_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.gumbel_softmax_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.identity_fn">identity_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.identity_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.identity_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.nan_to_num">nan_to_num</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.nan_to_num--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.nan_to_num--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.relu_fn">relu_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.relu_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.relu_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.relu_fn_df">relu_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.relu_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.relu_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.sigmoid_fn">sigmoid_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.sigmoid_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.sigmoid_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.softmax_fn">softmax_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.softmax_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.softmax_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.softplus_fn">softplus_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.softplus_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.softplus_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.softplus_fn_df">softplus_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.softplus_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.softplus_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.squareplus_fn">squareplus_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.squareplus_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.squareplus_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.model.squareplus_fn_df">squareplus_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.squareplus_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.model.squareplus_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#xgboostlss.utils">utils</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.exp_fn">exp_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.exp_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.exp_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.exp_fn_df">exp_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.exp_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.exp_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.gumbel_softmax_fn">gumbel_softmax_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.gumbel_softmax_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.gumbel_softmax_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.identity_fn">identity_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.identity_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.identity_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.nan_to_num">nan_to_num</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.nan_to_num--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.nan_to_num--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.relu_fn">relu_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.relu_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.relu_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.relu_fn_df">relu_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.relu_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.relu_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.sigmoid_fn">sigmoid_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.sigmoid_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.sigmoid_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.softmax_fn">softmax_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.softmax_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.softmax_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.softplus_fn">softplus_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.softplus_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.softplus_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.softplus_fn_df">softplus_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.softplus_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.softplus_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.squareplus_fn">squareplus_fn</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.squareplus_fn--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.squareplus_fn--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#xgboostlss.utils.squareplus_fn_df">squareplus_fn_df</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.squareplus_fn_df--arguments">Arguments</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#xgboostlss.utils.squareplus_fn_df--returns">Returns</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">XGBoostLSS</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">API Docs</li>
      <li class="breadcrumb-item active">API references</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/StatMixedML/XGBoostLSS/edit/master/docs/api.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="api-references">API references</h1>


<div class="doc doc-object doc-module">



<a id="xgboostlss"></a>
    <div class="doc doc-contents first">

        <p>XGBoostLSS - An extension of XGBoost to probabilistic forecasting</p>










<div class="doc doc-children">











<div class="doc doc-object doc-module">



<h2 id="xgboostlss.datasets" class="doc doc-heading">
            <code>datasets</code>


</h2>

    <div class="doc doc-contents ">

        <p>XGBoostLSS - An extension of XGBoost to probabilistic forecasting</p>










<div class="doc doc-children">











<div class="doc doc-object doc-module">



<h3 id="xgboostlss.datasets.data_loader" class="doc doc-heading">
            <code>data_loader</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="xgboostlss.datasets.data_loader.load_articlake_data" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_articlake_data</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Returns the arctic lake sediment data: sand, silt, clay compositions of 39 sediment samples at different water
depths in an Arctic lake.</p>


<details class="contains-the-following-columns" open>
  <summary>Contains the following columns</summary>
  <p>sand: numeric
    Vector of percentages of sand.
silt: numeric
    Vector of percentages of silt.
clay: numeric
    Vector of percentages of clay
depth: numeric
    Vector of water depths (meters) in which samples are taken.</p>
</details>        <h6 id="xgboostlss.datasets.data_loader.load_articlake_data--source">Source</h6>
<p>https://rdrr.io/rforge/DirichletReg/</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/datasets/data_loader.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_articlake_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the arctic lake sediment data: sand, silt, clay compositions of 39 sediment samples at different water</span>
<span class="sd">    depths in an Arctic lake.</span>

<span class="sd">    Contains the following columns:</span>
<span class="sd">        sand: numeric</span>
<span class="sd">            Vector of percentages of sand.</span>
<span class="sd">        silt: numeric</span>
<span class="sd">            Vector of percentages of silt.</span>
<span class="sd">        clay: numeric</span>
<span class="sd">            Vector of percentages of clay</span>
<span class="sd">        depth: numeric</span>
<span class="sd">            Vector of water depths (meters) in which samples are taken.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://rdrr.io/rforge/DirichletReg/</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">data_path</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">resource_stream</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;arcticlake.csv&quot;</span><span class="p">)</span>
    <span class="n">data_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.datasets.data_loader.load_simulated_gaussian_data" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_simulated_gaussian_data</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Returns train/test dataframe of a simulated example.</p>


<details class="contains-the-following-columns" open>
  <summary>Contains the following columns</summary>
  <p>y              int64: response
x              int64: x-feature
X1:X10         int64: random noise features</p>
</details>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/datasets/data_loader.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_simulated_gaussian_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns train/test dataframe of a simulated example.</span>

<span class="sd">    Contains the following columns:</span>
<span class="sd">        y              int64: response</span>
<span class="sd">        x              int64: x-feature</span>
<span class="sd">        X1:X10         int64: random noise features</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">train_path</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">resource_stream</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;gaussian_train_sim.csv&quot;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">train_path</span><span class="p">)</span>

    <span class="n">test_path</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">resource_stream</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;gaussian_test_sim.csv&quot;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">test_path</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.datasets.data_loader.load_simulated_multivariate_gaussian_data" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_simulated_multivariate_gaussian_data</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Returns train/test dataframe of a simulated example.</p>


<details class="contains-the-following-columns" open>
  <summary>Contains the following columns</summary>
  <p>y              int64: response
x              int64: x-feature</p>
</details>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/datasets/data_loader.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_simulated_multivariate_gaussian_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns train/test dataframe of a simulated example.</span>

<span class="sd">    Contains the following columns:</span>
<span class="sd">        y              int64: response</span>
<span class="sd">        x              int64: x-feature</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">data_path</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">resource_stream</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;sim_triv_gaussian.csv&quot;</span><span class="p">)</span>
    <span class="n">data_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.datasets.data_loader.load_simulated_multivariate_studentT_data" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_simulated_multivariate_studentT_data</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Returns train/test dataframe of a simulated example.</p>


<details class="contains-the-following-columns" open>
  <summary>Contains the following columns</summary>
  <p>y              int64: response
x              int64: x-feature</p>
</details>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/datasets/data_loader.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_simulated_multivariate_studentT_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns train/test dataframe of a simulated example.</span>

<span class="sd">    Contains the following columns:</span>
<span class="sd">        y              int64: response</span>
<span class="sd">        x              int64: x-feature</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">data_path</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">resource_stream</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;sim_triv_studentT.csv&quot;</span><span class="p">)</span>
    <span class="n">data_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">data_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.datasets.data_loader.load_simulated_studentT_data" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_simulated_studentT_data</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Returns train/test dataframe of a simulated example.</p>


<details class="contains-the-following-columns" open>
  <summary>Contains the following columns</summary>
  <p>y              int64: response
x              int64: x-feature
X1:X10         int64: random noise features</p>
</details>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/datasets/data_loader.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">load_simulated_studentT_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns train/test dataframe of a simulated example.</span>

<span class="sd">    Contains the following columns:</span>
<span class="sd">        y              int64: response</span>
<span class="sd">        x              int64: x-feature</span>
<span class="sd">        X1:X10         int64: random noise features</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">train_path</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">resource_stream</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;studentT_train_sim.csv&quot;</span><span class="p">)</span>
    <span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">train_path</span><span class="p">)</span>

    <span class="n">test_path</span> <span class="o">=</span> <span class="n">pkg_resources</span><span class="o">.</span><span class="n">resource_stream</span><span class="p">(</span><span class="vm">__name__</span><span class="p">,</span> <span class="s2">&quot;studentT_test_sim.csv&quot;</span><span class="p">)</span>
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">test_path</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="xgboostlss.distributions" class="doc doc-heading">
            <code>distributions</code>


</h2>

    <div class="doc doc-contents ">

        <p>XGBoostLSS - An extension of XGBoost to probabilistic forecasting</p>










<div class="doc doc-children">











<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Beta" class="doc doc-heading">
            <code>Beta</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Beta.Beta" class="doc doc-heading">
            <code>Beta</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Beta distribution class.</p>
<h6 id="xgboostlss.distributions.Beta.Beta--distributional-parameters">Distributional Parameters</h6>
<p>concentration1: torch.Tensor
    1st concentration parameter of the distribution (often referred to as alpha).
concentration0: torch.Tensor
    2nd concentration parameter of the distribution (often referred to as beta).</p>
<h6 id="xgboostlss.distributions.Beta.Beta--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#beta</p>
<h6 id="xgboostlss.distributions.Beta.Beta--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Beta.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Beta</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Beta distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    concentration1: torch.Tensor</span>
<span class="sd">        1st concentration parameter of the distribution (often referred to as alpha).</span>
<span class="sd">    concentration0: torch.Tensor</span>
<span class="sd">        2nd concentration parameter of the distribution (often referred to as beta).</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#beta</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Beta_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;concentration1&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;concentration0&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Beta.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Beta.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Beta.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Beta.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Beta.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Beta.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Beta.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Beta.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Beta.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Beta.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Beta.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Beta.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Beta.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Beta.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Beta.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Beta.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Cauchy" class="doc doc-heading">
            <code>Cauchy</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Cauchy.Cauchy" class="doc doc-heading">
            <code>Cauchy</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Cauchy distribution class.</p>
<h6 id="xgboostlss.distributions.Cauchy.Cauchy--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mode or median of the distribution.
scale: torch.Tensor
    Half width at half maximum.</p>
<h6 id="xgboostlss.distributions.Cauchy.Cauchy--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#cauchy</p>
<h6 id="xgboostlss.distributions.Cauchy.Cauchy--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Cauchy.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Cauchy</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cauchy distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mode or median of the distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Half width at half maximum.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#cauchy</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Cauchy_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Cauchy.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Cauchy.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Cauchy.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Cauchy.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Cauchy.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Cauchy.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Cauchy.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Cauchy.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Cauchy.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Cauchy.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Cauchy.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Cauchy.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Cauchy.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Cauchy.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Cauchy.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Cauchy.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Dirichlet" class="doc doc-heading">
            <code>Dirichlet</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Dirichlet.Dirichlet" class="doc doc-heading">
            <code>Dirichlet</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="Multivariate_DistributionClass (xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass)" href="#xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass">Multivariate_DistributionClass</a></code></p>



        <p>Dirichlet distribution class.</p>
<p>The Dirichlet distribution is commonly used for modelling non-negative compositional data, i.e., data that consist
of sub-sets that are fractions of some total. Compositional data are typically represented as proportions or
percentages summing to 1, so that the Dirichlet extends the univariate beta-distribution to the multivariate case.</p>
<h6 id="xgboostlss.distributions.Dirichlet.Dirichlet--distributional-parameters">Distributional Parameters</h6>
<p>concentration: torch.Tensor
    Concentration parameter of the distribution (often referred to as alpha).</p>
<h6 id="xgboostlss.distributions.Dirichlet.Dirichlet--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#dirichlet</p>
<h6 id="xgboostlss.distributions.Dirichlet.Dirichlet--parameters">Parameters</h6>
<p>D: int
    Number of targets.
stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential), "softplus" (softplus) or "relu" (rectified linear unit).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Dirichlet.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Dirichlet</span><span class="p">(</span><span class="n">Multivariate_DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dirichlet distribution class.</span>

<span class="sd">    The Dirichlet distribution is commonly used for modelling non-negative compositional data, i.e., data that consist</span>
<span class="sd">    of sub-sets that are fractions of some total. Compositional data are typically represented as proportions or</span>
<span class="sd">    percentages summing to 1, so that the Dirichlet extends the univariate beta-distribution to the multivariate case.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    concentration: torch.Tensor</span>
<span class="sd">        Concentration parameter of the distribution (often referred to as alpha).</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#dirichlet</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    D: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential), &quot;softplus&quot; (softplus) or &quot;relu&quot; (rectified linear unit).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">D</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>
        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid dimensionality type. Please choose an integer for D.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">D</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid dimensionality. Please choose D &gt;= 2.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select from &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39; or &#39;relu.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Dirichlet_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="o">.</span><span class="n">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">response_fn</span><span class="o">=</span><span class="n">response_fn</span><span class="p">)</span>
        <span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;concentration&quot;</span><span class="p">]</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="n">distribution_arg_names</span><span class="p">,</span>
                         <span class="n">n_targets</span><span class="o">=</span><span class="n">D</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">param_transform</span><span class="o">=</span><span class="n">Dirichlet</span><span class="o">.</span><span class="n">param_transform</span><span class="p">,</span>
                         <span class="n">get_dist_params</span><span class="o">=</span><span class="n">Dirichlet</span><span class="o">.</span><span class="n">get_dist_params</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                          <span class="n">response_fn</span><span class="p">:</span> <span class="n">Callable</span>
                          <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that transforms the distributional parameters to the desired scale.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        response_fn: Callable</span>
<span class="sd">            Response function.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        param_dict: Dict</span>
<span class="sd">            Dictionary of distributional parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Concentration</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;concentration_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">response_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)}</span>

        <span class="k">return</span> <span class="n">param_dict</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
                        <span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">n_obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that returns a list of parameters for a Dirichlet distribution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        params: List[torch.Tensor]</span>
<span class="sd">            List of distributional parameters.</span>
<span class="sd">        param_dict: Dict</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        rank: Optional[int]</span>
<span class="sd">            Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">        n_obs: int</span>
<span class="sd">            Number of observations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        params: List[torch.Tensor]</span>
<span class="sd">            List of parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Transform Parameters to respective scale</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span>
            <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">dist_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that returns the predicted distributional parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        dist_pred: torch.distributions.Distribution</span>
<span class="sd">            Predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dist_params_df: pd.DataFrame</span>
<span class="sd">            DataFrame with predicted distributional parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Concentration</span>
        <span class="n">dist_params_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">concentration</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">dist_params_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;concentration_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

        <span class="c1"># # Normalize to sum to 1</span>
        <span class="c1"># dist_params_df = dist_params_df.div(dist_params_df.sum(axis=1), axis=0)</span>

        <span class="k">return</span> <span class="n">dist_params_df</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.Dirichlet.Dirichlet.create_param_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">response_fn</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that transforms the distributional parameters to the desired scale.</p>
<h6 id="xgboostlss.distributions.Dirichlet.Dirichlet.create_param_dict--arguments">Arguments</h6>
<p>n_targets: int
    Number of targets.
response_fn: Callable
    Response function.</p>
<h6 id="xgboostlss.distributions.Dirichlet.Dirichlet.create_param_dict--returns">Returns</h6>
<p>param_dict: Dict
    Dictionary of distributional parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/Dirichlet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">response_fn</span><span class="p">:</span> <span class="n">Callable</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function that transforms the distributional parameters to the desired scale.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    response_fn: Callable</span>
<span class="sd">        Response function.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    param_dict: Dict</span>
<span class="sd">        Dictionary of distributional parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Concentration</span>
    <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;concentration_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">response_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)}</span>

    <span class="k">return</span> <span class="n">param_dict</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.Dirichlet.Dirichlet.get_dist_params" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">dist_pred</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns the predicted distributional parameters.</p>
<h6 id="xgboostlss.distributions.Dirichlet.Dirichlet.get_dist_params--arguments">Arguments</h6>
<p>n_targets: int
    Number of targets.
dist_pred: torch.distributions.Distribution
    Predicted distribution.</p>
<h6 id="xgboostlss.distributions.Dirichlet.Dirichlet.get_dist_params--returns">Returns</h6>
<p>dist_params_df: pd.DataFrame
    DataFrame with predicted distributional parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/Dirichlet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">dist_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns the predicted distributional parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    dist_pred: torch.distributions.Distribution</span>
<span class="sd">        Predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dist_params_df: pd.DataFrame</span>
<span class="sd">        DataFrame with predicted distributional parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Concentration</span>
    <span class="n">dist_params_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">concentration</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">dist_params_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;concentration_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

    <span class="c1"># # Normalize to sum to 1</span>
    <span class="c1"># dist_params_df = dist_params_df.div(dist_params_df.sum(axis=1), axis=0)</span>

    <span class="k">return</span> <span class="n">dist_params_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.Dirichlet.Dirichlet.param_transform" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns a list of parameters for a Dirichlet distribution.</p>
<h6 id="xgboostlss.distributions.Dirichlet.Dirichlet.param_transform--arguments">Arguments</h6>
<p>params: List[torch.Tensor]
    List of distributional parameters.
param_dict: Dict
n_targets: int
    Number of targets.
rank: Optional[int]
    Rank of the low-rank form of the covariance matrix.
n_obs: int
    Number of observations.</p>
<h6 id="xgboostlss.distributions.Dirichlet.Dirichlet.param_transform--returns">Returns</h6>
<p>params: List[torch.Tensor]
    List of parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/Dirichlet.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                    <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
                    <span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                    <span class="n">n_obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function that returns a list of parameters for a Dirichlet distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    params: List[torch.Tensor]</span>
<span class="sd">        List of distributional parameters.</span>
<span class="sd">    param_dict: Dict</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    rank: Optional[int]</span>
<span class="sd">        Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">    n_obs: int</span>
<span class="sd">        Number of observations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params: List[torch.Tensor]</span>
<span class="sd">        List of parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Transform Parameters to respective scale</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span>
        <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
    <span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">params</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Dirichlet.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Dirichlet.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Dirichlet.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Dirichlet.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Dirichlet.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Dirichlet.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Dirichlet.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Dirichlet.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Dirichlet.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Dirichlet.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Dirichlet.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Dirichlet.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Dirichlet.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Dirichlet.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Dirichlet.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Dirichlet.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Expectile" class="doc doc-heading">
            <code>Expectile</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Expectile.Expectile" class="doc doc-heading">
            <code>Expectile</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Expectile distribution class.</p>
<h6 id="xgboostlss.distributions.Expectile.Expectile--distributional-parameters">Distributional Parameters</h6>
<p>expectile: List
    List of specified expectiles.</p>
<h6 id="xgboostlss.distributions.Expectile.Expectile--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
expectiles: List
    List of expectiles in increasing order.
penalize_crossing: bool
    Whether to include a penalty term to discourage crossing of expectiles.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Expectile.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Expectile</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Expectile distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    expectile: List</span>
<span class="sd">        List of specified expectiles.</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    expectiles: List</span>
<span class="sd">        List of expectiles in increasing order.</span>
<span class="sd">    penalize_crossing: bool</span>
<span class="sd">        Whether to include a penalty term to discourage crossing of expectiles.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">expectiles</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                 <span class="n">penalize_crossing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">expectiles</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expectiles must be a list.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">([</span><span class="mi">0</span> <span class="o">&lt;</span> <span class="n">expectile</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">expectile</span> <span class="ow">in</span> <span class="n">expectiles</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expectiles must be between 0 and 1.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">penalize_crossing</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;penalize_crossing must be a boolean. Please choose from True or False.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Expectile_Torch</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">expectiles</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">expectile</span> <span class="ow">in</span> <span class="n">expectiles</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;expectile_</span><span class="si">{</span><span class="n">expectile</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">param_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">identity_fn</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                         <span class="n">tau</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">expectiles</span><span class="p">),</span>
                         <span class="n">penalize_crossing</span><span class="o">=</span><span class="n">penalize_crossing</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Expectile.Expectile_Torch" class="doc doc-heading">
            <code>Expectile_Torch</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.distributions.Distribution">Distribution</span></code></p>



        <p>PyTorch implementation of expectiles.</p>
<h6 id="xgboostlss.distributions.Expectile.Expectile_Torch--arguments">Arguments</h6>
<p>expectiles : List[torch.Tensor]
    List of expectiles.
penalize_crossing : bool
    Whether to include a penalty term to discourage crossing of expectiles.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Expectile.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Expectile_Torch</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    PyTorch implementation of expectiles.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    expectiles : List[torch.Tensor]</span>
<span class="sd">        List of expectiles.</span>
<span class="sd">    penalize_crossing : bool</span>
<span class="sd">        Whether to include a penalty term to discourage crossing of expectiles.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">expectiles</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                 <span class="n">penalize_crossing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Expectile_Torch</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expectiles</span> <span class="o">=</span> <span class="n">expectiles</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span> <span class="o">=</span> <span class="n">penalize_crossing</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;Expectile&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tau</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the log of the probability density function evaluated at `value`.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        value : torch.Tensor</span>
<span class="sd">            Response for which log probability is to be calculated.</span>
<span class="sd">        tau : List[torch.Tensor]</span>
<span class="sd">            List of asymmetry parameters.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Log probability of `value`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">penalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Calculate loss</span>
        <span class="n">predt_expectiles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">expectile</span><span class="p">,</span> <span class="n">tau_value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expectiles</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">expectile</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tau_value</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tau_value</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">expectile</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">predt_expectiles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">expectile</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Penalty term to discourage crossing of expectiles</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">:</span>
            <span class="n">predt_expectiles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predt_expectiles</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">penalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                <span class="p">(</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">predt_expectiles</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">penalty</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expectiles</span><span class="p">)</span>

        <span class="k">return</span> <span class="o">-</span><span class="n">loss</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.Expectile.Expectile_Torch.log_prob" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">log_prob</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Returns the log of the probability density function evaluated at <code>value</code>.</p>
<h6 id="xgboostlss.distributions.Expectile.Expectile_Torch.log_prob--arguments">Arguments</h6>
<p>value : torch.Tensor
    Response for which log probability is to be calculated.
tau : List[torch.Tensor]
    List of asymmetry parameters.</p>
<h6 id="xgboostlss.distributions.Expectile.Expectile_Torch.log_prob--returns">Returns</h6>
<p>torch.Tensor
    Log probability of <code>value</code>.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/Expectile.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">tau</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the log of the probability density function evaluated at `value`.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    value : torch.Tensor</span>
<span class="sd">        Response for which log probability is to be calculated.</span>
<span class="sd">    tau : List[torch.Tensor]</span>
<span class="sd">        List of asymmetry parameters.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    torch.Tensor</span>
<span class="sd">        Log probability of `value`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">penalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Calculate loss</span>
    <span class="n">predt_expectiles</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">expectile</span><span class="p">,</span> <span class="n">tau_value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expectiles</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">expectile</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tau_value</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tau_value</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">expectile</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">predt_expectiles</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">expectile</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Penalty term to discourage crossing of expectiles</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">:</span>
        <span class="n">predt_expectiles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predt_expectiles</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">penalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
            <span class="p">(</span><span class="o">~</span><span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">predt_expectiles</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">penalty</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expectiles</span><span class="p">)</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Expectile.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Expectile.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.expectile_norm" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">expectile_norm</span><span class="p">(</span><span class="n">tau</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Calculates expectiles from Normal distribution for given tau values.
For more details and distributions see https://rdrr.io/cran/expectreg/man/enorm.html</p>
<p>Arguments</p>
<hr />
<p>tau : np.ndarray
    Vector of expectiles from the respective distribution.
m : np.ndarray
    Mean of the Normal distribution.
sd : np.ndarray
    Standard deviation of the Normal distribution.</p>
<p>Returns</p>
<hr />
<p>np.ndarray</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/Expectile.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">expectile_norm</span><span class="p">(</span><span class="n">tau</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                   <span class="n">m</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                   <span class="n">sd</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates expectiles from Normal distribution for given tau values.</span>
<span class="sd">    For more details and distributions see https://rdrr.io/cran/expectreg/man/enorm.html</span>

<span class="sd">    Arguments</span>
<span class="sd">    _________</span>
<span class="sd">    tau : np.ndarray</span>
<span class="sd">        Vector of expectiles from the respective distribution.</span>
<span class="sd">    m : np.ndarray</span>
<span class="sd">        Mean of the Normal distribution.</span>
<span class="sd">    sd : np.ndarray</span>
<span class="sd">        Standard deviation of the Normal distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    _______</span>
<span class="sd">    np.ndarray</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tau</span><span class="p">[</span><span class="n">tau</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">tau</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">zz</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">*</span> <span class="n">tau</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">lower</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">tau</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float&quot;</span><span class="p">)</span>
    <span class="n">upper</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">upper</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">tau</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">diff</span> <span class="o">&gt;</span> <span class="mf">1e-10</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">index</span> <span class="o">&lt;</span> <span class="mi">1000</span><span class="p">):</span>
        <span class="n">root</span> <span class="o">=</span> <span class="n">expectile_pnorm</span><span class="p">(</span><span class="n">zz</span><span class="p">)</span> <span class="o">-</span> <span class="n">tau</span>
        <span class="n">root</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">root</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">lower</span><span class="p">[</span><span class="n">root</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">zz</span><span class="p">[</span><span class="n">root</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">upper</span><span class="p">[</span><span class="n">root</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">zz</span><span class="p">[</span><span class="n">root</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">zz</span> <span class="o">=</span> <span class="p">(</span><span class="n">upper</span> <span class="o">+</span> <span class="n">lower</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">root</span><span class="p">))</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">zz</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">tau</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>

    <span class="k">return</span> <span class="n">zz</span> <span class="o">*</span> <span class="n">sd</span> <span class="o">+</span> <span class="n">m</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.expectile_pnorm" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">expectile_pnorm</span><span class="p">(</span><span class="n">tau</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Normal Expectile Distribution Function.
For more details and distributions see https://rdrr.io/cran/expectreg/man/enorm.html</p>
<p>Arguments</p>
<hr />
<p>tau : np.ndarray
    Vector of expectiles from the respective distribution.
m : np.ndarray
    Mean of the Normal distribution.
sd : np.ndarray
    Standard deviation of the Normal distribution.</p>
<p>Returns</p>
<hr />
<p>tau : np.ndarray
    Expectiles from the Normal distribution.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/Expectile.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">expectile_pnorm</span><span class="p">(</span><span class="n">tau</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                    <span class="n">m</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="n">sd</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Normal Expectile Distribution Function.</span>
<span class="sd">    For more details and distributions see https://rdrr.io/cran/expectreg/man/enorm.html</span>

<span class="sd">    Arguments</span>
<span class="sd">    _________</span>
<span class="sd">    tau : np.ndarray</span>
<span class="sd">        Vector of expectiles from the respective distribution.</span>
<span class="sd">    m : np.ndarray</span>
<span class="sd">        Mean of the Normal distribution.</span>
<span class="sd">    sd : np.ndarray</span>
<span class="sd">        Standard deviation of the Normal distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    _______</span>
<span class="sd">    tau : np.ndarray</span>
<span class="sd">        Expectiles from the Normal distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">tau</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">/</span> <span class="n">sd</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sd</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="o">-</span><span class="n">d</span> <span class="o">-</span> <span class="n">z</span> <span class="o">*</span> <span class="n">p</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">u</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">u</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tau</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Expectile.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Expectile.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Expectile.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Expectile.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Expectile.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Expectile.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Expectile.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Expectile.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Expectile.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Expectile.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Expectile.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Expectile.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Expectile.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Expectile.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Gamma" class="doc doc-heading">
            <code>Gamma</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Gamma.Gamma" class="doc doc-heading">
            <code>Gamma</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Gamma distribution class.</p>
<h6 id="xgboostlss.distributions.Gamma.Gamma--distributional-parameters">Distributional Parameters</h6>
<p>concentration: torch.Tensor
    shape parameter of the distribution (often referred to as alpha)
rate: torch.Tensor
    rate = 1 / scale of the distribution (often referred to as beta)</p>
<h6 id="xgboostlss.distributions.Gamma.Gamma--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#gamma</p>
<h6 id="xgboostlss.distributions.Gamma.Gamma--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Gamma.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Gamma</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gamma distribution class.</span>

<span class="sd">     Distributional Parameters</span>
<span class="sd">    --------------------------</span>
<span class="sd">    concentration: torch.Tensor</span>
<span class="sd">        shape parameter of the distribution (often referred to as alpha)</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        rate = 1 / scale of the distribution (often referred to as beta)</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#gamma</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Gamma_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;concentration&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gamma.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Gamma.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Gamma.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Gamma.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Gamma.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Gamma.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Gamma.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Gamma.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Gamma.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Gamma.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gamma.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Gamma.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gamma.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gamma.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gamma.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gamma.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Gaussian" class="doc doc-heading">
            <code>Gaussian</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Gaussian.Gaussian" class="doc doc-heading">
            <code>Gaussian</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Gaussian distribution class.</p>
<h6 id="xgboostlss.distributions.Gaussian.Gaussian--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mean of the distribution (often referred to as mu).
scale: torch.Tensor
    Standard deviation of the distribution (often referred to as sigma).</p>
<h6 id="xgboostlss.distributions.Gaussian.Gaussian--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#normal</p>
<h6 id="xgboostlss.distributions.Gaussian.Gaussian--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Gaussian.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Gaussian</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of the distribution (often referred to as mu).</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Standard deviation of the distribution (often referred to as sigma).</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#normal</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Gaussian_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gaussian.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Gaussian.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Gaussian.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Gaussian.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Gaussian.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Gaussian.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Gaussian.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Gaussian.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Gaussian.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Gaussian.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gaussian.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Gaussian.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gaussian.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gaussian.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gaussian.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gaussian.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Gumbel" class="doc doc-heading">
            <code>Gumbel</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Gumbel.Gumbel" class="doc doc-heading">
            <code>Gumbel</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Gumbel distribution class.</p>
<h6 id="xgboostlss.distributions.Gumbel.Gumbel--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Location parameter of the distribution.
scale: torch.Tensor
    Scale parameter of the distribution.</p>
<h6 id="xgboostlss.distributions.Gumbel.Gumbel--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#gumbel</p>
<h6 id="xgboostlss.distributions.Gumbel.Gumbel--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Gumbel.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Gumbel</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Location parameter of the distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Scale parameter of the distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#gumbel</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Gumbel_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gumbel.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Gumbel.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Gumbel.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Gumbel.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Gumbel.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Gumbel.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Gumbel.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Gumbel.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Gumbel.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Gumbel.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gumbel.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Gumbel.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gumbel.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Gumbel.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Gumbel.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Gumbel.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Laplace" class="doc doc-heading">
            <code>Laplace</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Laplace.Laplace" class="doc doc-heading">
            <code>Laplace</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Laplace distribution class.</p>
<h6 id="xgboostlss.distributions.Laplace.Laplace--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mean of the distribution.
scale: torch.Tensor
    Scale of the distribution.</p>
<h6 id="xgboostlss.distributions.Laplace.Laplace--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#laplace</p>
<h6 id="xgboostlss.distributions.Laplace.Laplace--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Laplace.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Laplace</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Laplace distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of the distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Scale of the distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#laplace</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Laplace_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Laplace.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Laplace.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Laplace.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Laplace.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Laplace.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Laplace.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Laplace.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Laplace.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Laplace.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Laplace.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Laplace.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Laplace.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Laplace.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Laplace.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Laplace.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Laplace.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.LogNormal" class="doc doc-heading">
            <code>LogNormal</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.LogNormal.LogNormal" class="doc doc-heading">
            <code>LogNormal</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>LogNormal distribution class.</p>
<h6 id="xgboostlss.distributions.LogNormal.LogNormal--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mean of log of distribution.
scale: torch.Tensor
    Standard deviation of log of the distribution.</p>
<h6 id="xgboostlss.distributions.LogNormal.LogNormal--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#lognormal</p>
<h6 id="xgboostlss.distributions.LogNormal.LogNormal--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/LogNormal.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LogNormal</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LogNormal distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of log of distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Standard deviation of log of the distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#lognormal</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">LogNormal_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.LogNormal.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.LogNormal.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.LogNormal.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.LogNormal.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.LogNormal.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.LogNormal.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.LogNormal.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.LogNormal.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.LogNormal.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.LogNormal.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.LogNormal.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.LogNormal.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.LogNormal.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.LogNormal.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.LogNormal.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.LogNormal.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Logistic" class="doc doc-heading">
            <code>Logistic</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Logistic.Logistic" class="doc doc-heading">
            <code>Logistic</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Logistic distribution class.</p>
<h6 id="xgboostlss.distributions.Logistic.Logistic--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Location parameter.
scale: torch.Tensor
    Scale parameter.</p>
<h6 id="xgboostlss.distributions.Logistic.Logistic--source">Source</h6>
<p>https://docs.pyro.ai/en/dev/distributions.html#logistic</p>
<h6 id="xgboostlss.distributions.Logistic.Logistic--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Logistic.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Logistic</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Logistic distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Location parameter.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Scale parameter.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://docs.pyro.ai/en/dev/distributions.html#logistic</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Logistic_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Logistic.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Logistic.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Logistic.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Logistic.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Logistic.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Logistic.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Logistic.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Logistic.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Logistic.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Logistic.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Logistic.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Logistic.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Logistic.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Logistic.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Logistic.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Logistic.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.MVN" class="doc doc-heading">
            <code>MVN</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.MVN.MVN" class="doc doc-heading">
            <code>MVN</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="Multivariate_DistributionClass (xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass)" href="#xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass">Multivariate_DistributionClass</a></code></p>



        <p>Multivariate Normal distribution class. </p>
<p>The multivariate normal distribution is parameterized by a mean vector and a lower-triangular matrix L with
positive-valued diagonal entries, such that =LL'. This triangular matrix can be obtained via, e.g., a Cholesky
decomposition of the covariance.</p>
<h6 id="xgboostlss.distributions.MVN.MVN--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mean of the distribution (often referred to as mu).
scale_tril: torch.Tensor
    Lower-triangular factor of covariance, with positive-valued diagonal.</p>
<h6 id="xgboostlss.distributions.MVN.MVN--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#multivariatenormal</p>
<h6 id="xgboostlss.distributions.MVN.MVN--parameters">Parameters</h6>
<p>D: int
    Number of targets.
stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/MVN.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MVN</span><span class="p">(</span><span class="n">Multivariate_DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multivariate Normal distribution class. </span>

<span class="sd">    The multivariate normal distribution is parameterized by a mean vector and a lower-triangular matrix L with</span>
<span class="sd">    positive-valued diagonal entries, such that =LL&#39;. This triangular matrix can be obtained via, e.g., a Cholesky</span>
<span class="sd">    decomposition of the covariance.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of the distribution (often referred to as mu).</span>
<span class="sd">    scale_tril: torch.Tensor</span>
<span class="sd">        Lower-triangular factor of covariance, with positive-valued diagonal.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#multivariatenormal</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    D: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">D</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid dimensionality type. Please choose an integer for D.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">D</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid dimensionality. Please choose D &gt;= 2.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select from &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39; or &#39;relu.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">MultivariateNormal_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="n">MVN</span><span class="o">.</span><span class="n">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">response_fn</span><span class="o">=</span><span class="n">response_fn</span><span class="p">)</span>
        <span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;loc&quot;</span><span class="p">,</span> <span class="s2">&quot;scale_tril&quot;</span><span class="p">]</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="n">distribution_arg_names</span><span class="p">,</span>
                         <span class="n">n_targets</span><span class="o">=</span><span class="n">D</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">param_transform</span><span class="o">=</span><span class="n">MVN</span><span class="o">.</span><span class="n">param_transform</span><span class="p">,</span>
                         <span class="n">get_dist_params</span><span class="o">=</span><span class="n">MVN</span><span class="o">.</span><span class="n">get_dist_params</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                          <span class="n">response_fn</span><span class="p">:</span> <span class="n">Callable</span>
                          <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that transforms the distributional parameters to the desired scale.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        response_fn: Callable</span>
<span class="sd">            Response function.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        param_dict: Dict</span>
<span class="sd">            Dictionary of distributional parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Location</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;location_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">identity_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)}</span>

        <span class="c1"># Tril</span>
        <span class="n">tril_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">tril_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">tril_indices</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">n_tril</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">n_targets</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">tril_diag</span> <span class="o">=</span> <span class="n">tril_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">tril_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">tril_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_tril</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">tril_diag</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="n">tril_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;scale_tril_diag_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]):</span> <span class="n">response_fn</span><span class="p">})</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tril_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;scale_tril_offdiag_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]):</span> <span class="n">identity_fn</span><span class="p">})</span>

        <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tril_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">param_dict</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
                        <span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">n_obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that returns a list of parameters for a multivariate normal distribution, parameterized</span>
<span class="sd">        by a location vector and the lower triangular matrix of the covariance matrix (Cholesky).</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        params: List[torch.Tensor]</span>
<span class="sd">            List of distributional parameters.</span>
<span class="sd">        param_dict: Dict</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        rank: Optional[int]</span>
<span class="sd">            Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">        n_obs: int</span>
<span class="sd">            Number of observations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        params: List[torch.Tensor]</span>
<span class="sd">            List of parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Transform Parameters to respective scale</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="p">]</span>

        <span class="c1"># Location</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[:</span><span class="n">n_targets</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Scale Tril</span>
        <span class="n">tril_predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">n_targets</span><span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">tril_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">scale_tril</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tril_predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">scale_tril</span><span class="p">[:,</span> <span class="n">tril_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tril_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">tril_predt</span>

        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale_tril</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">dist_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that returns the predicted distributional parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        dist_pred: torch.distributions.Distribution</span>
<span class="sd">            Predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dist_params_df: pd.DataFrame</span>
<span class="sd">            DataFrame with predicted distributional parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Location</span>
        <span class="n">location_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">location_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;location_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

        <span class="c1"># Scale</span>
        <span class="n">scale_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">stddev</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">scale_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;scale_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

        <span class="c1"># Rho</span>
        <span class="n">n_obs</span> <span class="o">=</span> <span class="n">location_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_rho</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">n_targets</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_targets</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">covariance_matrix</span>
        <span class="n">rho_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">MVN</span><span class="o">.</span><span class="n">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rho</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_obs</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">rho_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">rho_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;rho_</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span><span class="w"> </span><span class="n">rho_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rho_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

        <span class="c1"># Concatenate</span>
        <span class="n">dist_params_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">location_df</span><span class="p">,</span> <span class="n">scale_df</span><span class="p">,</span> <span class="n">rho_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dist_params_df</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that calculates the correlation matrix from the covariance matrix.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        cov_mat: torch.Tensor</span>
<span class="sd">            Covariance matrix.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        cor_mat: np.ndarray</span>
<span class="sd">            Correlation matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)</span>
        <span class="n">diag</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)))</span>
        <span class="n">diag_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">diag</span><span class="p">)</span>
        <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">diag_inv</span> <span class="o">@</span> <span class="n">cov_mat</span> <span class="o">@</span> <span class="n">diag_inv</span>
        <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">cor_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">tril_indices_from</span><span class="p">(</span><span class="n">cor_mat</span><span class="p">,</span> <span class="n">k</span><span class="o">=-</span><span class="mi">1</span><span class="p">)]</span>

        <span class="k">return</span> <span class="n">cor_mat</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVN.MVN.covariance_to_correlation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the correlation matrix from the covariance matrix.</p>
<h6 id="xgboostlss.distributions.MVN.MVN.covariance_to_correlation--arguments">Arguments</h6>
<p>cov_mat: torch.Tensor
    Covariance matrix.</p>
<h6 id="xgboostlss.distributions.MVN.MVN.covariance_to_correlation--returns">Returns</h6>
<p>cor_mat: np.ndarray
    Correlation matrix.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVN.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function that calculates the correlation matrix from the covariance matrix.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    cov_mat: torch.Tensor</span>
<span class="sd">        Covariance matrix.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    cor_mat: np.ndarray</span>
<span class="sd">        Correlation matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)</span>
    <span class="n">diag</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)))</span>
    <span class="n">diag_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">diag</span><span class="p">)</span>
    <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">diag_inv</span> <span class="o">@</span> <span class="n">cov_mat</span> <span class="o">@</span> <span class="n">diag_inv</span>
    <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">cor_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">tril_indices_from</span><span class="p">(</span><span class="n">cor_mat</span><span class="p">,</span> <span class="n">k</span><span class="o">=-</span><span class="mi">1</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">cor_mat</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVN.MVN.create_param_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">response_fn</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that transforms the distributional parameters to the desired scale.</p>
<h6 id="xgboostlss.distributions.MVN.MVN.create_param_dict--arguments">Arguments</h6>
<p>n_targets: int
    Number of targets.
response_fn: Callable
    Response function.</p>
<h6 id="xgboostlss.distributions.MVN.MVN.create_param_dict--returns">Returns</h6>
<p>param_dict: Dict
    Dictionary of distributional parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVN.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">response_fn</span><span class="p">:</span> <span class="n">Callable</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function that transforms the distributional parameters to the desired scale.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    response_fn: Callable</span>
<span class="sd">        Response function.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    param_dict: Dict</span>
<span class="sd">        Dictionary of distributional parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Location</span>
    <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;location_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">identity_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)}</span>

    <span class="c1"># Tril</span>
    <span class="n">tril_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">tril_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">tril_indices</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">n_tril</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">n_targets</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">tril_diag</span> <span class="o">=</span> <span class="n">tril_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">tril_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">tril_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_tril</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tril_diag</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">tril_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;scale_tril_diag_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]):</span> <span class="n">response_fn</span><span class="p">})</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tril_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;scale_tril_offdiag_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]):</span> <span class="n">identity_fn</span><span class="p">})</span>

    <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tril_dict</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">param_dict</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVN.MVN.get_dist_params" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">dist_pred</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns the predicted distributional parameters.</p>
<h6 id="xgboostlss.distributions.MVN.MVN.get_dist_params--arguments">Arguments</h6>
<p>n_targets: int
    Number of targets.
dist_pred: torch.distributions.Distribution
    Predicted distribution.</p>
<h6 id="xgboostlss.distributions.MVN.MVN.get_dist_params--returns">Returns</h6>
<p>dist_params_df: pd.DataFrame
    DataFrame with predicted distributional parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVN.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">dist_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns the predicted distributional parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    dist_pred: torch.distributions.Distribution</span>
<span class="sd">        Predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dist_params_df: pd.DataFrame</span>
<span class="sd">        DataFrame with predicted distributional parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Location</span>
    <span class="n">location_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">location_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;location_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

    <span class="c1"># Scale</span>
    <span class="n">scale_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">stddev</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">scale_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;scale_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

    <span class="c1"># Rho</span>
    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">location_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_rho</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">n_targets</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_targets</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">covariance_matrix</span>
    <span class="n">rho_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">MVN</span><span class="o">.</span><span class="n">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rho</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_obs</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">rho_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">rho_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;rho_</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span><span class="w"> </span><span class="n">rho_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rho_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

    <span class="c1"># Concatenate</span>
    <span class="n">dist_params_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">location_df</span><span class="p">,</span> <span class="n">scale_df</span><span class="p">,</span> <span class="n">rho_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dist_params_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVN.MVN.param_transform" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns a list of parameters for a multivariate normal distribution, parameterized
by a location vector and the lower triangular matrix of the covariance matrix (Cholesky).</p>
<h6 id="xgboostlss.distributions.MVN.MVN.param_transform--arguments">Arguments</h6>
<p>params: List[torch.Tensor]
    List of distributional parameters.
param_dict: Dict
n_targets: int
    Number of targets.
rank: Optional[int]
    Rank of the low-rank form of the covariance matrix.
n_obs: int
    Number of observations.</p>
<h6 id="xgboostlss.distributions.MVN.MVN.param_transform--returns">Returns</h6>
<p>params: List[torch.Tensor]
    List of parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVN.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                    <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
                    <span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                    <span class="n">n_obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function that returns a list of parameters for a multivariate normal distribution, parameterized</span>
<span class="sd">    by a location vector and the lower triangular matrix of the covariance matrix (Cholesky).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    params: List[torch.Tensor]</span>
<span class="sd">        List of distributional parameters.</span>
<span class="sd">    param_dict: Dict</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    rank: Optional[int]</span>
<span class="sd">        Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">    n_obs: int</span>
<span class="sd">        Number of observations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params: List[torch.Tensor]</span>
<span class="sd">        List of parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Transform Parameters to respective scale</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
    <span class="p">]</span>

    <span class="c1"># Location</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[:</span><span class="n">n_targets</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Scale Tril</span>
    <span class="n">tril_predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">n_targets</span><span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">tril_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">scale_tril</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tril_predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">scale_tril</span><span class="p">[:,</span> <span class="n">tril_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tril_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">tril_predt</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale_tril</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">params</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVN.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.MVN.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.MVN.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.MVN.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.MVN.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.MVN.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.MVN.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.MVN.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.MVN.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.MVN.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVN.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.MVN.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVN.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVN.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.MVN_LoRa" class="doc doc-heading">
            <code>MVN_LoRa</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa" class="doc doc-heading">
            <code>MVN_LoRa</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="Multivariate_DistributionClass (xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass)" href="#xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass">Multivariate_DistributionClass</a></code></p>



        <p>Multivariate Normal distribution class.</p>
<p>Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by
<code>cov_factor</code> and <code>cov_diag</code>:</p>
<pre><code>`covariance_matrix = cov_factor @ cov_factor.T + cov_diag`
</code></pre>
<h6 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mean of the distribution (often referred to as mu).
cov_factor: torch.Tensor
    Factor part of low-rank form of covariance matrix.
cov_diag: torch.Tensor
    Diagonal part of low-rank form of covariance matrix.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#lowrankmultivariatenormal</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa--parameters">Parameters</h6>
<p>D: int
    Number of targets.
rank: int
    Rank of the low-rank form of the covariance matrix.
stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/MVN_LoRa.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MVN_LoRa</span><span class="p">(</span><span class="n">Multivariate_DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multivariate Normal distribution class.</span>

<span class="sd">    Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by</span>
<span class="sd">    `cov_factor` and `cov_diag`:</span>

<span class="sd">        `covariance_matrix = cov_factor @ cov_factor.T + cov_diag`</span>


<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of the distribution (often referred to as mu).</span>
<span class="sd">    cov_factor: torch.Tensor</span>
<span class="sd">        Factor part of low-rank form of covariance matrix.</span>
<span class="sd">    cov_diag: torch.Tensor</span>
<span class="sd">        Diagonal part of low-rank form of covariance matrix.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#lowrankmultivariatenormal</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    D: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    rank: int</span>
<span class="sd">        Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">D</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>
        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid dimensionality type. Please choose an integer for D.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">D</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid dimensionality. Please choose D &gt;= 2.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select from &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39; or &#39;relu.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">LowRankMultivariateNormal_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="n">MVN_LoRa</span><span class="o">.</span><span class="n">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">response_fn</span><span class="o">=</span><span class="n">response_fn</span><span class="p">)</span>
        <span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;loc&quot;</span><span class="p">,</span> <span class="s2">&quot;cov_factor&quot;</span><span class="p">,</span> <span class="s2">&quot;cov_diag&quot;</span><span class="p">]</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="n">distribution_arg_names</span><span class="p">,</span>
                         <span class="n">n_targets</span><span class="o">=</span><span class="n">D</span><span class="p">,</span>
                         <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">param_transform</span><span class="o">=</span><span class="n">MVN_LoRa</span><span class="o">.</span><span class="n">param_transform</span><span class="p">,</span>
                         <span class="n">get_dist_params</span><span class="o">=</span><span class="n">MVN_LoRa</span><span class="o">.</span><span class="n">get_dist_params</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                          <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                          <span class="n">response_fn</span><span class="p">:</span> <span class="n">Callable</span>
                          <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that transforms the distributional parameters to the desired scale.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        rank: int</span>
<span class="sd">            Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">        response_fn: Callable</span>
<span class="sd">            Response function.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        param_dict: Dict</span>
<span class="sd">            Dictionary of distributional parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Location</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;location_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">identity_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)}</span>

        <span class="c1"># Low Rank Factor</span>
        <span class="n">cov_factor_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;cov_factor_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">identity_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span> <span class="o">*</span> <span class="n">rank</span><span class="p">)}</span>
        <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">cov_factor_dict</span><span class="p">)</span>

        <span class="c1"># Low Rank Diagonal</span>
        <span class="n">cov_diag_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;cov_diag_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">response_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)}</span>
        <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">cov_diag_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">param_dict</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
                        <span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">n_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that returns a list of parameters for a multivariate normal distribution, parameterized</span>
<span class="sd">        by a covariance matrix having a low-rank form parameterized by `cov_factor` and `cov_diag`:</span>

<span class="sd">        `covariance_matrix = cov_factor @ cov_factor.T + cov_diag`</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        params: List[torch.Tensor]</span>
<span class="sd">            List of distributional parameters.</span>
<span class="sd">        param_dict: Dict</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        rank: int</span>
<span class="sd">            Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">        n_obs: Optional[int],</span>
<span class="sd">            Number of observations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        params: List[torch.Tensor]</span>
<span class="sd">            List of parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Transform Parameters to respective scale</span>
        <span class="n">n_params</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="p">]</span>

        <span class="c1"># Location</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[:</span><span class="n">n_targets</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Low Rank Factor</span>
        <span class="n">cov_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="n">params</span><span class="p">[</span><span class="n">n_targets</span><span class="p">:(</span><span class="n">n_params</span> <span class="o">-</span> <span class="n">n_targets</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>

        <span class="c1"># Low Rank Diagonal</span>
        <span class="n">cov_diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="n">n_targets</span><span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">loc</span><span class="p">,</span> <span class="n">cov_factor</span><span class="p">,</span> <span class="n">cov_diag</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">dist_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that returns the predicted distributional parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        dist_pred: torch.distributions.Distribution</span>
<span class="sd">            Predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dist_params_df: pd.DataFrame</span>
<span class="sd">            DataFrame with predicted distributional parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Location</span>
        <span class="n">location_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">location_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;location_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

        <span class="c1"># Sigma</span>
        <span class="n">scale_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">stddev</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">scale_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;scale_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

        <span class="c1"># Rho</span>
        <span class="n">n_obs</span> <span class="o">=</span> <span class="n">location_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_rho</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">n_targets</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_targets</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">covariance_matrix</span>
        <span class="n">rho_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                <span class="p">[</span><span class="n">MVN_LoRa</span><span class="o">.</span><span class="n">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rho</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_obs</span><span class="p">)],</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">rho_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">rho_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;rho_</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span><span class="w"> </span><span class="n">rho_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rho_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

        <span class="c1"># Concatenate</span>
        <span class="n">dist_params_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">location_df</span><span class="p">,</span> <span class="n">scale_df</span><span class="p">,</span> <span class="n">rho_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dist_params_df</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that calculates the correlation matrix from the covariance matrix.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        cov_mat: torch.Tensor</span>
<span class="sd">            Covariance matrix.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        cor_mat: np.ndarray</span>
<span class="sd">            Correlation matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)</span>
        <span class="n">diag</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)))</span>
        <span class="n">diag_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">diag</span><span class="p">)</span>
        <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">diag_inv</span> <span class="o">@</span> <span class="n">cov_mat</span> <span class="o">@</span> <span class="n">diag_inv</span>
        <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">cor_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">tril_indices_from</span><span class="p">(</span><span class="n">cor_mat</span><span class="p">,</span> <span class="n">k</span><span class="o">=-</span><span class="mi">1</span><span class="p">)]</span>

        <span class="k">return</span> <span class="n">cor_mat</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.covariance_to_correlation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the correlation matrix from the covariance matrix.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.covariance_to_correlation--arguments">Arguments</h6>
<p>cov_mat: torch.Tensor
    Covariance matrix.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.covariance_to_correlation--returns">Returns</h6>
<p>cor_mat: np.ndarray
    Correlation matrix.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVN_LoRa.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function that calculates the correlation matrix from the covariance matrix.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    cov_mat: torch.Tensor</span>
<span class="sd">        Covariance matrix.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    cor_mat: np.ndarray</span>
<span class="sd">        Correlation matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)</span>
    <span class="n">diag</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)))</span>
    <span class="n">diag_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">diag</span><span class="p">)</span>
    <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">diag_inv</span> <span class="o">@</span> <span class="n">cov_mat</span> <span class="o">@</span> <span class="n">diag_inv</span>
    <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">cor_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">tril_indices_from</span><span class="p">(</span><span class="n">cor_mat</span><span class="p">,</span> <span class="n">k</span><span class="o">=-</span><span class="mi">1</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">cor_mat</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.create_param_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">response_fn</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that transforms the distributional parameters to the desired scale.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.create_param_dict--arguments">Arguments</h6>
<p>n_targets: int
    Number of targets.
rank: int
    Rank of the low-rank form of the covariance matrix.
response_fn: Callable
    Response function.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.create_param_dict--returns">Returns</h6>
<p>param_dict: Dict
    Dictionary of distributional parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVN_LoRa.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">response_fn</span><span class="p">:</span> <span class="n">Callable</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function that transforms the distributional parameters to the desired scale.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    rank: int</span>
<span class="sd">        Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">    response_fn: Callable</span>
<span class="sd">        Response function.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    param_dict: Dict</span>
<span class="sd">        Dictionary of distributional parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Location</span>
    <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;location_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">identity_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)}</span>

    <span class="c1"># Low Rank Factor</span>
    <span class="n">cov_factor_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;cov_factor_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">identity_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span> <span class="o">*</span> <span class="n">rank</span><span class="p">)}</span>
    <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">cov_factor_dict</span><span class="p">)</span>

    <span class="c1"># Low Rank Diagonal</span>
    <span class="n">cov_diag_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;cov_diag_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">response_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)}</span>
    <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">cov_diag_dict</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">param_dict</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.get_dist_params" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">dist_pred</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns the predicted distributional parameters.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.get_dist_params--arguments">Arguments</h6>
<p>n_targets: int
    Number of targets.
dist_pred: torch.distributions.Distribution
    Predicted distribution.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.get_dist_params--returns">Returns</h6>
<p>dist_params_df: pd.DataFrame
    DataFrame with predicted distributional parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVN_LoRa.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">dist_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns the predicted distributional parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    dist_pred: torch.distributions.Distribution</span>
<span class="sd">        Predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dist_params_df: pd.DataFrame</span>
<span class="sd">        DataFrame with predicted distributional parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Location</span>
    <span class="n">location_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">location_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;location_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

    <span class="c1"># Sigma</span>
    <span class="n">scale_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">stddev</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">scale_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;scale_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

    <span class="c1"># Rho</span>
    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">location_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_rho</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">n_targets</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_targets</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">covariance_matrix</span>
    <span class="n">rho_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span><span class="n">MVN_LoRa</span><span class="o">.</span><span class="n">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rho</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_obs</span><span class="p">)],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">rho_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">rho_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;rho_</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span><span class="w"> </span><span class="n">rho_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rho_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

    <span class="c1"># Concatenate</span>
    <span class="n">dist_params_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">location_df</span><span class="p">,</span> <span class="n">scale_df</span><span class="p">,</span> <span class="n">rho_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dist_params_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.param_transform" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns a list of parameters for a multivariate normal distribution, parameterized
by a covariance matrix having a low-rank form parameterized by <code>cov_factor</code> and <code>cov_diag</code>:</p>
<p><code>covariance_matrix = cov_factor @ cov_factor.T + cov_diag</code></p>
<h6 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.param_transform--arguments">Arguments</h6>
<p>params: List[torch.Tensor]
    List of distributional parameters.
param_dict: Dict
n_targets: int
    Number of targets.
rank: int
    Rank of the low-rank form of the covariance matrix.
n_obs: Optional[int],
    Number of observations.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.MVN_LoRa.param_transform--returns">Returns</h6>
<p>params: List[torch.Tensor]
    List of parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVN_LoRa.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                    <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
                    <span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">n_obs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function that returns a list of parameters for a multivariate normal distribution, parameterized</span>
<span class="sd">    by a covariance matrix having a low-rank form parameterized by `cov_factor` and `cov_diag`:</span>

<span class="sd">    `covariance_matrix = cov_factor @ cov_factor.T + cov_diag`</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    params: List[torch.Tensor]</span>
<span class="sd">        List of distributional parameters.</span>
<span class="sd">    param_dict: Dict</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    rank: int</span>
<span class="sd">        Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">    n_obs: Optional[int],</span>
<span class="sd">        Number of observations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params: List[torch.Tensor]</span>
<span class="sd">        List of parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Transform Parameters to respective scale</span>
    <span class="n">n_params</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
    <span class="p">]</span>

    <span class="c1"># Location</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[:</span><span class="n">n_targets</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Low Rank Factor</span>
    <span class="n">cov_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
        <span class="n">params</span><span class="p">[</span><span class="n">n_targets</span><span class="p">:(</span><span class="n">n_params</span> <span class="o">-</span> <span class="n">n_targets</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>

    <span class="c1"># Low Rank Diagonal</span>
    <span class="n">cov_diag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="n">n_targets</span><span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">loc</span><span class="p">,</span> <span class="n">cov_factor</span><span class="p">,</span> <span class="n">cov_diag</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">params</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.MVN_LoRa.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVN_LoRa.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVN_LoRa.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.MVT" class="doc doc-heading">
            <code>MVT</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.MVT.MVT" class="doc doc-heading">
            <code>MVT</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="Multivariate_DistributionClass (xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass)" href="#xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass">Multivariate_DistributionClass</a></code></p>



        <p>Multivariate Student-T distribution class.</p>
<p>The multivariate Student-T distribution is parameterized by a degree of freedom df vector, a mean vector and a
lower-triangular matrix L with positive-valued diagonal entries, such that =LL'. This triangular matrix can be
obtained via, e.g., a Cholesky decomposition of the covariance.</p>
<h6 id="xgboostlss.distributions.MVT.MVT--distributional-parameters">Distributional Parameters</h6>
<p>df: torch.Tensor
    Degrees of freedom.
loc: torch.Tensor
    Mean of the distribution (often referred to as mu).
scale_tril: torch.Tensor
    Lower-triangular factor of covariance, with positive-valued diagonal.</p>
<h6 id="xgboostlss.distributions.MVT.MVT--source">Source</h6>
<p>https://docs.pyro.ai/en/stable/distributions.html#multivariatestudentt</p>
<h6 id="xgboostlss.distributions.MVT.MVT--parameters">Parameters</h6>
<p>D: int
    Number of targets.
stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/MVT.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MVT</span><span class="p">(</span><span class="n">Multivariate_DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multivariate Student-T distribution class.</span>

<span class="sd">    The multivariate Student-T distribution is parameterized by a degree of freedom df vector, a mean vector and a</span>
<span class="sd">    lower-triangular matrix L with positive-valued diagonal entries, such that =LL&#39;. This triangular matrix can be</span>
<span class="sd">    obtained via, e.g., a Cholesky decomposition of the covariance.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    df: torch.Tensor</span>
<span class="sd">        Degrees of freedom.</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of the distribution (often referred to as mu).</span>
<span class="sd">    scale_tril: torch.Tensor</span>
<span class="sd">        Lower-triangular factor of covariance, with positive-valued diagonal.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://docs.pyro.ai/en/stable/distributions.html#multivariatestudentt</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    D: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">D</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>
        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid dimensionality type. Please choose an integer for D.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">D</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid dimensionality. Please choose D &gt;= 2.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select from &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">exp_fn</span><span class="p">,</span> <span class="n">exp_fn_df</span><span class="p">),</span>
            <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">softplus_fn</span><span class="p">,</span> <span class="n">softplus_fn_df</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span><span class="p">,</span> <span class="n">response_fn_df</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39; or &#39;relu.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">MultivariateStudentT_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="n">MVT</span><span class="o">.</span><span class="n">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">response_fn</span><span class="o">=</span><span class="n">response_fn</span><span class="p">,</span> <span class="n">response_fn_df</span><span class="o">=</span><span class="n">response_fn_df</span><span class="p">)</span>
        <span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;df&quot;</span><span class="p">,</span> <span class="s2">&quot;loc&quot;</span><span class="p">,</span> <span class="s2">&quot;scale_tril&quot;</span><span class="p">]</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="n">distribution_arg_names</span><span class="p">,</span>
                         <span class="n">n_targets</span><span class="o">=</span><span class="n">D</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">param_transform</span><span class="o">=</span><span class="n">MVT</span><span class="o">.</span><span class="n">param_transform</span><span class="p">,</span>
                         <span class="n">get_dist_params</span><span class="o">=</span><span class="n">MVT</span><span class="o">.</span><span class="n">get_dist_params</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                          <span class="n">response_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                          <span class="n">response_fn_df</span><span class="p">:</span> <span class="n">Callable</span>
                          <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that transforms the distributional parameters to the desired scale.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        response_fn: Callable</span>
<span class="sd">            Response function.</span>
<span class="sd">        response_fn_df: Callable</span>
<span class="sd">            Response function for the degrees of freedom.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        param_dict: Dict</span>
<span class="sd">            Dictionary of distributional parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Df</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;df&quot;</span><span class="p">:</span> <span class="n">response_fn_df</span><span class="p">}</span>

        <span class="c1"># Location</span>
        <span class="n">loc_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;location_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">identity_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)}</span>
        <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loc_dict</span><span class="p">)</span>

        <span class="c1"># Tril</span>
        <span class="n">tril_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">tril_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">tril_indices</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">n_tril</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">n_targets</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">tril_diag</span> <span class="o">=</span> <span class="n">tril_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">tril_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">tril_dict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_tril</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">tril_diag</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="n">tril_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;scale_tril_diag_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]):</span> <span class="n">response_fn</span><span class="p">})</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tril_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;scale_tril_offdiag_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]):</span> <span class="n">identity_fn</span><span class="p">})</span>

        <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tril_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">param_dict</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                        <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
                        <span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                        <span class="n">n_obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that returns a list of parameters for a multivariate Student-T, parameterized</span>
<span class="sd">        by a location vector and the lower triangular matrix of the covariance matrix (Cholesky).</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        params: List[torch.Tensor]</span>
<span class="sd">            List of distributional parameters.</span>
<span class="sd">        param_dict: Dict</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        rank: Optional[int]</span>
<span class="sd">            Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">        n_obs: int</span>
<span class="sd">            Number of observations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        params: List[torch.Tensor]</span>
<span class="sd">            List of parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Transform Parameters to respective scale</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="p">]</span>

        <span class="c1"># Df</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">)</span>

        <span class="c1"># Location</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">:(</span><span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Scale Tril</span>
        <span class="n">tril_predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[(</span><span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">tril_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">scale_tril</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tril_predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">scale_tril</span><span class="p">[:,</span> <span class="n">tril_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tril_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">tril_predt</span>

        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">df</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale_tril</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">dist_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that returns the predicted distributional parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        dist_pred: torch.distributions.Distribution</span>
<span class="sd">            Predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dist_params_df: pd.DataFrame</span>
<span class="sd">            DataFrame with predicted distributional parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Df</span>
        <span class="n">Df_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">Df_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;df&quot;</span><span class="p">]</span>

        <span class="c1"># Location</span>
        <span class="n">location_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">location_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;location_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

        <span class="c1"># Scale</span>
        <span class="n">scale_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">stddev</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">scale_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;scale_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

        <span class="c1"># Rho</span>
        <span class="n">n_obs</span> <span class="o">=</span> <span class="n">location_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_rho</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">n_targets</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_targets</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># The covariance is df / (df - 2) * covariance_matrix</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">covariance_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">covariance_matrix</span> <span class="o">*</span> <span class="p">(</span><span class="n">df</span> <span class="o">/</span> <span class="p">(</span><span class="n">df</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">rho_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">MVT</span><span class="o">.</span><span class="n">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rho</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_obs</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">rho_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">rho_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;rho_</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span><span class="w"> </span><span class="n">rho_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rho_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

        <span class="c1"># Concatenate</span>
        <span class="n">dist_params_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">Df_df</span><span class="p">,</span> <span class="n">location_df</span><span class="p">,</span> <span class="n">scale_df</span><span class="p">,</span> <span class="n">rho_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dist_params_df</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Function that calculates the correlation matrix from the covariance matrix.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        cov_mat: torch.Tensor</span>
<span class="sd">            Covariance matrix.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        cor_mat: np.ndarray</span>
<span class="sd">            Correlation matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)</span>
        <span class="n">diag</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)))</span>
        <span class="n">diag_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">diag</span><span class="p">)</span>
        <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">diag_inv</span> <span class="o">@</span> <span class="n">cov_mat</span> <span class="o">@</span> <span class="n">diag_inv</span>
        <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">cor_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">tril_indices_from</span><span class="p">(</span><span class="n">cor_mat</span><span class="p">,</span> <span class="n">k</span><span class="o">=-</span><span class="mi">1</span><span class="p">)]</span>

        <span class="k">return</span> <span class="n">cor_mat</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVT.MVT.covariance_to_correlation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the correlation matrix from the covariance matrix.</p>
<h6 id="xgboostlss.distributions.MVT.MVT.covariance_to_correlation--arguments">Arguments</h6>
<p>cov_mat: torch.Tensor
    Covariance matrix.</p>
<h6 id="xgboostlss.distributions.MVT.MVT.covariance_to_correlation--returns">Returns</h6>
<p>cor_mat: np.ndarray
    Correlation matrix.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVT.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function that calculates the correlation matrix from the covariance matrix.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    cov_mat: torch.Tensor</span>
<span class="sd">        Covariance matrix.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    cor_mat: np.ndarray</span>
<span class="sd">        Correlation matrix.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)</span>
    <span class="n">diag</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">)))</span>
    <span class="n">diag_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">diag</span><span class="p">)</span>
    <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">diag_inv</span> <span class="o">@</span> <span class="n">cov_mat</span> <span class="o">@</span> <span class="n">diag_inv</span>
    <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">cor_mat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">tril_indices_from</span><span class="p">(</span><span class="n">cor_mat</span><span class="p">,</span> <span class="n">k</span><span class="o">=-</span><span class="mi">1</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">cor_mat</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVT.MVT.create_param_dict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">response_fn</span><span class="p">,</span> <span class="n">response_fn_df</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that transforms the distributional parameters to the desired scale.</p>
<h6 id="xgboostlss.distributions.MVT.MVT.create_param_dict--arguments">Arguments</h6>
<p>n_targets: int
    Number of targets.
response_fn: Callable
    Response function.
response_fn_df: Callable
    Response function for the degrees of freedom.</p>
<h6 id="xgboostlss.distributions.MVT.MVT.create_param_dict--returns">Returns</h6>
<p>param_dict: Dict
    Dictionary of distributional parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVT.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_param_dict</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">response_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                      <span class="n">response_fn_df</span><span class="p">:</span> <span class="n">Callable</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function that transforms the distributional parameters to the desired scale.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    response_fn: Callable</span>
<span class="sd">        Response function.</span>
<span class="sd">    response_fn_df: Callable</span>
<span class="sd">        Response function for the degrees of freedom.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    param_dict: Dict</span>
<span class="sd">        Dictionary of distributional parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Df</span>
    <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;df&quot;</span><span class="p">:</span> <span class="n">response_fn_df</span><span class="p">}</span>

    <span class="c1"># Location</span>
    <span class="n">loc_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;location_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="n">identity_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)}</span>
    <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">loc_dict</span><span class="p">)</span>

    <span class="c1"># Tril</span>
    <span class="n">tril_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">tril_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">tril_indices</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">n_tril</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">n_targets</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">tril_diag</span> <span class="o">=</span> <span class="n">tril_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">tril_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">tril_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_tril</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tril_diag</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">tril_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;scale_tril_diag_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]):</span> <span class="n">response_fn</span><span class="p">})</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tril_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;scale_tril_offdiag_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tril_idx</span><span class="p">[:,</span> <span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]):</span> <span class="n">identity_fn</span><span class="p">})</span>

    <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tril_dict</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">param_dict</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVT.MVT.get_dist_params" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">dist_pred</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns the predicted distributional parameters.</p>
<h6 id="xgboostlss.distributions.MVT.MVT.get_dist_params--arguments">Arguments</h6>
<p>n_targets: int
    Number of targets.
dist_pred: torch.distributions.Distribution
    Predicted distribution.</p>
<h6 id="xgboostlss.distributions.MVT.MVT.get_dist_params--returns">Returns</h6>
<p>dist_params_df: pd.DataFrame
    DataFrame with predicted distributional parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVT.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">dist_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns the predicted distributional parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    dist_pred: torch.distributions.Distribution</span>
<span class="sd">        Predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dist_params_df: pd.DataFrame</span>
<span class="sd">        DataFrame with predicted distributional parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Df</span>
    <span class="n">Df_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">Df_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;df&quot;</span><span class="p">]</span>

    <span class="c1"># Location</span>
    <span class="n">location_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">loc</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">location_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;location_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

    <span class="c1"># Scale</span>
    <span class="n">scale_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">stddev</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">scale_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;scale_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_targets</span><span class="p">)]</span>

    <span class="c1"># Rho</span>
    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">location_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_rho</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">n_targets</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_targets</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># The covariance is df / (df - 2) * covariance_matrix</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">covariance_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">cov_mat</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">covariance_matrix</span> <span class="o">*</span> <span class="p">(</span><span class="n">df</span> <span class="o">/</span> <span class="p">(</span><span class="n">df</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">rho_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">MVT</span><span class="o">.</span><span class="n">covariance_to_correlation</span><span class="p">(</span><span class="n">cov_mat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rho</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_obs</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">rho_idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">combinations</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">rho_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;rho_</span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span><span class="w"> </span><span class="n">rho_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rho_df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

    <span class="c1"># Concatenate</span>
    <span class="n">dist_params_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">Df_df</span><span class="p">,</span> <span class="n">location_df</span><span class="p">,</span> <span class="n">scale_df</span><span class="p">,</span> <span class="n">rho_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dist_params_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.MVT.MVT.param_transform" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">n_obs</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns a list of parameters for a multivariate Student-T, parameterized
by a location vector and the lower triangular matrix of the covariance matrix (Cholesky).</p>
<h6 id="xgboostlss.distributions.MVT.MVT.param_transform--arguments">Arguments</h6>
<p>params: List[torch.Tensor]
    List of distributional parameters.
param_dict: Dict
n_targets: int
    Number of targets.
rank: Optional[int]
    Rank of the low-rank form of the covariance matrix.
n_obs: int
    Number of observations.</p>
<h6 id="xgboostlss.distributions.MVT.MVT.param_transform--returns">Returns</h6>
<p>params: List[torch.Tensor]
    List of parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/MVT.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                    <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
                    <span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
                    <span class="n">n_obs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Function that returns a list of parameters for a multivariate Student-T, parameterized</span>
<span class="sd">    by a location vector and the lower triangular matrix of the covariance matrix (Cholesky).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    params: List[torch.Tensor]</span>
<span class="sd">        List of distributional parameters.</span>
<span class="sd">    param_dict: Dict</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    rank: Optional[int]</span>
<span class="sd">        Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">    n_obs: int</span>
<span class="sd">        Number of observations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params: List[torch.Tensor]</span>
<span class="sd">        List of parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Transform Parameters to respective scale</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
    <span class="p">]</span>

    <span class="c1"># Df</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">)</span>

    <span class="c1"># Location</span>
    <span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">:(</span><span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Scale Tril</span>
    <span class="n">tril_predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">params</span><span class="p">[(</span><span class="n">n_targets</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">tril_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril_indices</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">scale_tril</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_obs</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tril_predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">scale_tril</span><span class="p">[:,</span> <span class="n">tril_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tril_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">tril_predt</span>

    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">df</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale_tril</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">params</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVT.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.MVT.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.MVT.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.MVT.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.MVT.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.MVT.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.MVT.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.MVT.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.MVT.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.MVT.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVT.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.MVT.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVT.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.MVT.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.MVT.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.MVT.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Mixture" class="doc doc-heading">
            <code>Mixture</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Mixture.Mixture" class="doc doc-heading">
            <code>Mixture</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="MixtureDistributionClass (xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass)" href="#xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass">MixtureDistributionClass</a></code></p>



        <p>Mixture-Density distribution class.</p>
<p>Implements a mixture-density distribution for univariate targets, where all components are from different
parameterizations of the same distribution-type. A mixture-density distribution is a concept used to model a
complex distribution that arises from combining multiple simpler distributions. The Mixture-Density distribution
is parameterized by a categorical selecting distribution (over M components) and M-component distributions. For more
information on the Mixture-Density distribution, see:</p>
<pre><code>Bishop, C. M. (1994). Mixture density networks. Technical Report NCRG/4288, Aston University, Birmingham, UK.
</code></pre>
<h6 id="xgboostlss.distributions.Mixture.Mixture--distributional-parameters">Distributional Parameters</h6>
<p>Inherits the distributional parameters from the component distributions.</p>
<h6 id="xgboostlss.distributions.Mixture.Mixture--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#mixturesamefamily</p>
<h6 id="xgboostlss.distributions.Mixture.Mixture--parameters">Parameters</h6>
<p>component_distribution: torch.distributions.Distribution
    Distribution class for the components of the mixture distribution. Has to be one of the available
    univariate distributions of the package.
M: int
    Number of components in the mixture distribution.
hessian_mode: str
    Mode for computing the Hessian. Must be one of the following:</p>
<pre><code>    - "individual": Each parameter is treated as a separate tensor. As a result, the Hessian corresponds to the
    second-order derivative with respect to that specific parameter only. The resulting Hessians capture the
    curvature of the loss w.r.t. each individual parameter. This is usually more runtime intensive, but can
    be more accurate.

    - "grouped": Each tensor contains all parameters for a specific parameter-type, e.g., for a Gaussian-Mixture
    with M=2, loc=[loc_1, loc_2], scale=[scale_1, scale_2], and mix_prob=[mix_prob_1, mix_prob_2]. When
    computing the Hessian, the derivatives for all parameters in the respective tensor are calculated jointly.
    The resulting Hessians capture the curvature of the loss w.r.t. the entire parameter-type. This is usually
    less runtime intensive, but can be less accurate.
</code></pre>
<p>tau: float, non-negative scalar temperature.
    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
    defined as:</p>
<pre><code>    s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}

where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to

    Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>


<details class="initialize" open>
  <summary>bool</summary>
  <p>Whether to initialize the distributional parameters with unconditional start values. Initialization can help
to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
solutions if the unconditional start values are far from the optimal values.</p>
</details>






              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Mixture.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Mixture</span><span class="p">(</span><span class="n">MixtureDistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mixture-Density distribution class.</span>

<span class="sd">    Implements a mixture-density distribution for univariate targets, where all components are from different</span>
<span class="sd">    parameterizations of the same distribution-type. A mixture-density distribution is a concept used to model a</span>
<span class="sd">    complex distribution that arises from combining multiple simpler distributions. The Mixture-Density distribution</span>
<span class="sd">    is parameterized by a categorical selecting distribution (over M components) and M-component distributions. For more</span>
<span class="sd">    information on the Mixture-Density distribution, see:</span>

<span class="sd">        Bishop, C. M. (1994). Mixture density networks. Technical Report NCRG/4288, Aston University, Birmingham, UK.</span>


<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    Inherits the distributional parameters from the component distributions.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#mixturesamefamily</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    component_distribution: torch.distributions.Distribution</span>
<span class="sd">        Distribution class for the components of the mixture distribution. Has to be one of the available</span>
<span class="sd">        univariate distributions of the package.</span>
<span class="sd">    M: int</span>
<span class="sd">        Number of components in the mixture distribution.</span>
<span class="sd">    hessian_mode: str</span>
<span class="sd">        Mode for computing the Hessian. Must be one of the following:</span>

<span class="sd">            - &quot;individual&quot;: Each parameter is treated as a separate tensor. As a result, the Hessian corresponds to the</span>
<span class="sd">            second-order derivative with respect to that specific parameter only. The resulting Hessians capture the</span>
<span class="sd">            curvature of the loss w.r.t. each individual parameter. This is usually more runtime intensive, but can</span>
<span class="sd">            be more accurate.</span>

<span class="sd">            - &quot;grouped&quot;: Each tensor contains all parameters for a specific parameter-type, e.g., for a Gaussian-Mixture</span>
<span class="sd">            with M=2, loc=[loc_1, loc_2], scale=[scale_1, scale_2], and mix_prob=[mix_prob_1, mix_prob_2]. When</span>
<span class="sd">            computing the Hessian, the derivatives for all parameters in the respective tensor are calculated jointly.</span>
<span class="sd">            The resulting Hessians capture the curvature of the loss w.r.t. the entire parameter-type. This is usually</span>
<span class="sd">            less runtime intensive, but can be less accurate.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">        version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">        differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">        categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">        Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">        Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">        defined as:</span>

<span class="sd">            s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">        where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">        of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">        approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">            Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">component_distribution</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                 <span class="n">M</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">hessian_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;individual&quot;</span><span class="p">,</span>
                 <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="n">mixt_dist</span> <span class="o">=</span> <span class="n">get_component_distributions</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">str</span><span class="p">(</span><span class="n">component_distribution</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mixt_dist</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;component_distribution must be one of the following: </span><span class="si">{</span><span class="n">mixt_dist</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;M must be an integer.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">M</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;M must be greater than 1.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">component_distribution</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">!=</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Loss for component_distribution must be &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hessian_mode</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;hessian_mode must be a string.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hessian_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;individual&quot;</span><span class="p">,</span> <span class="s2">&quot;grouped&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;hessian_mode must be either &#39;individual&#39; or &#39;grouped&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;tau must be a float.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tau</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;tau must be greater than 0.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="n">component_distribution</span><span class="o">.</span><span class="n">param_dict</span>
        <span class="n">preset_gumbel_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">gumbel_softmax_fn</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>
        <span class="n">param_dict</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;mix_prob&quot;</span><span class="p">:</span> <span class="n">preset_gumbel_fn</span><span class="p">})</span>
        <span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">param_dict</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">component_distribution</span><span class="p">,</span>
                         <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">,</span>
                         <span class="n">temperature</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span>
                         <span class="n">hessian_mode</span><span class="o">=</span><span class="n">hessian_mode</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="n">component_distribution</span><span class="o">.</span><span class="n">discrete</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">distribution_arg_names</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">component_distribution</span><span class="o">.</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="n">distribution_arg_names</span><span class="p">,</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">component_distribution</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Mixture.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Mixture.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Mixture.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Mixture.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Mixture.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Mixture.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Mixture.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Mixture.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Mixture.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Mixture.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Mixture.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Mixture.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Mixture.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Mixture.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Mixture.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Mixture.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.NegativeBinomial" class="doc doc-heading">
            <code>NegativeBinomial</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.NegativeBinomial.NegativeBinomial" class="doc doc-heading">
            <code>NegativeBinomial</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>NegativeBinomial distribution class.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.NegativeBinomial--distributional-parameters">Distributional Parameters</h6>
<p>total_count: torch.Tensor
    Non-negative number of negative Bernoulli trials to stop.
probs: torch.Tensor
    Event probabilities of success in the half open interval [0, 1).
logits: torch.Tensor
    Event log-odds for probabilities of success.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.NegativeBinomial--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#negativebinomial</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.NegativeBinomial--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn_total_count: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential), "softplus" (softplus) or "relu" (rectified linear unit).
response_fn_probs: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "sigmoid" (sigmoid).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/NegativeBinomial.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">NegativeBinomial</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    NegativeBinomial distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    total_count: torch.Tensor</span>
<span class="sd">        Non-negative number of negative Bernoulli trials to stop.</span>
<span class="sd">    probs: torch.Tensor</span>
<span class="sd">        Event probabilities of success in the half open interval [0, 1).</span>
<span class="sd">    logits: torch.Tensor</span>
<span class="sd">        Event log-odds for probabilities of success.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#negativebinomial</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn_total_count: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential), &quot;softplus&quot; (softplus) or &quot;relu&quot; (rectified linear unit).</span>
<span class="sd">    response_fn_probs: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;sigmoid&quot; (sigmoid).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn_total_count</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
                 <span class="n">response_fn_probs</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1">#  Specify Response Functions for total_count</span>
        <span class="n">response_functions_total_count</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn_total_count</span> <span class="ow">in</span> <span class="n">response_functions_total_count</span><span class="p">:</span>
            <span class="n">response_fn_total_count</span> <span class="o">=</span> <span class="n">response_functions_total_count</span><span class="p">[</span><span class="n">response_fn_total_count</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for total_count. Please choose from &#39;exp&#39;, &#39;softplus&#39; or &#39;relu&#39;.&quot;</span><span class="p">)</span>

        <span class="c1">#  Specify Response Functions for probs</span>
        <span class="n">response_functions_probs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn_probs</span> <span class="ow">in</span> <span class="n">response_functions_probs</span><span class="p">:</span>
            <span class="n">response_fn_probs</span> <span class="o">=</span> <span class="n">response_functions_probs</span><span class="p">[</span><span class="n">response_fn_probs</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for probs. Please select &#39;sigmoid&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">NegativeBinomial_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;total_count&quot;</span><span class="p">:</span> <span class="n">response_fn_total_count</span><span class="p">,</span> <span class="s2">&quot;probs&quot;</span><span class="p">:</span> <span class="n">response_fn_probs</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.NegativeBinomial.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.NegativeBinomial.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.NegativeBinomial.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Poisson" class="doc doc-heading">
            <code>Poisson</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Poisson.Poisson" class="doc doc-heading">
            <code>Poisson</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Poisson distribution class.</p>
<h6 id="xgboostlss.distributions.Poisson.Poisson--distributional-parameters">Distributional Parameters</h6>
<p>rate: torch.Tensor
    Rate parameter of the distribution (often referred to as lambda).</p>
<h6 id="xgboostlss.distributions.Poisson.Poisson--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#poisson</p>
<h6 id="xgboostlss.distributions.Poisson.Poisson--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential), "softplus" (softplus) or "relu" (rectified linear unit).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Poisson.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Poisson</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Poisson distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        Rate parameter of the distribution (often referred to as lambda).</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#poisson</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential), &quot;softplus&quot; (softplus) or &quot;relu&quot; (rectified linear unit).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for total_count. Please choose from &#39;exp&#39;, &#39;softplus&#39; or &#39;relu&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Poisson_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Poisson.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Poisson.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Poisson.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Poisson.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Poisson.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Poisson.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Poisson.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Poisson.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Poisson.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Poisson.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Poisson.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Poisson.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Poisson.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Poisson.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Poisson.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Poisson.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.SplineFlow" class="doc doc-heading">
            <code>SplineFlow</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.SplineFlow.SplineFlow" class="doc doc-heading">
            <code>SplineFlow</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="NormalizingFlowClass (xgboostlss.distributions.flow_utils.NormalizingFlowClass)" href="#xgboostlss.distributions.flow_utils.NormalizingFlowClass">NormalizingFlowClass</a></code></p>



        <p>Spline Flow class.</p>
<p>The spline flow is a normalizing flow based on element-wise rational spline bijections of linear and quadratic
order (Durkan et al., 2019; Dolatabadi et al., 2020). Rational splines are functions that are comprised of segments
that are the ratio of two polynomials. Rational splines offer an excellent combination of functional flexibility
whilst maintaining a numerically stable inverse.</p>
<p>For more details, see:
- Durkan, C., Bekasov, A., Murray, I. and Papamakarios, G. Neural Spline Flows. NeurIPS 2019.
- Dolatabadi, H. M., Erfani, S. and Leckie, C., Invertible Generative Modeling using Linear Rational Splines. AISTATS 2020.</p>
<h6 id="xgboostlss.distributions.SplineFlow.SplineFlow--source">Source</h6>
<p>https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.transforms.Spline</p>
<h6 id="xgboostlss.distributions.SplineFlow.SplineFlow--arguments">Arguments</h6>
<p>target_support: str
    The target support. Options are
        - "real": [-inf, inf]
        - "positive": [0, inf]
        - "positive_integer": [0, 1, 2, 3, ...]
        - "unit_interval": [0, 1]
count_bins: int
    The number of segments comprising the spline.
bound: float
    The quantity "K" determining the bounding box, [-K,K] x [-K,K] of the spline. By adjusting the
    "K" value, you can control the size of the bounding box and consequently control the range of inputs that
    the spline transform operates on. Larger values of "K" will result in a wider valid range for the spline
    transformation, while smaller values will restrict the valid range to a smaller region. Should be chosen
    based on the range of the data.
order: str
    The order of the spline. Options are "linear" or "quadratic".
stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD" or "L2".
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/SplineFlow.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SplineFlow</span><span class="p">(</span><span class="n">NormalizingFlowClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Spline Flow class.</span>

<span class="sd">    The spline flow is a normalizing flow based on element-wise rational spline bijections of linear and quadratic</span>
<span class="sd">    order (Durkan et al., 2019; Dolatabadi et al., 2020). Rational splines are functions that are comprised of segments</span>
<span class="sd">    that are the ratio of two polynomials. Rational splines offer an excellent combination of functional flexibility</span>
<span class="sd">    whilst maintaining a numerically stable inverse.</span>

<span class="sd">    For more details, see:</span>
<span class="sd">    - Durkan, C., Bekasov, A., Murray, I. and Papamakarios, G. Neural Spline Flows. NeurIPS 2019.</span>
<span class="sd">    - Dolatabadi, H. M., Erfani, S. and Leckie, C., Invertible Generative Modeling using Linear Rational Splines. AISTATS 2020.</span>


<span class="sd">    Source</span>
<span class="sd">    ---------</span>
<span class="sd">    https://docs.pyro.ai/en/stable/distributions.html#pyro.distributions.transforms.Spline</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    target_support: str</span>
<span class="sd">        The target support. Options are</span>
<span class="sd">            - &quot;real&quot;: [-inf, inf]</span>
<span class="sd">            - &quot;positive&quot;: [0, inf]</span>
<span class="sd">            - &quot;positive_integer&quot;: [0, 1, 2, 3, ...]</span>
<span class="sd">            - &quot;unit_interval&quot;: [0, 1]</span>
<span class="sd">    count_bins: int</span>
<span class="sd">        The number of segments comprising the spline.</span>
<span class="sd">    bound: float</span>
<span class="sd">        The quantity &quot;K&quot; determining the bounding box, [-K,K] x [-K,K] of the spline. By adjusting the</span>
<span class="sd">        &quot;K&quot; value, you can control the size of the bounding box and consequently control the range of inputs that</span>
<span class="sd">        the spline transform operates on. Larger values of &quot;K&quot; will result in a wider valid range for the spline</span>
<span class="sd">        transformation, while smaller values will restrict the valid range to a smaller region. Should be chosen</span>
<span class="sd">        based on the range of the data.</span>
<span class="sd">    order: str</span>
<span class="sd">        The order of the spline. Options are &quot;linear&quot; or &quot;quadratic&quot;.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">target_support</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;real&quot;</span><span class="p">,</span>
                 <span class="n">count_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
                 <span class="n">bound</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span>
                 <span class="n">order</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Specify Target Transform</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target_support</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;target_support must be a string.&quot;</span><span class="p">)</span>

        <span class="n">transforms</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;real&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">identity_transform</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
            <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SoftplusTransform</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span>
            <span class="s2">&quot;positive_integer&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SoftplusTransform</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
            <span class="s2">&quot;unit_interval&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SigmoidTransform</span><span class="p">(),</span> <span class="kc">False</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">target_support</span> <span class="ow">in</span> <span class="n">transforms</span><span class="p">:</span>
            <span class="n">target_transform</span><span class="p">,</span> <span class="n">discrete</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">[</span><span class="n">target_support</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid target_support. Options are &#39;real&#39;, &#39;positive&#39;, &#39;positive_integer&#39;, or &#39;unit_interval&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if count_bins is valid</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">count_bins</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;count_bins must be an integer.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">count_bins</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;count_bins must be a positive integer &gt; 0.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if bound is float</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bound</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;bound must be a float.&quot;</span><span class="p">)</span>

        <span class="c1"># Number of parameters</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;order must be a string.&quot;</span><span class="p">)</span>

        <span class="n">order_params</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;quadratic&quot;</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">count_bins</span> <span class="o">+</span> <span class="p">(</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
            <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">count_bins</span> <span class="o">+</span> <span class="p">(</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">order</span> <span class="ow">in</span> <span class="n">order_params</span><span class="p">:</span>
            <span class="n">n_params</span> <span class="o">=</span> <span class="n">order_params</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid order specification. Options are &#39;linear&#39; or &#39;quadratic&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if stabilization method is valid.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stabilization</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;stabilization must be a string.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Options are &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if loss function is valid.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;loss_fn must be a string.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss_fn. Options are &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if initialize is valid.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify parameter dictionary</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;param_</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">identity_fn</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_params</span><span class="p">)}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Normalizing Flow Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="o">=</span><span class="n">Normal</span><span class="p">,</span>                     <span class="c1"># Base distribution, currently only Normal is supported.</span>
                         <span class="n">flow_transform</span><span class="o">=</span><span class="n">Spline</span><span class="p">,</span>
                         <span class="n">count_bins</span><span class="o">=</span><span class="n">count_bins</span><span class="p">,</span>
                         <span class="n">bound</span><span class="o">=</span><span class="n">bound</span><span class="p">,</span>
                         <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="n">n_params</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">target_transform</span><span class="o">=</span><span class="n">target_transform</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="n">discrete</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.StudentT" class="doc doc-heading">
            <code>StudentT</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.StudentT.StudentT" class="doc doc-heading">
            <code>StudentT</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Student-T Distribution Class</p>
<h6 id="xgboostlss.distributions.StudentT.StudentT--distributional-parameters">Distributional Parameters</h6>
<p>df: torch.Tensor
    Degrees of freedom.
loc: torch.Tensor
    Mean of the distribution.
scale: torch.Tensor
    Scale of the distribution.</p>
<h6 id="xgboostlss.distributions.StudentT.StudentT--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#studentt</p>
<h6 id="xgboostlss.distributions.StudentT.StudentT--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/StudentT.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">StudentT</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Student-T Distribution Class</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    df: torch.Tensor</span>
<span class="sd">        Degrees of freedom.</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of the distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Scale of the distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#studentt</span>

<span class="sd">     Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">exp_fn</span><span class="p">,</span> <span class="n">exp_fn_df</span><span class="p">),</span>
            <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">softplus_fn</span><span class="p">,</span> <span class="n">softplus_fn_df</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span><span class="p">,</span> <span class="n">response_fn_df</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">StudentT_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;df&quot;</span><span class="p">:</span> <span class="n">response_fn_df</span><span class="p">,</span> <span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.StudentT.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.StudentT.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.StudentT.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.StudentT.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.StudentT.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.StudentT.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.StudentT.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.StudentT.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.StudentT.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.StudentT.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.StudentT.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.StudentT.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.StudentT.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.StudentT.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.StudentT.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.StudentT.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.Weibull" class="doc doc-heading">
            <code>Weibull</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.Weibull.Weibull" class="doc doc-heading">
            <code>Weibull</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Weibull distribution class.</p>
<h6 id="xgboostlss.distributions.Weibull.Weibull--distributional-parameters">Distributional Parameters</h6>
<p>scale: torch.Tensor
    Scale parameter of distribution (lambda).
concentration: torch.Tensor
    Concentration parameter of distribution (k/shape).</p>
<h6 id="xgboostlss.distributions.Weibull.Weibull--source">Source</h6>
<p>https://pytorch.org/docs/stable/distributions.html#weibull</p>
<h6 id="xgboostlss.distributions.Weibull.Weibull--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/Weibull.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Weibull</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Weibull distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Scale parameter of distribution (lambda).</span>
<span class="sd">    concentration: torch.Tensor</span>
<span class="sd">        Concentration parameter of distribution (k/shape).</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://pytorch.org/docs/stable/distributions.html#weibull</span>

<span class="sd">     Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">,</span> <span class="s2">&quot;crps&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please choose from &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">Weibull_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;concentration&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Weibull.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Weibull.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.Weibull.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.Weibull.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.Weibull.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.Weibull.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Weibull.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.Weibull.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.Weibull.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.Weibull.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Weibull.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.Weibull.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Weibull.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.Weibull.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.Weibull.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.Weibull.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.ZABeta" class="doc doc-heading">
            <code>ZABeta</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.ZABeta.ZABeta" class="doc doc-heading">
            <code>ZABeta</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Zero-Adjusted Beta distribution class.</p>
<p>The zero-adjusted Beta distribution is similar to the Beta distribution but allows zeros as y values.</p>
<h6 id="xgboostlss.distributions.ZABeta.ZABeta--distributional-parameters">Distributional Parameters</h6>
<p>concentration1: torch.Tensor
    1st concentration parameter of the distribution (often referred to as alpha).
concentration0: torch.Tensor
    2nd concentration parameter of the distribution (often referred to as beta).
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="xgboostlss.distributions.ZABeta.ZABeta--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</p>
<h6 id="xgboostlss.distributions.ZABeta.ZABeta--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/ZABeta.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZABeta</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Zero-Adjusted Beta distribution class.</span>

<span class="sd">    The zero-adjusted Beta distribution is similar to the Beta distribution but allows zeros as y values.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    concentration1: torch.Tensor</span>
<span class="sd">        1st concentration parameter of the distribution (often referred to as alpha).</span>
<span class="sd">    concentration0: torch.Tensor</span>
<span class="sd">        2nd concentration parameter of the distribution (often referred to as beta).</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">ZeroAdjustedBeta_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;concentration1&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;concentration0&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZABeta.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.ZABeta.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.ZABeta.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.ZABeta.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.ZABeta.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.ZABeta.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.ZABeta.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.ZABeta.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.ZABeta.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.ZABeta.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZABeta.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.ZABeta.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZABeta.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZABeta.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZABeta.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZABeta.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.ZAGamma" class="doc doc-heading">
            <code>ZAGamma</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.ZAGamma.ZAGamma" class="doc doc-heading">
            <code>ZAGamma</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Zero-Adjusted Gamma distribution class.</p>
<p>The zero-adjusted Gamma distribution is similar to the Gamma distribution but allows zeros as y values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.ZAGamma--distributional-parameters">Distributional Parameters</h6>
<p>concentration: torch.Tensor
    shape parameter of the distribution (often referred to as alpha)
rate: torch.Tensor
    rate = 1 / scale of the distribution (often referred to as beta)
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="xgboostlss.distributions.ZAGamma.ZAGamma--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</p>
<h6 id="xgboostlss.distributions.ZAGamma.ZAGamma--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/ZAGamma.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZAGamma</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Zero-Adjusted Gamma distribution class.</span>

<span class="sd">    The zero-adjusted Gamma distribution is similar to the Gamma distribution but allows zeros as y values.</span>

<span class="sd">     Distributional Parameters</span>
<span class="sd">    --------------------------</span>
<span class="sd">    concentration: torch.Tensor</span>
<span class="sd">        shape parameter of the distribution (often referred to as alpha)</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        rate = 1 / scale of the distribution (often referred to as beta)</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">ZeroAdjustedGamma_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;concentration&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZAGamma.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.ZAGamma.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.ZAGamma.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.ZAGamma.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.ZAGamma.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.ZAGamma.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.ZAGamma.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.ZAGamma.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.ZAGamma.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.ZAGamma.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZAGamma.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.ZAGamma.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZAGamma.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZAGamma.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZAGamma.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZAGamma.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.ZALN" class="doc doc-heading">
            <code>ZALN</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.ZALN.ZALN" class="doc doc-heading">
            <code>ZALN</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Zero-Adjusted LogNormal distribution class.</p>
<p>The zero-adjusted Log-Normal distribution is similar to the Log-Normal distribution but allows zeros as y values.</p>
<h6 id="xgboostlss.distributions.ZALN.ZALN--distributional-parameters">Distributional Parameters</h6>
<p>loc: torch.Tensor
    Mean of log of distribution.
scale: torch.Tensor
    Standard deviation of log of the distribution.
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="xgboostlss.distributions.ZALN.ZALN--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</p>
<h6 id="xgboostlss.distributions.ZALN.ZALN--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential) or "softplus" (softplus).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/ZALN.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZALN</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Zero-Adjusted LogNormal distribution class.</span>

<span class="sd">    The zero-adjusted Log-Normal distribution is similar to the Log-Normal distribution but allows zeros as y values.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of log of distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Standard deviation of log of the distribution.</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential) or &quot;softplus&quot; (softplus).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;exp&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function. Please choose from &#39;exp&#39; or &#39;softplus&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">ZeroAdjustedLogNormal_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">identity_fn</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span>  <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZALN.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.ZALN.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.ZALN.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.ZALN.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.ZALN.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.ZALN.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.ZALN.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.ZALN.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.ZALN.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.ZALN.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZALN.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.ZALN.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZALN.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZALN.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZALN.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZALN.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.ZINB" class="doc doc-heading">
            <code>ZINB</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.ZINB.ZINB" class="doc doc-heading">
            <code>ZINB</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Zero-Inflated Negative Binomial distribution class.</p>
<h6 id="xgboostlss.distributions.ZINB.ZINB--distributional-parameters">Distributional Parameters</h6>
<p>total_count: torch.Tensor
    Non-negative number of negative Bernoulli trials to stop.
probs: torch.Tensor
    Event probabilities of success in the half open interval [0, 1).
gate: torch.Tensor
    Probability of extra zeros given via a Bernoulli distribution.</p>
<h6 id="xgboostlss.distributions.ZINB.ZINB--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</p>
<h6 id="xgboostlss.distributions.ZINB.ZINB--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn_total_count: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential), "softplus" (softplus) or "relu" (rectified linear unit).
response_fn_probs: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "sigmoid" (sigmoid).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/ZINB.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZINB</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Zero-Inflated Negative Binomial distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    total_count: torch.Tensor</span>
<span class="sd">        Non-negative number of negative Bernoulli trials to stop.</span>
<span class="sd">    probs: torch.Tensor</span>
<span class="sd">        Event probabilities of success in the half open interval [0, 1).</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of extra zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn_total_count: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential), &quot;softplus&quot; (softplus) or &quot;relu&quot; (rectified linear unit).</span>
<span class="sd">    response_fn_probs: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;sigmoid&quot; (sigmoid).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn_total_count</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
                 <span class="n">response_fn_probs</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1">#  Specify Response Functions for total_count</span>
        <span class="n">response_functions_total_count</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn_total_count</span> <span class="ow">in</span> <span class="n">response_functions_total_count</span><span class="p">:</span>
            <span class="n">response_fn_total_count</span> <span class="o">=</span> <span class="n">response_functions_total_count</span><span class="p">[</span><span class="n">response_fn_total_count</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for total_count. Please choose from &#39;exp&#39;, &#39;softplus&#39; or &#39;relu&#39;.&quot;</span><span class="p">)</span>

        <span class="c1">#  Specify Response Functions for probs</span>
        <span class="n">response_functions_probs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn_probs</span> <span class="ow">in</span> <span class="n">response_functions_probs</span><span class="p">:</span>
            <span class="n">response_fn_probs</span> <span class="o">=</span> <span class="n">response_functions_probs</span><span class="p">[</span><span class="n">response_fn_probs</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for probs. Please select &#39;sigmoid&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">ZeroInflatedNegativeBinomial_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;total_count&quot;</span><span class="p">:</span> <span class="n">response_fn_total_count</span><span class="p">,</span> <span class="s2">&quot;probs&quot;</span><span class="p">:</span> <span class="n">response_fn_probs</span><span class="p">,</span> <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZINB.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.ZINB.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.ZINB.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.ZINB.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.ZINB.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.ZINB.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.ZINB.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.ZINB.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.ZINB.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.ZINB.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZINB.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.ZINB.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZINB.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZINB.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZINB.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZINB.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.ZIPoisson" class="doc doc-heading">
            <code>ZIPoisson</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.ZIPoisson.ZIPoisson" class="doc doc-heading">
            <code>ZIPoisson</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="DistributionClass (xgboostlss.distributions.distribution_utils.DistributionClass)" href="#xgboostlss.distributions.distribution_utils.DistributionClass">DistributionClass</a></code></p>



        <p>Zero-Inflated Poisson distribution class.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.ZIPoisson--distributional-parameters">Distributional Parameters</h6>
<p>rate: torch.Tensor
    Rate parameter of the distribution (often referred to as lambda).
gate: torch.Tensor
    Probability of extra zeros given via a Bernoulli distribution.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.ZIPoisson--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L121</p>
<h6 id="xgboostlss.distributions.ZIPoisson.ZIPoisson--parameters">Parameters</h6>
<p>stabilization: str
    Stabilization method for the Gradient and Hessian. Options are "None", "MAD", "L2".
response_fn: str
    Response function for transforming the distributional parameters to the correct support. Options are
    "exp" (exponential), "softplus" (softplus) or "relu" (rectified linear unit).
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/ZIPoisson.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZIPoisson</span><span class="p">(</span><span class="n">DistributionClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Zero-Inflated Poisson distribution class.</span>

<span class="sd">    Distributional Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        Rate parameter of the distribution (often referred to as lambda).</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of extra zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    -------------------------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L121</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------------------</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method for the Gradient and Hessian. Options are &quot;None&quot;, &quot;MAD&quot;, &quot;L2&quot;.</span>
<span class="sd">    response_fn: str</span>
<span class="sd">        Response function for transforming the distributional parameters to the correct support. Options are</span>
<span class="sd">        &quot;exp&quot; (exponential), &quot;softplus&quot; (softplus) or &quot;relu&quot; (rectified linear unit).</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">response_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="c1"># Input Checks</span>
        <span class="k">if</span> <span class="n">stabilization</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;None&quot;</span><span class="p">,</span> <span class="s2">&quot;MAD&quot;</span><span class="p">,</span> <span class="s2">&quot;L2&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stabilization method. Please choose from &#39;None&#39;, &#39;MAD&#39; or &#39;L2&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss_fn</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;nll&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initialize</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid initialize. Please choose from True or False.&quot;</span><span class="p">)</span>

        <span class="c1"># Specify Response Functions</span>
        <span class="n">response_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;exp&quot;</span><span class="p">:</span> <span class="n">exp_fn</span><span class="p">,</span> <span class="s2">&quot;softplus&quot;</span><span class="p">:</span> <span class="n">softplus_fn</span><span class="p">,</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">relu_fn</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="n">response_functions</span><span class="p">:</span>
            <span class="n">response_fn</span> <span class="o">=</span> <span class="n">response_functions</span><span class="p">[</span><span class="n">response_fn</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Invalid response function for total_count. Please choose from &#39;exp&#39;, &#39;softplus&#39; or &#39;relu&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Set the parameters specific to the distribution</span>
        <span class="n">distribution</span> <span class="o">=</span> <span class="n">ZeroInflatedPoisson_Torch</span>
        <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">response_fn</span><span class="p">,</span> <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">sigmoid_fn</span><span class="p">}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="o">.</span><span class="n">set_default_validate_args</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Specify Distribution Class</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">distribution</span><span class="o">=</span><span class="n">distribution</span><span class="p">,</span>
                         <span class="n">univariate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">discrete</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">n_dist_param</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">param_dict</span><span class="p">),</span>
                         <span class="n">stabilization</span><span class="o">=</span><span class="n">stabilization</span><span class="p">,</span>
                         <span class="n">param_dict</span><span class="o">=</span><span class="n">param_dict</span><span class="p">,</span>
                         <span class="n">distribution_arg_names</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                         <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
                         <span class="n">initialize</span><span class="o">=</span><span class="n">initialize</span><span class="p">,</span>
                         <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.exp_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.exp_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.exp_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.exp_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h6 id="xgboostlss.distributions.ZIPoisson.gumbel_softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.gumbel_softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.identity_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.identity_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.nan_to_num--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.nan_to_num--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.ZIPoisson.relu_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.relu_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h6 id="xgboostlss.distributions.ZIPoisson.relu_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.relu_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h6 id="xgboostlss.distributions.ZIPoisson.sigmoid_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.sigmoid_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.softmax_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.softmax_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.softplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.softplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.softplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.softplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.squareplus_fn--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.squareplus_fn--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.ZIPoisson.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.squareplus_fn_df--arguments">Arguments</h6>
<p>predt: torch.tensor
    Predicted values.</p>
<h6 id="xgboostlss.distributions.ZIPoisson.squareplus_fn_df--returns">Returns</h6>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.distribution_utils" class="doc doc-heading">
            <code>distribution_utils</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.distribution_utils.DistributionClass" class="doc doc-heading">
            <code>DistributionClass</code>


</h4>


    <div class="doc doc-contents ">



        <p>Generic class that contains general functions for univariate distributions.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass--arguments">Arguments</h6>
<p>distribution: torch.distributions.Distribution
    PyTorch Distribution class.
univariate: bool
    Whether the distribution is univariate or multivariate.
discrete: bool
    Whether the support of the distribution is discrete or continuous.
n_dist_param: int
    Number of distributional parameters.
stabilization: str
    Stabilization method.
param_dict: Dict[str, Any]
    Dictionary that maps distributional parameters to their response scale.
distribution_arg_names: List
    List of distributional parameter names.
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.
tau: List
    List of expectiles. Only used for Expectile distributon.
penalize_crossing: bool
    Whether to include a penalty term to discourage crossing of expectiles. Only used for Expectile distribution.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DistributionClass</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generic class that contains general functions for univariate distributions.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    distribution: torch.distributions.Distribution</span>
<span class="sd">        PyTorch Distribution class.</span>
<span class="sd">    univariate: bool</span>
<span class="sd">        Whether the distribution is univariate or multivariate.</span>
<span class="sd">    discrete: bool</span>
<span class="sd">        Whether the support of the distribution is discrete or continuous.</span>
<span class="sd">    n_dist_param: int</span>
<span class="sd">        Number of distributional parameters.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method.</span>
<span class="sd">    param_dict: Dict[str, Any]</span>
<span class="sd">        Dictionary that maps distributional parameters to their response scale.</span>
<span class="sd">    distribution_arg_names: List</span>
<span class="sd">        List of distributional parameter names.</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    tau: List</span>
<span class="sd">        List of expectiles. Only used for Expectile distributon.</span>
<span class="sd">    penalize_crossing: bool</span>
<span class="sd">        Whether to include a penalty term to discourage crossing of expectiles. Only used for Expectile distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">distribution</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">univariate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">discrete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_dist_param</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">distribution_arg_names</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">tau</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">penalize_crossing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">univariate</span> <span class="o">=</span> <span class="n">univariate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span> <span class="o">=</span> <span class="n">discrete</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">=</span> <span class="n">n_dist_param</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">=</span> <span class="n">stabilization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">param_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="n">distribution_arg_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize</span> <span class="o">=</span> <span class="n">initialize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span> <span class="o">=</span> <span class="n">penalize_crossing</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function to estimate gradients and hessians of distributional parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: xgb.DMatrix</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        grad: np.ndarray</span>
<span class="sd">            Gradient.</span>
<span class="sd">        hess: np.ndarray</span>
<span class="sd">            Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Weights</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Use 1 as weight if no weights are specified</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate gradients and hessians</span>
        <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that evaluates the predictions using the specified loss function.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: xgb.DMatrix</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        name: str</span>
<span class="sd">            Name of the evaluation metric.</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate loss</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">loss_fn_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                             <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the loss for a given set of distributional parameters. Only used for calculating</span>
<span class="sd">        the loss for the start values.</span>

<span class="sd">        Parameter</span>
<span class="sd">        ---------</span>
<span class="sd">        params: torch.Tensor</span>
<span class="sd">            Distributional parameters.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Replace NaNs and infinity values with 0.5</span>
        <span class="n">nan_inf_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_idx</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>

        <span class="c1"># Transform parameters to response scale</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">response_fn</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="p">]</span>

        <span class="c1"># Specify Distribution and Loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                               <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the starting values for each distributional parameter.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Data from which starting values are calculated.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        start_values: np.ndarray</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert target to torch.tensor</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Initialize parameters</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)]</span>

        <span class="c1"># Specify optimizer</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">20</span><span class="p">]),</span> <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

        <span class="c1"># Define learning rate scheduler</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

        <span class="c1"># Define closure</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>

        <span class="c1"># Optimize parameters</span>
        <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
            <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="c1"># Get final loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Get start values</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)])</span>

        <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                        <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that returns the predicted parameters and the loss.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>
<span class="sd">        start_values: List</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        requires_grad: bool</span>
<span class="sd">            Whether to add to the computational graph or not.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        predt: List of torch.Tensors</span>
<span class="sd">            Predicted parameters.</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Predicted Parameters</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

        <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
        <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
        <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Convert to torch.tensor</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Predicted Parameters transformed to response scale</span>
        <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">response_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="p">]</span>

        <span class="c1"># Specify Distribution and Loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">predt_transformed</span><span class="p">))</span>
            <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
                <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_fit</span><span class="o">.</span><span class="n">rsample</span><span class="p">((</span><span class="mi">30</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">crps_score</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dist_samples</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="n">predt_transformed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that draws n_samples from a predicted distribution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt_params: pd.DataFrame</span>
<span class="sd">            pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">        n_samples: int</span>
<span class="sd">            Number of sample to draw from predicted response distribution.</span>
<span class="sd">        seed: int</span>
<span class="sd">            Manual seed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred_dist: pd.DataFrame</span>
<span class="sd">            DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="n">arg_name</span><span class="p">:</span> <span class="n">param</span> <span class="k">for</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">pred_params</span><span class="o">.</span><span class="n">T</span><span class="p">)}</span>
            <span class="n">dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_samples</span><span class="p">)</span>
            <span class="n">dist_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dist_samples</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">booster</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                     <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">,</span>
                     <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that predicts from the trained model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        booster : xgb.Booster</span>
<span class="sd">            Trained model.</span>
<span class="sd">        start_values : np.ndarray</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        data : xgb.DMatrix</span>
<span class="sd">            Data to predict from.</span>
<span class="sd">        pred_type : str</span>
<span class="sd">            Type of prediction:</span>
<span class="sd">            - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">            - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">            - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">            - &quot;expectiles&quot; returns the predicted expectiles.</span>
<span class="sd">        n_samples : int</span>
<span class="sd">            Number of samples to draw from the predicted distribution.</span>
<span class="sd">        quantiles : List[float]</span>
<span class="sd">            List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">        seed : int</span>
<span class="sd">            Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred : pd.DataFrame</span>
<span class="sd">            Predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set base_margin as starting point for each distributional parameter. Requires base_score=0 in parameters.</span>
        <span class="n">base_margin_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
        <span class="n">data</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_test</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

        <span class="n">predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">output_margin</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Transform predicted parameters to response scale</span>
        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">response_fun</span><span class="p">(</span>
                    <span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span>
                <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_params_predt</span><span class="p">)</span>
        <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span> <span class="ow">or</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;expectiles&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dist_params_predt</span>

        <span class="c1"># Draw samples from predicted response distribution</span>
        <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                            <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pred_samples_df</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
            <span class="c1"># Calculate quantiles from predicted response distribution</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
                <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pred_quant_df</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                       <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates gradients and hessians.</span>

<span class="sd">        Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">        Arguments:</span>
<span class="sd">        ---------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss.</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            List of predicted parameters.</span>
<span class="sd">        weights: np.ndarray</span>
<span class="sd">            Weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">        -------</span>
<span class="sd">        grad: torch.Tensor</span>
<span class="sd">            Gradients.</span>
<span class="sd">        hess: torch.Tensor</span>
<span class="sd">            Hessians.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
            <span class="c1"># Gradient and Hessian</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
            <span class="c1"># Gradient and Hessian</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

        <span class="c1"># Stabilization of Derivatives</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

        <span class="c1"># Reshape</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Weighting</span>
        <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
        <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that stabilizes Gradients and Hessians.</span>

<span class="sd">        As XGBoostLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important</span>
<span class="sd">        that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,</span>
<span class="sd">        the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.</span>
<span class="sd">        Another way to improve convergence might be to standardize the response variable. This is especially useful if the</span>
<span class="sd">        range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and</span>
<span class="sd">        the standardization of the response are not always advised but need to be carefully considered.</span>
<span class="sd">        Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_der : torch.Tensor</span>
<span class="sd">            Input derivative, either Gradient or Hessian.</span>
<span class="sd">        type: str</span>
<span class="sd">            Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        stab_der : torch.Tensor</span>
<span class="sd">            Stabilized Gradient or Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

        <span class="k">return</span> <span class="n">stab_der</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">crps_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y: torch.Tensor</span>
<span class="sd">            Response variable of shape (n_observations,1).</span>
<span class="sd">        yhat_dist: torch.Tensor</span>
<span class="sd">            Predicted samples of shape (n_samples, n_observations).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        crps: torch.Tensor</span>
<span class="sd">            CRPS score.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.</span>
<span class="sd">        Journal of the American Statistical Association. 102. 359-378.</span>

<span class="sd">        Source</span>
<span class="sd">        ------</span>
<span class="sd">        https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get the number of observations</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">yhat_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Sort the forecasts in ascending order</span>
        <span class="n">yhat_dist_sorted</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">yhat_dist</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Create temporary tensors</span>
        <span class="n">y_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">yhat_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># Loop over the predicted samples generated per observation</span>
        <span class="k">for</span> <span class="n">yhat</span> <span class="ow">in</span> <span class="n">yhat_dist_sorted</span><span class="p">:</span>
            <span class="n">yhat</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">yhat</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">yhat_cdf</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="p">(</span><span class="o">~</span><span class="n">flag</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="n">y_cdf</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">y_cdf</span> <span class="o">+=</span> <span class="n">flag</span>
            <span class="n">yhat_cdf</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span>
            <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">yhat</span>

        <span class="c1"># In case y_cdf == 0 after the loop</span>
        <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">crps</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">dist_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">candidate_distributions</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                    <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that selects the most suitable distribution among the candidate_distributions for the target variable,</span>
<span class="sd">        based on the NegLogLikelihood (lower is better).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Response variable.</span>
<span class="sd">        candidate_distributions: List</span>
<span class="sd">            List of candidate distributions.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations for the optimization.</span>
<span class="sd">        plot: bool</span>
<span class="sd">            If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">        figure_size: tuple</span>
<span class="sd">            Figure size of the density plot.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        fit_df: pd.DataFrame</span>
<span class="sd">            Dataframe with the loss values of the fitted candidate distributions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dist_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate distributions&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)):</span>
                <span class="n">dist_name</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution&quot;</span><span class="p">)</span>
                <span class="n">dist_sel</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dist_name</span><span class="p">)()</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                        <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span>
                         <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">]</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                        <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                         <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="n">dist_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate distributions completed&quot;</span><span class="p">)</span>
            <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dist_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;dist_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
                <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
                <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
                <span class="s2">&quot;the required dependencies.&quot;</span>
            <span class="p">)</span>
            <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

            <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

            <span class="c1"># Select best distribution</span>
            <span class="n">best_dist</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">dist</span> <span class="ow">in</span> <span class="n">candidate_distributions</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="n">dist</span>
                    <span class="k">break</span>
            <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="p">,</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])()</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

            <span class="c1"># Transform parameters to the response scale and draw samples</span>
            <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
                <span class="p">],</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span>
                                                      <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                                      <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

            <span class="c1"># Plot actual and fitted distribution</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_dist</span><span class="p">[</span><span class="s1">&#39;distribution&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.distribution_utils.DistributionClass.calculate_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the starting values for each distributional parameter.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.calculate_start_values--arguments">Arguments</h6>
<p>target: np.ndarray
    Data from which starting values are calculated.
max_iter: int
    Maximum number of iterations.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.calculate_start_values--returns">Returns</h6>
<p>loss: float
    Loss value.
start_values: np.ndarray
    Starting values for each distributional parameter.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                           <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the starting values for each distributional parameter.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Data from which starting values are calculated.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    start_values: np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Convert target to torch.tensor</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Initialize parameters</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)]</span>

    <span class="c1"># Specify optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">20</span><span class="p">]),</span> <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

    <span class="c1"># Define learning rate scheduler</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Define closure</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="c1"># Optimize parameters</span>
    <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
        <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># Get final loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Get start values</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)])</span>

    <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.distribution_utils.DistributionClass.compute_gradients_and_hessians" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Calculates gradients and hessians.</p>
<p>Output gradients and hessians have shape (n_samples*n_outputs, 1).</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.compute_gradients_and_hessians--arguments">Arguments:</h6>
<p>loss: torch.Tensor
    Loss.
predt: torch.Tensor
    List of predicted parameters.
weights: np.ndarray
    Weights.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.compute_gradients_and_hessians--returns">Returns:</h6>
<p>grad: torch.Tensor
    Gradients.
hess: torch.Tensor
    Hessians.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                   <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates gradients and hessians.</span>

<span class="sd">    Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    ---------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss.</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        List of predicted parameters.</span>
<span class="sd">    weights: np.ndarray</span>
<span class="sd">        Weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">    -------</span>
<span class="sd">    grad: torch.Tensor</span>
<span class="sd">        Gradients.</span>
<span class="sd">    hess: torch.Tensor</span>
<span class="sd">        Hessians.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
        <span class="c1"># Gradient and Hessian</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
        <span class="c1"># Gradient and Hessian</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

    <span class="c1"># Stabilization of Derivatives</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

    <span class="c1"># Reshape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Weighting</span>
    <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
    <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.distribution_utils.DistributionClass.crps_score" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">crps_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.crps_score--parameters">Parameters</h6>
<p>y: torch.Tensor
    Response variable of shape (n_observations,1).
yhat_dist: torch.Tensor
    Predicted samples of shape (n_samples, n_observations).</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.crps_score--returns">Returns</h6>
<p>crps: torch.Tensor
    CRPS score.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.crps_score--references">References</h6>
<p>Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.
Journal of the American Statistical Association. 102. 359-378.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.crps_score--source">Source</h6>
<p>https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">crps_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y: torch.Tensor</span>
<span class="sd">        Response variable of shape (n_observations,1).</span>
<span class="sd">    yhat_dist: torch.Tensor</span>
<span class="sd">        Predicted samples of shape (n_samples, n_observations).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    crps: torch.Tensor</span>
<span class="sd">        CRPS score.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.</span>
<span class="sd">    Journal of the American Statistical Association. 102. 359-378.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the number of observations</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">yhat_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Sort the forecasts in ascending order</span>
    <span class="n">yhat_dist_sorted</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">yhat_dist</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Create temporary tensors</span>
    <span class="n">y_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">yhat_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">crps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Loop over the predicted samples generated per observation</span>
    <span class="k">for</span> <span class="n">yhat</span> <span class="ow">in</span> <span class="n">yhat_dist_sorted</span><span class="p">:</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">yhat</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">yhat_cdf</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="p">(</span><span class="o">~</span><span class="n">flag</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="n">y_cdf</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">y_cdf</span> <span class="o">+=</span> <span class="n">flag</span>
        <span class="n">yhat_cdf</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span>
        <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">yhat</span>

    <span class="c1"># In case y_cdf == 0 after the loop</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">crps</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.distribution_utils.DistributionClass.dist_select" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">dist_select</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">candidate_distributions</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that selects the most suitable distribution among the candidate_distributions for the target variable,
based on the NegLogLikelihood (lower is better).</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.dist_select--parameters">Parameters</h6>
<p>target: np.ndarray
    Response variable.
candidate_distributions: List
    List of candidate distributions.
max_iter: int
    Maximum number of iterations for the optimization.
plot: bool
    If True, a density plot of the actual and fitted distribution is created.
figure_size: tuple
    Figure size of the density plot.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.dist_select--returns">Returns</h6>
<p>fit_df: pd.DataFrame
    Dataframe with the loss values of the fitted candidate distributions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">dist_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                <span class="n">candidate_distributions</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that selects the most suitable distribution among the candidate_distributions for the target variable,</span>
<span class="sd">    based on the NegLogLikelihood (lower is better).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Response variable.</span>
<span class="sd">    candidate_distributions: List</span>
<span class="sd">        List of candidate distributions.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations for the optimization.</span>
<span class="sd">    plot: bool</span>
<span class="sd">        If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">    figure_size: tuple</span>
<span class="sd">        Figure size of the density plot.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fit_df: pd.DataFrame</span>
<span class="sd">        Dataframe with the loss values of the fitted candidate distributions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dist_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate distributions&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)):</span>
            <span class="n">dist_name</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution&quot;</span><span class="p">)</span>
            <span class="n">dist_sel</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dist_name</span><span class="p">)()</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                    <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span>
                     <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">]</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                    <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                     <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="n">dist_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate distributions completed&quot;</span><span class="p">)</span>
        <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dist_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;dist_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

        <span class="c1"># Select best distribution</span>
        <span class="n">best_dist</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">dist</span> <span class="ow">in</span> <span class="n">candidate_distributions</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">dist</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="n">dist</span>
                <span class="k">break</span>
        <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="p">,</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])()</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

        <span class="c1"># Transform parameters to the response scale and draw samples</span>
        <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span>
                                                  <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                                  <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

        <span class="c1"># Plot actual and fitted distribution</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_dist</span><span class="p">[</span><span class="s1">&#39;distribution&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.distribution_utils.DistributionClass.draw_samples" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that draws n_samples from a predicted distribution.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.draw_samples--arguments">Arguments</h6>
<p>predt_params: pd.DataFrame
    pd.DataFrame with predicted distributional parameters.
n_samples: int
    Number of sample to draw from predicted response distribution.
seed: int
    Manual seed.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.draw_samples--returns">Returns</h6>
<p>pred_dist: pd.DataFrame
    DataFrame with n_samples drawn from predicted response distribution.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that draws n_samples from a predicted distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt_params: pd.DataFrame</span>
<span class="sd">        pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">    n_samples: int</span>
<span class="sd">        Number of sample to draw from predicted response distribution.</span>
<span class="sd">    seed: int</span>
<span class="sd">        Manual seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred_dist: pd.DataFrame</span>
<span class="sd">        DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="n">arg_name</span><span class="p">:</span> <span class="n">param</span> <span class="k">for</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">pred_params</span><span class="o">.</span><span class="n">T</span><span class="p">)}</span>
        <span class="n">dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_samples</span><span class="p">)</span>
        <span class="n">dist_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dist_samples</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.distribution_utils.DistributionClass.get_params_loss" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns the predicted parameters and the loss.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.get_params_loss--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
target: torch.Tensor
    Target values.
start_values: List
    Starting values for each distributional parameter.
requires_grad: bool
    Whether to add to the computational graph or not.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.get_params_loss--returns">Returns</h6>
<p>predt: List of torch.Tensors
    Predicted parameters.
loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns the predicted parameters and the loss.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>
<span class="sd">    start_values: List</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    requires_grad: bool</span>
<span class="sd">        Whether to add to the computational graph or not.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: List of torch.Tensors</span>
<span class="sd">        Predicted parameters.</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Predicted Parameters</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

    <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
    <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
    <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Convert to torch.tensor</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Predicted Parameters transformed to response scale</span>
    <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">response_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="p">]</span>

    <span class="c1"># Specify Distribution and Loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">predt_transformed</span><span class="p">))</span>
        <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_fit</span><span class="o">.</span><span class="n">rsample</span><span class="p">((</span><span class="mi">30</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">crps_score</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dist_samples</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="n">predt_transformed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.distribution_utils.DistributionClass.loss_fn_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the loss for a given set of distributional parameters. Only used for calculating
the loss for the start values.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.loss_fn_start_values--parameter">Parameter</h6>
<p>params: torch.Tensor
    Distributional parameters.
target: torch.Tensor
    Target values.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.loss_fn_start_values--returns">Returns</h6>
<p>loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">loss_fn_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the loss for a given set of distributional parameters. Only used for calculating</span>
<span class="sd">    the loss for the start values.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    params: torch.Tensor</span>
<span class="sd">        Distributional parameters.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Replace NaNs and infinity values with 0.5</span>
    <span class="n">nan_inf_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_idx</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>

    <span class="c1"># Transform parameters to response scale</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">response_fn</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
    <span class="p">]</span>

    <span class="c1"># Specify Distribution and Loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">penalize_crossing</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tau</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.distribution_utils.DistributionClass.metric_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">metric_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that evaluates the predictions using the specified loss function.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.metric_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: xgb.DMatrix
    Data used for training.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.metric_fn--returns">Returns</h6>
<p>name: str
    Name of the evaluation metric.
loss: float
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that evaluates the predictions using the specified loss function.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: xgb.DMatrix</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    name: str</span>
<span class="sd">        Name of the evaluation metric.</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate loss</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.distribution_utils.DistributionClass.objective_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">objective_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function to estimate gradients and hessians of distributional parameters.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.objective_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: xgb.DMatrix
    Data used for training.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.objective_fn--returns">Returns</h6>
<p>grad: np.ndarray
    Gradient.
hess: np.ndarray
    Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to estimate gradients and hessians of distributional parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: xgb.DMatrix</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grad: np.ndarray</span>
<span class="sd">        Gradient.</span>
<span class="sd">    hess: np.ndarray</span>
<span class="sd">        Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Weights</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Use 1 as weight if no weights are specified</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate gradients and hessians</span>
    <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.distribution_utils.DistributionClass.predict_dist" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict_dist</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">pred_type</span><span class="o">=</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that predicts from the trained model.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.predict_dist--arguments">Arguments</h6>
<p>booster : xgb.Booster
    Trained model.
start_values : np.ndarray
    Starting values for each distributional parameter.
data : xgb.DMatrix
    Data to predict from.
pred_type : str
    Type of prediction:
    - "samples" draws n_samples from the predicted distribution.
    - "quantiles" calculates the quantiles from the predicted distribution.
    - "parameters" returns the predicted distributional parameters.
    - "expectiles" returns the predicted expectiles.
n_samples : int
    Number of samples to draw from the predicted distribution.
quantiles : List[float]
    List of quantiles to calculate from the predicted distribution.
seed : int
    Seed for random number generator used to draw samples from the predicted distribution.</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.predict_dist--returns">Returns</h6>
<p>pred : pd.DataFrame
    Predictions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">booster</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                 <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">,</span>
                 <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that predicts from the trained model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    booster : xgb.Booster</span>
<span class="sd">        Trained model.</span>
<span class="sd">    start_values : np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    data : xgb.DMatrix</span>
<span class="sd">        Data to predict from.</span>
<span class="sd">    pred_type : str</span>
<span class="sd">        Type of prediction:</span>
<span class="sd">        - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">        - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">        - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">        - &quot;expectiles&quot; returns the predicted expectiles.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to draw from the predicted distribution.</span>
<span class="sd">    quantiles : List[float]</span>
<span class="sd">        List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred : pd.DataFrame</span>
<span class="sd">        Predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set base_margin as starting point for each distributional parameter. Requires base_score=0 in parameters.</span>
    <span class="n">base_margin_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
    <span class="n">data</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_test</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

    <span class="n">predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">output_margin</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Transform predicted parameters to response scale</span>
    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">response_fun</span><span class="p">(</span>
                <span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span>
            <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_params_predt</span><span class="p">)</span>
    <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span> <span class="ow">or</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;expectiles&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dist_params_predt</span>

    <span class="c1"># Draw samples from predicted response distribution</span>
    <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                        <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pred_samples_df</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
        <span class="c1"># Calculate quantiles from predicted response distribution</span>
        <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred_quant_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.distribution_utils.DistributionClass.stabilize_derivative" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;MAD&#39;</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that stabilizes Gradients and Hessians.</p>
<p>As XGBoostLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important
that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,
the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.
Another way to improve convergence might be to standardize the response variable. This is especially useful if the
range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and
the standardization of the response are not always advised but need to be carefully considered.
Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.stabilize_derivative--parameters">Parameters</h6>
<p>input_der : torch.Tensor
    Input derivative, either Gradient or Hessian.
type: str
    Stabilization method. Can be either "None", "MAD" or "L2".</p>
<h6 id="xgboostlss.distributions.distribution_utils.DistributionClass.stabilize_derivative--returns">Returns</h6>
<p>stab_der : torch.Tensor
    Stabilized Gradient or Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that stabilizes Gradients and Hessians.</span>

<span class="sd">    As XGBoostLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important</span>
<span class="sd">    that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,</span>
<span class="sd">    the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.</span>
<span class="sd">    Another way to improve convergence might be to standardize the response variable. This is especially useful if the</span>
<span class="sd">    range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and</span>
<span class="sd">    the standardization of the response are not always advised but need to be carefully considered.</span>
<span class="sd">    Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_der : torch.Tensor</span>
<span class="sd">        Input derivative, either Gradient or Hessian.</span>
<span class="sd">    type: str</span>
<span class="sd">        Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    stab_der : torch.Tensor</span>
<span class="sd">        Stabilized Gradient or Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">stab_der</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.flow_utils" class="doc doc-heading">
            <code>flow_utils</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass" class="doc doc-heading">
            <code>NormalizingFlowClass</code>


</h4>


    <div class="doc doc-contents ">



        <p>Generic class that contains general functions for normalizing flows.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass--arguments">Arguments</h6>
<p>base_dist: torch.distributions.Distribution
    PyTorch Distribution class. Currently only Normal is supported.
flow_transform: Transform
    Specify the normalizing flow transform.
count_bins: Optional[int]
    The number of segments comprising the spline. Only used if flow_transform is Spline.
bound: Optional[float]
    The quantity "K" determining the bounding box, [-K,K] x [-K,K] of the spline. By adjusting the
    "K" value, you can control the size of the bounding box and consequently control the range of inputs that
    the spline transform operates on. Larger values of "K" will result in a wider valid range for the spline
    transformation, while smaller values will restrict the valid range to a smaller region. Should be chosen
    based on the range of the data. Only used if flow_transform is Spline.
order: Optional[str]
    The order of the spline. Options are "linear" or "quadratic". Only used if flow_transform is Spline.
n_dist_param: int
    Number of parameters.
param_dict: Dict[str, Any]
    Dictionary that maps parameters to their response scale.
distribution_arg_names: List
    List of distributional parameter names.
target_transform: Transform
    Specify the target transform.
discrete: bool
    Whether the target is discrete or not.
univariate: bool
    Whether the distribution is univariate or multivariate.
stabilization: str
    Stabilization method. Options are "None", "MAD" or "L2".
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood) or "crps" (continuous ranked probability score).
    Note that if "crps" is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.
    Hence, using the CRPS disregards any variation in the curvature of the loss function.
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">NormalizingFlowClass</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generic class that contains general functions for normalizing flows.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    base_dist: torch.distributions.Distribution</span>
<span class="sd">        PyTorch Distribution class. Currently only Normal is supported.</span>
<span class="sd">    flow_transform: Transform</span>
<span class="sd">        Specify the normalizing flow transform.</span>
<span class="sd">    count_bins: Optional[int]</span>
<span class="sd">        The number of segments comprising the spline. Only used if flow_transform is Spline.</span>
<span class="sd">    bound: Optional[float]</span>
<span class="sd">        The quantity &quot;K&quot; determining the bounding box, [-K,K] x [-K,K] of the spline. By adjusting the</span>
<span class="sd">        &quot;K&quot; value, you can control the size of the bounding box and consequently control the range of inputs that</span>
<span class="sd">        the spline transform operates on. Larger values of &quot;K&quot; will result in a wider valid range for the spline</span>
<span class="sd">        transformation, while smaller values will restrict the valid range to a smaller region. Should be chosen</span>
<span class="sd">        based on the range of the data. Only used if flow_transform is Spline.</span>
<span class="sd">    order: Optional[str]</span>
<span class="sd">        The order of the spline. Options are &quot;linear&quot; or &quot;quadratic&quot;. Only used if flow_transform is Spline.</span>
<span class="sd">    n_dist_param: int</span>
<span class="sd">        Number of parameters.</span>
<span class="sd">    param_dict: Dict[str, Any]</span>
<span class="sd">        Dictionary that maps parameters to their response scale.</span>
<span class="sd">    distribution_arg_names: List</span>
<span class="sd">        List of distributional parameter names.</span>
<span class="sd">    target_transform: Transform</span>
<span class="sd">        Specify the target transform.</span>
<span class="sd">    discrete: bool</span>
<span class="sd">        Whether the target is discrete or not.</span>
<span class="sd">    univariate: bool</span>
<span class="sd">        Whether the distribution is univariate or multivariate.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method. Options are &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood) or &quot;crps&quot; (continuous ranked probability score).</span>
<span class="sd">        Note that if &quot;crps&quot; is used, the Hessian is set to 1, as the current CRPS version is not twice differentiable.</span>
<span class="sd">        Hence, using the CRPS disregards any variation in the curvature of the loss function.</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_dist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">flow_transform</span><span class="p">:</span> <span class="n">Transform</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">count_bins</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
                 <span class="n">bound</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span>
                 <span class="n">order</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;quadratic&quot;</span><span class="p">,</span>
                 <span class="n">n_dist_param</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">distribution_arg_names</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">target_transform</span><span class="p">:</span> <span class="n">Transform</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">discrete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">univariate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span> <span class="o">=</span> <span class="n">base_dist</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flow_transform</span> <span class="o">=</span> <span class="n">flow_transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span> <span class="o">=</span> <span class="n">count_bins</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bound</span> <span class="o">=</span> <span class="n">bound</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">=</span> <span class="n">order</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">=</span> <span class="n">n_dist_param</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">param_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="n">distribution_arg_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_transform</span> <span class="o">=</span> <span class="n">target_transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span> <span class="o">=</span> <span class="n">discrete</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">univariate</span> <span class="o">=</span> <span class="n">univariate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">=</span> <span class="n">stabilization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize</span> <span class="o">=</span> <span class="n">initialize</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function to estimate gradients and hessians of normalizing flow parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: xgb.DMatrix</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        grad: np.ndarray</span>
<span class="sd">            Gradient.</span>
<span class="sd">        hess: np.ndarray</span>
<span class="sd">            Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Weights</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Use 1 as weight if no weights are specified</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate gradients and hessians</span>
        <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">)</span>
        <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that evaluates the predictions using the specified loss function.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: xgb.DMatrix</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        name: str</span>
<span class="sd">            Name of the evaluation metric.</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate loss</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                               <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates starting values for each parameter.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Data from which starting values are calculated.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        start_values: np.ndarray</span>
<span class="sd">            Starting values for each parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert target to torch.tensor</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Create Normalizing Flow</span>
        <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Specify optimizer</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                          <span class="n">lr</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                          <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">50</span><span class="p">]),</span>
                          <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

        <span class="c1"># Define learning rate scheduler</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

        <span class="c1"># Define closure</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">flow_dist</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>

        <span class="c1"># Optimize parameters</span>
        <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-5</span>           <span class="c1"># Tolerance level for loss change</span>
        <span class="n">patience</span> <span class="o">=</span> <span class="mi">5</span>               <span class="c1"># Patience level for loss change</span>
        <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
        <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
            <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="c1"># Stopping criterion (no improvement in loss)</span>
            <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">best_loss</span> <span class="o">-</span> <span class="n">tolerance</span><span class="p">:</span>
                <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">epochs_without_change</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">epochs_without_change</span> <span class="o">&gt;=</span> <span class="n">patience</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="c1"># Get final loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Get start values</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">param</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">start_values</span><span class="p">])</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                        <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that returns the predicted parameters and the loss.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>
<span class="sd">        start_values: List</span>
<span class="sd">            Starting values for each parameter.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            Predicted parameters.</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Reshape Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Predicted Parameters</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

        <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
        <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
        <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Convert to torch.tensor</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Specify Normalizing Flow</span>
        <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Replace parameters with estimated ones</span>
        <span class="n">params</span><span class="p">,</span> <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">flow_dist</span><span class="p">)</span>

        <span class="c1"># Calculate loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">flow_dist</span><span class="o">.</span><span class="n">rsample</span><span class="p">((</span><span class="mi">30</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">crps_score</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dist_samples</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_spline_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transform</span><span class="p">:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that constructs a Normalizing Flow.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        input_dim: int</span>
<span class="sd">            Input dimension.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        spline_flow: Transform</span>
<span class="sd">            Normalizing Flow.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Create flow distribution (currently only Normal)</span>
        <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_dim</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
        <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>

        <span class="c1"># Create Spline Transform</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
        <span class="n">spline_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flow_transform</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span>
                                               <span class="n">count_bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span>
                                               <span class="n">bound</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bound</span><span class="p">,</span>
                                               <span class="n">order</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">order</span><span class="p">)</span>

        <span class="c1"># Create Normalizing Flow</span>
        <span class="n">spline_flow</span> <span class="o">=</span> <span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">flow_dist</span><span class="p">,</span> <span class="p">[</span><span class="n">spline_transform</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_transform</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">spline_flow</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">replace_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                           <span class="n">flow_dist</span><span class="p">:</span> <span class="n">Transform</span><span class="p">,</span>
                           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">Transform</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Replace parameters with estimated ones.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        params: torch.Tensor</span>
<span class="sd">            Estimated parameters.</span>
<span class="sd">        flow_dist: Transform</span>
<span class="sd">            Normalizing Flow.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        params_list: List</span>
<span class="sd">            List of estimated parameters.</span>
<span class="sd">        flow_dist: Transform</span>
<span class="sd">            Normalizing Flow with estimated parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Split parameters into list</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="s2">&quot;quadratic&quot;</span><span class="p">:</span>
            <span class="n">params_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="n">params</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
            <span class="n">params_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="n">params</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">],</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Replace parameters</span>
        <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">new_value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">params_list</span><span class="p">):</span>
            <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">new_value</span>

        <span class="c1"># Get parameters (including require_grad=True)</span>
        <span class="n">params_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">params_list</span><span class="p">,</span> <span class="n">flow_dist</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that draws n_samples from a predicted distribution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt_params: pd.DataFrame</span>
<span class="sd">            pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">        n_samples: int</span>
<span class="sd">            Number of sample to draw from predicted response distribution.</span>
<span class="sd">        seed: int</span>
<span class="sd">            Manual seed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred_dist: pd.DataFrame</span>
<span class="sd">            DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># Specify Normalizing Flow</span>
        <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="n">flow_dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">pred_params</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Replace parameters with estimated ones</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">flow_dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">pred_params</span><span class="p">,</span> <span class="n">flow_dist_pred</span><span class="p">)</span>

        <span class="c1"># Draw samples</span>
        <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">flow_dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">flow_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flow_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">flow_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">flow_samples</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">booster</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                     <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">,</span>
                     <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that predicts from the trained model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        booster : xgb.Booster</span>
<span class="sd">            Trained model.</span>
<span class="sd">        start_values : np.ndarray</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        data : xgb.DMatrix</span>
<span class="sd">            Data to predict from.</span>
<span class="sd">        pred_type : str</span>
<span class="sd">            Type of prediction:</span>
<span class="sd">            - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">            - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">            - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">        n_samples : int</span>
<span class="sd">            Number of samples to draw from the predicted distribution.</span>
<span class="sd">        quantiles : List[float]</span>
<span class="sd">            List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">        seed : int</span>
<span class="sd">            Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred : pd.DataFrame</span>
<span class="sd">            Predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set base_margin as starting point for each distributional parameter. Requires base_score=0 in parameters.</span>
        <span class="n">base_margin_predt</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
        <span class="n">data</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_predt</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

        <span class="c1"># Predict distributional parameters</span>
        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">output_margin</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

        <span class="c1"># Draw samples from predicted response distribution</span>
        <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                            <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dist_params_predt</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pred_samples_df</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
            <span class="c1"># Calculate quantiles from predicted response distribution</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
                <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pred_quant_df</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                       <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates gradients and hessians.</span>

<span class="sd">        Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">        Arguments:</span>
<span class="sd">        ---------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss.</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            List of predicted parameters.</span>
<span class="sd">        weights: np.ndarray</span>
<span class="sd">            Weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">        -------</span>
<span class="sd">        grad: torch.Tensor</span>
<span class="sd">            Gradients.</span>
<span class="sd">        hess: torch.Tensor</span>
<span class="sd">            Hessians.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
            <span class="c1"># Gradient and Hessian</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
            <span class="c1"># Gradient and Hessian</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

        <span class="c1"># Stabilization of Derivatives</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

        <span class="c1"># Reshape</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Weighting</span>
        <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
        <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that stabilizes Gradients and Hessians.</span>

<span class="sd">        Since parameters are estimated by optimizing Gradients and Hessians, it is important that these are comparable</span>
<span class="sd">        in magnitude for all parameters. Due to imbalances regarding the ranges, the estimation might become unstable</span>
<span class="sd">        so that it does not converge (or converge very slowly) to the optimal solution. Another way to improve</span>
<span class="sd">        convergence might be to standardize the response variable. This is especially useful if the range of the</span>
<span class="sd">        response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and the</span>
<span class="sd">        standardization of the response are not always advised but need to be carefully considered.</span>

<span class="sd">        Source</span>
<span class="sd">        ---------</span>
<span class="sd">        https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        input_der : torch.Tensor</span>
<span class="sd">            Input derivative, either Gradient or Hessian.</span>
<span class="sd">        type: str</span>
<span class="sd">            Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">        Returns</span>
<span class="sd">        ---------</span>
<span class="sd">        stab_der : torch.Tensor</span>
<span class="sd">            Stabilized Gradient or Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

        <span class="k">return</span> <span class="n">stab_der</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">crps_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        y: torch.Tensor</span>
<span class="sd">            Response variable of shape (n_observations,1).</span>
<span class="sd">        yhat_dist: torch.Tensor</span>
<span class="sd">            Predicted samples of shape (n_samples, n_observations).</span>

<span class="sd">        Returns</span>
<span class="sd">        ---------</span>
<span class="sd">        crps: torch.Tensor</span>
<span class="sd">            CRPS score.</span>

<span class="sd">        References</span>
<span class="sd">        ---------</span>
<span class="sd">        Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.</span>
<span class="sd">        Journal of the American Statistical Association. 102. 359-378.</span>

<span class="sd">        Source</span>
<span class="sd">        ---------</span>
<span class="sd">        https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get the number of observations</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">yhat_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Sort the forecasts in ascending order</span>
        <span class="n">yhat_dist_sorted</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">yhat_dist</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Create temporary tensors</span>
        <span class="n">y_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">yhat_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># Loop over the predicted samples generated per observation</span>
        <span class="k">for</span> <span class="n">yhat</span> <span class="ow">in</span> <span class="n">yhat_dist_sorted</span><span class="p">:</span>
            <span class="n">yhat</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">yhat</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">yhat_cdf</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">crps</span> <span class="o">+=</span> <span class="p">(</span><span class="o">~</span><span class="n">flag</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="n">y_cdf</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">y_cdf</span> <span class="o">+=</span> <span class="n">flag</span>
            <span class="n">yhat_cdf</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span>
            <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">yhat</span>

        <span class="c1"># In case y_cdf == 0 after the loop</span>
        <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">crps</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">flow_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">candidate_flows</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                    <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that selects the most suitable normalizing flow specification among the candidate_flow for the</span>
<span class="sd">        target variable, based on the NegLogLikelihood (lower is better).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Response variable.</span>
<span class="sd">        candidate_flows: List</span>
<span class="sd">            List of candidate normalizing flow specifications.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations for the optimization.</span>
<span class="sd">        plot: bool</span>
<span class="sd">            If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">        figure_size: tuple</span>
<span class="sd">            Figure size of the density plot.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        fit_df: pd.DataFrame</span>
<span class="sd">            Dataframe with the loss values of the fitted normalizing flow.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">flow_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_flows</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate normalizing flows&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="n">candidate_flows</span><span class="p">:</span>
                <span class="n">flow_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;&#39;&gt;&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">flow_spec</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(count_bins: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">count_bins</span><span class="si">}</span><span class="s2">, order: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">order</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="n">flow_name</span> <span class="o">=</span> <span class="n">flow_name</span> <span class="o">+</span> <span class="n">flow_spec</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">flow_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">flow_sel</span> <span class="o">=</span> <span class="n">flow</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">flow_sel</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span>
                         <span class="s2">&quot;NormFlow&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">]</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">flow_sel</span><span class="si">}</span><span class="s2"> NormFlow: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                         <span class="s2">&quot;NormFlow&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow_sel</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="n">flow_sel</span><span class="o">.</span><span class="n">n_dist_param</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="n">flow_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate normalizing flows completed&quot;</span><span class="p">)</span>
            <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">flow_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;flow_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
                <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
                <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
                <span class="s2">&quot;the required dependencies.&quot;</span>
            <span class="p">)</span>
            <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

            <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

            <span class="c1"># Select normalizing flow with the lowest loss</span>
            <span class="n">best_flow</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="n">candidate_flows</span><span class="p">:</span>
                <span class="n">flow_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;&#39;&gt;&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">flow_spec</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(count_bins: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">count_bins</span><span class="si">}</span><span class="s2">, order: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">order</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="n">flow_name</span> <span class="o">=</span> <span class="n">flow_name</span> <span class="o">+</span> <span class="n">flow_spec</span>
                <span class="k">if</span> <span class="n">flow_name</span> <span class="o">==</span> <span class="n">best_flow</span><span class="p">[</span><span class="s2">&quot;NormFlow&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="n">best_flow_sel</span> <span class="o">=</span> <span class="n">flow</span>
                    <span class="k">break</span>

            <span class="c1"># Draw samples from distribution</span>
            <span class="n">flow_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_flow</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">flow_dist_sel</span> <span class="o">=</span> <span class="n">best_flow_sel</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">flow_dist_sel</span> <span class="o">=</span> <span class="n">best_flow_sel</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">flow_params</span><span class="p">,</span> <span class="n">flow_dist_sel</span><span class="p">)</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
            <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">flow_dist_sel</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

            <span class="c1"># Plot actual and fitted distribution</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">flow_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_flow</span><span class="p">[</span><span class="s1">&#39;NormFlow&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.calculate_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates starting values for each parameter.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.calculate_start_values--arguments">Arguments</h6>
<p>target: np.ndarray
    Data from which starting values are calculated.
max_iter: int
    Maximum number of iterations.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.calculate_start_values--returns">Returns</h6>
<p>loss: float
    Loss value.
start_values: np.ndarray
    Starting values for each parameter.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                           <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates starting values for each parameter.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Data from which starting values are calculated.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    start_values: np.ndarray</span>
<span class="sd">        Starting values for each parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Convert target to torch.tensor</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Create Normalizing Flow</span>
    <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Specify optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                      <span class="n">lr</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                      <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">50</span><span class="p">]),</span>
                      <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

    <span class="c1"># Define learning rate scheduler</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

    <span class="c1"># Define closure</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">flow_dist</span><span class="o">.</span><span class="n">clear_cache</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="c1"># Optimize parameters</span>
    <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-5</span>           <span class="c1"># Tolerance level for loss change</span>
    <span class="n">patience</span> <span class="o">=</span> <span class="mi">5</span>               <span class="c1"># Patience level for loss change</span>
    <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
    <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
        <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="c1"># Stopping criterion (no improvement in loss)</span>
        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">best_loss</span> <span class="o">-</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">epochs_without_change</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">epochs_without_change</span> <span class="o">&gt;=</span> <span class="n">patience</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="c1"># Get final loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Get start values</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">param</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">start_values</span><span class="p">])</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.compute_gradients_and_hessians" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Calculates gradients and hessians.</p>
<p>Output gradients and hessians have shape (n_samples*n_outputs, 1).</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.compute_gradients_and_hessians--arguments">Arguments:</h6>
<p>loss: torch.Tensor
    Loss.
predt: torch.Tensor
    List of predicted parameters.
weights: np.ndarray
    Weights.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.compute_gradients_and_hessians--returns">Returns:</h6>
<p>grad: torch.Tensor
    Gradients.
hess: torch.Tensor
    Hessians.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                   <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates gradients and hessians.</span>

<span class="sd">    Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    ---------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss.</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        List of predicted parameters.</span>
<span class="sd">    weights: np.ndarray</span>
<span class="sd">        Weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">    -------</span>
<span class="sd">    grad: torch.Tensor</span>
<span class="sd">        Gradients.</span>
<span class="sd">    hess: torch.Tensor</span>
<span class="sd">        Hessians.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
        <span class="c1"># Gradient and Hessian</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
        <span class="c1"># Gradient and Hessian</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

    <span class="c1"># Stabilization of Derivatives</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

    <span class="c1"># Reshape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Weighting</span>
    <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
    <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.create_spline_flow" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that constructs a Normalizing Flow.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.create_spline_flow--arguments">Arguments</h6>
<p>input_dim: int
    Input dimension.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.create_spline_flow--returns">Returns</h6>
<p>spline_flow: Transform
    Normalizing Flow.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">create_spline_flow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                       <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transform</span><span class="p">:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that constructs a Normalizing Flow.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    input_dim: int</span>
<span class="sd">        Input dimension.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    spline_flow: Transform</span>
<span class="sd">        Normalizing Flow.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Create flow distribution (currently only Normal)</span>
    <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_dim</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_dim</span><span class="p">)</span>
    <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>

    <span class="c1"># Create Spline Transform</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">spline_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flow_transform</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span>
                                           <span class="n">count_bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span>
                                           <span class="n">bound</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bound</span><span class="p">,</span>
                                           <span class="n">order</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">order</span><span class="p">)</span>

    <span class="c1"># Create Normalizing Flow</span>
    <span class="n">spline_flow</span> <span class="o">=</span> <span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">flow_dist</span><span class="p">,</span> <span class="p">[</span><span class="n">spline_transform</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_transform</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">spline_flow</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.crps_score" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">crps_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.crps_score--arguments">Arguments</h6>
<p>y: torch.Tensor
    Response variable of shape (n_observations,1).
yhat_dist: torch.Tensor
    Predicted samples of shape (n_samples, n_observations).</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.crps_score--returns">Returns</h6>
<p>crps: torch.Tensor
    CRPS score.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.crps_score--references">References</h6>
<p>Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.
Journal of the American Statistical Association. 102. 359-378.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.crps_score--source">Source</h6>
<p>https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">crps_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">yhat_dist</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the Continuous Ranked Probability Score (CRPS) for a given set of predicted samples.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    y: torch.Tensor</span>
<span class="sd">        Response variable of shape (n_observations,1).</span>
<span class="sd">    yhat_dist: torch.Tensor</span>
<span class="sd">        Predicted samples of shape (n_samples, n_observations).</span>

<span class="sd">    Returns</span>
<span class="sd">    ---------</span>
<span class="sd">    crps: torch.Tensor</span>
<span class="sd">        CRPS score.</span>

<span class="sd">    References</span>
<span class="sd">    ---------</span>
<span class="sd">    Gneiting, Tilmann &amp; Raftery, Adrian. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation.</span>
<span class="sd">    Journal of the American Statistical Association. 102. 359-378.</span>

<span class="sd">    Source</span>
<span class="sd">    ---------</span>
<span class="sd">    https://github.com/elephaint/pgbm/blob/main/pgbm/torch/pgbm_dist.py#L549</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get the number of observations</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">yhat_dist</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Sort the forecasts in ascending order</span>
    <span class="n">yhat_dist_sorted</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">yhat_dist</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Create temporary tensors</span>
    <span class="n">y_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">yhat_cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">crps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Loop over the predicted samples generated per observation</span>
    <span class="k">for</span> <span class="n">yhat</span> <span class="ow">in</span> <span class="n">yhat_dist_sorted</span><span class="p">:</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">yhat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">yhat</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">yhat_cdf</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">crps</span> <span class="o">+=</span> <span class="p">(</span><span class="o">~</span><span class="n">flag</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">yhat_prev</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">yhat_cdf</span> <span class="o">-</span> <span class="n">y_cdf</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">y_cdf</span> <span class="o">+=</span> <span class="n">flag</span>
        <span class="n">yhat_cdf</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">n_samples</span>
        <span class="n">yhat_prev</span> <span class="o">=</span> <span class="n">yhat</span>

    <span class="c1"># In case y_cdf == 0 after the loop</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_cdf</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">crps</span> <span class="o">+=</span> <span class="n">flag</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">crps</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.draw_samples" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that draws n_samples from a predicted distribution.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.draw_samples--arguments">Arguments</h6>
<p>predt_params: pd.DataFrame
    pd.DataFrame with predicted distributional parameters.
n_samples: int
    Number of sample to draw from predicted response distribution.
seed: int
    Manual seed.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.draw_samples--returns">Returns</h6>
<p>pred_dist: pd.DataFrame
    DataFrame with n_samples drawn from predicted response distribution.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that draws n_samples from a predicted distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt_params: pd.DataFrame</span>
<span class="sd">        pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">    n_samples: int</span>
<span class="sd">        Number of sample to draw from predicted response distribution.</span>
<span class="sd">    seed: int</span>
<span class="sd">        Manual seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred_dist: pd.DataFrame</span>
<span class="sd">        DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Specify Normalizing Flow</span>
    <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">flow_dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">pred_params</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Replace parameters with estimated ones</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">flow_dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">pred_params</span><span class="p">,</span> <span class="n">flow_dist_pred</span><span class="p">)</span>

    <span class="c1"># Draw samples</span>
    <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">flow_dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">flow_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flow_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
        <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">flow_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">flow_samples</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.flow_select" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">flow_select</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">candidate_flows</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that selects the most suitable normalizing flow specification among the candidate_flow for the
target variable, based on the NegLogLikelihood (lower is better).</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.flow_select--parameters">Parameters</h6>
<p>target: np.ndarray
    Response variable.
candidate_flows: List
    List of candidate normalizing flow specifications.
max_iter: int
    Maximum number of iterations for the optimization.
plot: bool
    If True, a density plot of the actual and fitted distribution is created.
figure_size: tuple
    Figure size of the density plot.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.flow_select--returns">Returns</h6>
<p>fit_df: pd.DataFrame
    Dataframe with the loss values of the fitted normalizing flow.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">flow_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                <span class="n">candidate_flows</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that selects the most suitable normalizing flow specification among the candidate_flow for the</span>
<span class="sd">    target variable, based on the NegLogLikelihood (lower is better).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Response variable.</span>
<span class="sd">    candidate_flows: List</span>
<span class="sd">        List of candidate normalizing flow specifications.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations for the optimization.</span>
<span class="sd">    plot: bool</span>
<span class="sd">        If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">    figure_size: tuple</span>
<span class="sd">        Figure size of the density plot.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fit_df: pd.DataFrame</span>
<span class="sd">        Dataframe with the loss values of the fitted normalizing flow.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">flow_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_flows</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate normalizing flows&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="n">candidate_flows</span><span class="p">:</span>
            <span class="n">flow_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;&#39;&gt;&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">flow_spec</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(count_bins: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">count_bins</span><span class="si">}</span><span class="s2">, order: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">order</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="n">flow_name</span> <span class="o">=</span> <span class="n">flow_name</span> <span class="o">+</span> <span class="n">flow_spec</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">flow_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">flow_sel</span> <span class="o">=</span> <span class="n">flow</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">flow_sel</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span>
                     <span class="s2">&quot;NormFlow&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">]</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">flow_sel</span><span class="si">}</span><span class="s2"> NormFlow: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                     <span class="s2">&quot;NormFlow&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow_sel</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="n">flow_sel</span><span class="o">.</span><span class="n">n_dist_param</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="n">flow_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate normalizing flows completed&quot;</span><span class="p">)</span>
        <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">flow_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">flow_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;flow_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

        <span class="c1"># Select normalizing flow with the lowest loss</span>
        <span class="n">best_flow</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">flow</span> <span class="ow">in</span> <span class="n">candidate_flows</span><span class="p">:</span>
            <span class="n">flow_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">flow</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;&#39;&gt;&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">flow_spec</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;(count_bins: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">count_bins</span><span class="si">}</span><span class="s2">, order: </span><span class="si">{</span><span class="n">flow</span><span class="o">.</span><span class="n">order</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="n">flow_name</span> <span class="o">=</span> <span class="n">flow_name</span> <span class="o">+</span> <span class="n">flow_spec</span>
            <span class="k">if</span> <span class="n">flow_name</span> <span class="o">==</span> <span class="n">best_flow</span><span class="p">[</span><span class="s2">&quot;NormFlow&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">best_flow_sel</span> <span class="o">=</span> <span class="n">flow</span>
                <span class="k">break</span>

        <span class="c1"># Draw samples from distribution</span>
        <span class="n">flow_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_flow</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">flow_dist_sel</span> <span class="o">=</span> <span class="n">best_flow_sel</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">flow_dist_sel</span> <span class="o">=</span> <span class="n">best_flow_sel</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">flow_params</span><span class="p">,</span> <span class="n">flow_dist_sel</span><span class="p">)</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">flow_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">flow_dist_sel</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

        <span class="c1"># Plot actual and fitted distribution</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">flow_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_flow</span><span class="p">[</span><span class="s1">&#39;NormFlow&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.get_params_loss" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns the predicted parameters and the loss.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.get_params_loss--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
target: torch.Tensor
    Target values.
start_values: List
    Starting values for each parameter.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.get_params_loss--returns">Returns</h6>
<p>predt: torch.Tensor
    Predicted parameters.
loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns the predicted parameters and the loss.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>
<span class="sd">    start_values: List</span>
<span class="sd">        Starting values for each parameter.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        Predicted parameters.</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Reshape Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Predicted Parameters</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

    <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
    <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
    <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Convert to torch.tensor</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Specify Normalizing Flow</span>
    <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_spline_flow</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Replace parameters with estimated ones</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">flow_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replace_parameters</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">flow_dist</span><span class="p">)</span>

    <span class="c1"># Calculate loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;nll&quot;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">==</span> <span class="s2">&quot;crps&quot;</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">flow_dist</span><span class="o">.</span><span class="n">rsample</span><span class="p">((</span><span class="mi">30</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">crps_score</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dist_samples</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid loss function. Please select &#39;nll&#39; or &#39;crps&#39;.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.metric_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">metric_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that evaluates the predictions using the specified loss function.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.metric_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: xgb.DMatrix
    Data used for training.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.metric_fn--returns">Returns</h6>
<p>name: str
    Name of the evaluation metric.
loss: float
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that evaluates the predictions using the specified loss function.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: xgb.DMatrix</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    name: str</span>
<span class="sd">        Name of the evaluation metric.</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate loss</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.objective_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">objective_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function to estimate gradients and hessians of normalizing flow parameters.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.objective_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: xgb.DMatrix
    Data used for training.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.objective_fn--returns">Returns</h6>
<p>grad: np.ndarray
    Gradient.
hess: np.ndarray
    Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to estimate gradients and hessians of normalizing flow parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: xgb.DMatrix</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grad: np.ndarray</span>
<span class="sd">        Gradient.</span>
<span class="sd">    hess: np.ndarray</span>
<span class="sd">        Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Weights</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Use 1 as weight if no weights are specified</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate gradients and hessians</span>
    <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">)</span>
    <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.predict_dist" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict_dist</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">pred_type</span><span class="o">=</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that predicts from the trained model.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.predict_dist--arguments">Arguments</h6>
<p>booster : xgb.Booster
    Trained model.
start_values : np.ndarray
    Starting values for each distributional parameter.
data : xgb.DMatrix
    Data to predict from.
pred_type : str
    Type of prediction:
    - "samples" draws n_samples from the predicted distribution.
    - "quantiles" calculates the quantiles from the predicted distribution.
    - "parameters" returns the predicted distributional parameters.
n_samples : int
    Number of samples to draw from the predicted distribution.
quantiles : List[float]
    List of quantiles to calculate from the predicted distribution.
seed : int
    Seed for random number generator used to draw samples from the predicted distribution.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.predict_dist--returns">Returns</h6>
<p>pred : pd.DataFrame
    Predictions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">booster</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                 <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">,</span>
                 <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that predicts from the trained model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    booster : xgb.Booster</span>
<span class="sd">        Trained model.</span>
<span class="sd">    start_values : np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    data : xgb.DMatrix</span>
<span class="sd">        Data to predict from.</span>
<span class="sd">    pred_type : str</span>
<span class="sd">        Type of prediction:</span>
<span class="sd">        - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">        - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">        - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to draw from the predicted distribution.</span>
<span class="sd">    quantiles : List[float]</span>
<span class="sd">        List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred : pd.DataFrame</span>
<span class="sd">        Predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set base_margin as starting point for each distributional parameter. Requires base_score=0 in parameters.</span>
    <span class="n">base_margin_predt</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
    <span class="n">data</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_predt</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

    <span class="c1"># Predict distributional parameters</span>
    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">output_margin</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

    <span class="c1"># Draw samples from predicted response distribution</span>
    <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                        <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dist_params_predt</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pred_samples_df</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
        <span class="c1"># Calculate quantiles from predicted response distribution</span>
        <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred_quant_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.replace_parameters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">replace_parameters</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">flow_dist</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Replace parameters with estimated ones.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.replace_parameters--arguments">Arguments</h6>
<p>params: torch.Tensor
    Estimated parameters.
flow_dist: Transform
    Normalizing Flow.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.replace_parameters--returns">Returns</h6>
<p>params_list: List
    List of estimated parameters.
flow_dist: Transform
    Normalizing Flow with estimated parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">replace_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                       <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">flow_dist</span><span class="p">:</span> <span class="n">Transform</span><span class="p">,</span>
                       <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">Transform</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace parameters with estimated ones.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    params: torch.Tensor</span>
<span class="sd">        Estimated parameters.</span>
<span class="sd">    flow_dist: Transform</span>
<span class="sd">        Normalizing Flow.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params_list: List</span>
<span class="sd">        List of estimated parameters.</span>
<span class="sd">    flow_dist: Transform</span>
<span class="sd">        Normalizing Flow with estimated parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Split parameters into list</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="s2">&quot;quadratic&quot;</span><span class="p">:</span>
        <span class="n">params_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">params</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="n">params_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">params</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_bins</span><span class="p">],</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Replace parameters</span>
    <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">new_value</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">params_list</span><span class="p">):</span>
        <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">new_value</span>

    <span class="c1"># Get parameters (including require_grad=True)</span>
    <span class="n">params_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">flow_dist</span><span class="o">.</span><span class="n">transforms</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">params_list</span><span class="p">,</span> <span class="n">flow_dist</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.stabilize_derivative" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;MAD&#39;</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that stabilizes Gradients and Hessians.</p>
<p>Since parameters are estimated by optimizing Gradients and Hessians, it is important that these are comparable
in magnitude for all parameters. Due to imbalances regarding the ranges, the estimation might become unstable
so that it does not converge (or converge very slowly) to the optimal solution. Another way to improve
convergence might be to standardize the response variable. This is especially useful if the range of the
response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and the
standardization of the response are not always advised but need to be carefully considered.</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.stabilize_derivative--source">Source</h6>
<p>https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.stabilize_derivative--arguments">Arguments</h6>
<p>input_der : torch.Tensor
    Input derivative, either Gradient or Hessian.
type: str
    Stabilization method. Can be either "None", "MAD" or "L2".</p>
<h6 id="xgboostlss.distributions.flow_utils.NormalizingFlowClass.stabilize_derivative--returns">Returns</h6>
<p>stab_der : torch.Tensor
    Stabilized Gradient or Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/flow_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that stabilizes Gradients and Hessians.</span>

<span class="sd">    Since parameters are estimated by optimizing Gradients and Hessians, it is important that these are comparable</span>
<span class="sd">    in magnitude for all parameters. Due to imbalances regarding the ranges, the estimation might become unstable</span>
<span class="sd">    so that it does not converge (or converge very slowly) to the optimal solution. Another way to improve</span>
<span class="sd">    convergence might be to standardize the response variable. This is especially useful if the range of the</span>
<span class="sd">    response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and the</span>
<span class="sd">    standardization of the response are not always advised but need to be carefully considered.</span>

<span class="sd">    Source</span>
<span class="sd">    ---------</span>
<span class="sd">    https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    input_der : torch.Tensor</span>
<span class="sd">        Input derivative, either Gradient or Hessian.</span>
<span class="sd">    type: str</span>
<span class="sd">        Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">    Returns</span>
<span class="sd">    ---------</span>
<span class="sd">    stab_der : torch.Tensor</span>
<span class="sd">        Stabilized Gradient or Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">stab_der</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.mixture_distribution_utils" class="doc doc-heading">
            <code>mixture_distribution_utils</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass" class="doc doc-heading">
            <code>MixtureDistributionClass</code>


</h4>


    <div class="doc doc-contents ">



        <p>Generic class that contains general functions for mixed-density distributions.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass--arguments">Arguments</h6>
<p>distribution: torch.distributions.Distribution
    PyTorch Distribution class.
M: int
    Number of components in the mixture distribution.
temperature: float
    Temperature for the Gumbel-Softmax distribution.
hessian_mode: str
    Mode for computing the Hessian. Must be one of the following:</p>
<pre><code>    - "individual": Each parameter is treated as a separate tensor. As a result, when the Hessian is calculated
    for each gradient element, this corresponds to the second derivative with respect to that specific tensor
    element only. This means the resulting Hessians capture the curvature of the loss w.r.t. each individual
    parameter. This is usually more runtime intensive, but can also be more accurate.

    - "grouped": Each parameter is a tensor containing all values for a specific parameter type,
    e.g., loc, scale, or mixture probabilities for a Gaussian Mixture. When computing the Hessian for each
    gradient element, the Hessian matrix for all the values in the respective tensor are calculated together.
    The resulting Hessians capture the curvature of the loss w.r.t. the entire parameter type tensor. This is
    usually less runtime intensive, but can be less accurate.
</code></pre>
<p>univariate: bool
    Whether the distribution is univariate or multivariate.
discrete: bool
    Whether the support of the distribution is discrete or continuous.
n_dist_param: int
    Number of distributional parameters.
stabilization: str
    Stabilization method.
param_dict: Dict[str, Any]
    Dictionary that maps distributional parameters to their response scale.
distribution_arg_names: List
    List of distributional parameter names.
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MixtureDistributionClass</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generic class that contains general functions for mixed-density distributions.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    distribution: torch.distributions.Distribution</span>
<span class="sd">        PyTorch Distribution class.</span>
<span class="sd">    M: int</span>
<span class="sd">        Number of components in the mixture distribution.</span>
<span class="sd">    temperature: float</span>
<span class="sd">        Temperature for the Gumbel-Softmax distribution.</span>
<span class="sd">    hessian_mode: str</span>
<span class="sd">        Mode for computing the Hessian. Must be one of the following:</span>

<span class="sd">            - &quot;individual&quot;: Each parameter is treated as a separate tensor. As a result, when the Hessian is calculated</span>
<span class="sd">            for each gradient element, this corresponds to the second derivative with respect to that specific tensor</span>
<span class="sd">            element only. This means the resulting Hessians capture the curvature of the loss w.r.t. each individual</span>
<span class="sd">            parameter. This is usually more runtime intensive, but can also be more accurate.</span>

<span class="sd">            - &quot;grouped&quot;: Each parameter is a tensor containing all values for a specific parameter type,</span>
<span class="sd">            e.g., loc, scale, or mixture probabilities for a Gaussian Mixture. When computing the Hessian for each</span>
<span class="sd">            gradient element, the Hessian matrix for all the values in the respective tensor are calculated together.</span>
<span class="sd">            The resulting Hessians capture the curvature of the loss w.r.t. the entire parameter type tensor. This is</span>
<span class="sd">            usually less runtime intensive, but can be less accurate.</span>
<span class="sd">    univariate: bool</span>
<span class="sd">        Whether the distribution is univariate or multivariate.</span>
<span class="sd">    discrete: bool</span>
<span class="sd">        Whether the support of the distribution is discrete or continuous.</span>
<span class="sd">    n_dist_param: int</span>
<span class="sd">        Number of distributional parameters.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method.</span>
<span class="sd">    param_dict: Dict[str, Any]</span>
<span class="sd">        Dictionary that maps distributional parameters to their response scale.</span>
<span class="sd">    distribution_arg_names: List</span>
<span class="sd">        List of distributional parameter names.</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">distribution</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">M</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">hessian_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;individual&quot;</span><span class="p">,</span>
                 <span class="n">univariate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">discrete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_dist_param</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">distribution_arg_names</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">=</span> <span class="n">M</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hessian_mode</span> <span class="o">=</span> <span class="n">hessian_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">univariate</span> <span class="o">=</span> <span class="n">univariate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span> <span class="o">=</span> <span class="n">discrete</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">=</span> <span class="n">n_dist_param</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">=</span> <span class="n">stabilization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">param_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="n">distribution_arg_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize</span> <span class="o">=</span> <span class="n">initialize</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function to estimate gradients and hessians of distributional parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: xgb.DMatrix</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        grad: np.ndarray</span>
<span class="sd">            Gradient.</span>
<span class="sd">        hess: np.ndarray</span>
<span class="sd">            Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Weights</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Use 1 as weight if no weights are specified</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate gradients and hessians</span>
        <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that evaluates the predictions using the specified loss function.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: xgb.DMatrix</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        name: str</span>
<span class="sd">            Name of the evaluation metric.</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate loss</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_mixture_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                    <span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that creates a mixture distribution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        params: torch.Tensor</span>
<span class="sd">            Distributional parameters.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dist: torch.distributions.Distribution</span>
<span class="sd">            Mixture distribution.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Create Mixture Distribution</span>
        <span class="n">mixture_cat</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mixture_comp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">mixture_dist</span> <span class="o">=</span> <span class="n">MixtureSameFamily</span><span class="p">(</span><span class="n">mixture_cat</span><span class="p">,</span> <span class="n">mixture_comp</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mixture_dist</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">loss_fn_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                             <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the loss for a given set of distributional parameters. Only used for calculating</span>
<span class="sd">        the loss for the start values.</span>

<span class="sd">        Parameter</span>
<span class="sd">        ---------</span>
<span class="sd">        params: torch.Tensor</span>
<span class="sd">            Distributional parameters.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Replace NaNs and infinity values with 0.5</span>
        <span class="n">nan_inf_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_idx</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Transform parameters to response scale</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">response_fn</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span>

        <span class="c1"># Specify Distribution and Loss</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                               <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the starting values for each distributional parameter.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Data from which starting values are calculated.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        start_values: np.ndarray</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert target to torch.tensor</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

        <span class="c1"># Initialize parameters</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)]</span>

        <span class="c1"># Specify optimizer</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">20</span><span class="p">]),</span> <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

        <span class="c1"># Define learning rate scheduler</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

        <span class="c1"># Define closure</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>

        <span class="c1"># Optimize parameters</span>
        <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-5</span>
        <span class="n">patience</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
        <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
            <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="c1"># Stopping criterion (no improvement in loss)</span>
            <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">best_loss</span> <span class="o">-</span> <span class="n">tolerance</span><span class="p">:</span>
                <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">epochs_without_change</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">epochs_without_change</span> <span class="o">&gt;=</span> <span class="n">patience</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="c1"># Get final loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Get start values</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)])</span>

        <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                        <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that returns the predicted parameters and the loss.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>
<span class="sd">        start_values: List</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        requires_grad: bool</span>
<span class="sd">            Whether to add to the computational graph or not.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        predt: List of torch.Tensors</span>
<span class="sd">            Predicted parameters.</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Predicted Parameters</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

        <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
        <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
        <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hessian_mode</span> <span class="o">==</span> <span class="s2">&quot;grouped&quot;</span><span class="p">:</span>
            <span class="c1"># Convert to torch.Tensor: splits the parameters into tensors for each parameter-type</span>
            <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Transform parameters to response scale</span>
            <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[</span><span class="n">response_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Convert to torch.Tensor: splits the parameters into tensors for each parameter individually</span>
            <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Transform parameters to response scale</span>
            <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
            <span class="n">max_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span>
            <span class="n">index_ranges</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">&gt;=</span> <span class="n">max_index</span><span class="p">:</span>
                    <span class="n">index_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
                    <span class="k">break</span>
                <span class="n">index_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">))</span>

            <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">index_ranges</span><span class="p">):</span>
                <span class="n">predt_transformed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">[</span><span class="n">key</span><span class="p">](</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>

        <span class="c1"># Specify Distribution and Loss</span>
        <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">predt_transformed</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that draws n_samples from a predicted distribution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt_params: pd.DataFrame</span>
<span class="sd">            pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">        n_samples: int</span>
<span class="sd">            Number of sample to draw from predicted response distribution.</span>
<span class="sd">        seed: int</span>
<span class="sd">            Manual seed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred_dist: pd.DataFrame</span>
<span class="sd">            DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">pred_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">pred_params</span><span class="p">)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_samples</span><span class="p">)</span>
        <span class="n">dist_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dist_samples</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">booster</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                     <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">,</span>
                     <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that predicts from the trained model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        booster : xgb.Booster</span>
<span class="sd">            Trained model.</span>
<span class="sd">        start_values : np.ndarray</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        data : xgb.DMatrix</span>
<span class="sd">            Data to predict from.</span>
<span class="sd">        pred_type : str</span>
<span class="sd">            Type of prediction:</span>
<span class="sd">            - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">            - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">            - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">        n_samples : int</span>
<span class="sd">            Number of samples to draw from the predicted distribution.</span>
<span class="sd">        quantiles : List[float]</span>
<span class="sd">            List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">        seed : int</span>
<span class="sd">            Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred : pd.DataFrame</span>
<span class="sd">            Predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set base_margin as starting point for each distributional parameter. Requires base_score=0 in parameters.</span>
        <span class="n">base_margin_predt</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
        <span class="n">data</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_predt</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

        <span class="n">predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">output_margin</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Transform predicted parameters to response scale</span>
        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">response_fun</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_params_predt</span><span class="p">)</span>
        <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span>

        <span class="c1"># Draw samples from predicted response distribution</span>
        <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                            <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dist_params_predt</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pred_samples_df</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
            <span class="c1"># Calculate quantiles from predicted response distribution</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
                <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pred_quant_df</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                       <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                                       <span class="n">predt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                                       <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates gradients and hessians.</span>

<span class="sd">        Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">        Arguments:</span>
<span class="sd">        ---------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss.</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            List of predicted parameters.</span>
<span class="sd">        weights: np.ndarray</span>
<span class="sd">            Weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">        -------</span>
<span class="sd">        grad: torch.Tensor</span>
<span class="sd">            Gradients.</span>
<span class="sd">        hess: torch.Tensor</span>
<span class="sd">            Hessians.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Gradient and Hessian</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

        <span class="c1"># Stabilization of Derivatives</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

        <span class="c1"># Reshape</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Weighting</span>
        <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
        <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that stabilizes Gradients and Hessians.</span>

<span class="sd">        As XGBoostLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important</span>
<span class="sd">        that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,</span>
<span class="sd">        the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.</span>
<span class="sd">        Another way to improve convergence might be to standardize the response variable. This is especially useful if the</span>
<span class="sd">        range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and</span>
<span class="sd">        the standardization of the response are not always advised but need to be carefully considered.</span>
<span class="sd">        Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_der : torch.Tensor</span>
<span class="sd">            Input derivative, either Gradient or Hessian.</span>
<span class="sd">        type: str</span>
<span class="sd">            Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        stab_der : torch.Tensor</span>
<span class="sd">            Stabilized Gradient or Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

        <span class="k">return</span> <span class="n">stab_der</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">dist_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">candidate_distributions</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                    <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that selects the most suitable distribution among the candidate_distributions for the target variable,</span>
<span class="sd">        based on the NegLogLikelihood (lower is better).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Response variable.</span>
<span class="sd">        candidate_distributions: List</span>
<span class="sd">            List of candidate distributions.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations for the optimization.</span>
<span class="sd">        plot: bool</span>
<span class="sd">            If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">        figure_size: tuple</span>
<span class="sd">            Figure size of the density plot.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        fit_df: pd.DataFrame</span>
<span class="sd">            Dataframe with the loss values of the fitted candidate distributions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dist_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate distributions&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)):</span>
                <span class="n">dist_name</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="n">n_mix</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
                <span class="n">tau</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">temperature</span>
                <span class="n">dist_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Mixture(</span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2">, tau=</span><span class="si">{</span><span class="n">tau</span><span class="si">}</span><span class="s2">, M=</span><span class="si">{</span><span class="n">n_mix</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution&quot;</span><span class="p">)</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span>
                         <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">],</span>
                         <span class="s2">&quot;dist_pos&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                         <span class="s2">&quot;M&quot;</span><span class="p">:</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                         <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span>
                         <span class="s2">&quot;dist_pos&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                         <span class="s2">&quot;M&quot;</span><span class="p">:</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="n">dist_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate distributions completed&quot;</span><span class="p">)</span>
            <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dist_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;dist_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
                <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
                <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
                <span class="s2">&quot;the required dependencies.&quot;</span>
            <span class="p">)</span>
            <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

            <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

            <span class="c1"># Select best distribution</span>
            <span class="n">best_dist</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="n">best_dist_pos</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;dist_pos&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">best_dist_pos</span><span class="p">]</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
                <span class="p">],</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">)</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span>
                                                      <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                                      <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

            <span class="c1"># Plot actual and fitted distribution</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
            <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_dist</span><span class="p">[</span><span class="s1">&#39;distribution&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

        <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">,</span> <span class="s2">&quot;dist_pos&quot;</span><span class="p">,</span> <span class="s2">&quot;M&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.calculate_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the starting values for each distributional parameter.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.calculate_start_values--arguments">Arguments</h6>
<p>target: np.ndarray
    Data from which starting values are calculated.
max_iter: int
    Maximum number of iterations.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.calculate_start_values--returns">Returns</h6>
<p>loss: float
    Loss value.
start_values: np.ndarray
    Starting values for each distributional parameter.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                           <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the starting values for each distributional parameter.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Data from which starting values are calculated.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    start_values: np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Convert target to torch.tensor</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="c1"># Initialize parameters</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)]</span>

    <span class="c1"># Specify optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">20</span><span class="p">]),</span> <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

    <span class="c1"># Define learning rate scheduler</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Define closure</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="c1"># Optimize parameters</span>
    <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tolerance</span> <span class="o">=</span> <span class="mf">1e-5</span>
    <span class="n">patience</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">)</span>
    <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
        <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="c1"># Stopping criterion (no improvement in loss)</span>
        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">best_loss</span> <span class="o">-</span> <span class="n">tolerance</span><span class="p">:</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">epochs_without_change</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">epochs_without_change</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">epochs_without_change</span> <span class="o">&gt;=</span> <span class="n">patience</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="c1"># Get final loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Get start values</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)])</span>

    <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.compute_gradients_and_hessians" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Calculates gradients and hessians.</p>
<p>Output gradients and hessians have shape (n_samples*n_outputs, 1).</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.compute_gradients_and_hessians--arguments">Arguments:</h6>
<p>loss: torch.Tensor
    Loss.
predt: torch.Tensor
    List of predicted parameters.
weights: np.ndarray
    Weights.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.compute_gradients_and_hessians--returns">Returns:</h6>
<p>grad: torch.Tensor
    Gradients.
hess: torch.Tensor
    Hessians.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                   <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                                   <span class="n">predt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                                   <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates gradients and hessians.</span>

<span class="sd">    Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    ---------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss.</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        List of predicted parameters.</span>
<span class="sd">    weights: np.ndarray</span>
<span class="sd">        Weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">    -------</span>
<span class="sd">    grad: torch.Tensor</span>
<span class="sd">        Gradients.</span>
<span class="sd">    hess: torch.Tensor</span>
<span class="sd">        Hessians.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Gradient and Hessian</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

    <span class="c1"># Stabilization of Derivatives</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

    <span class="c1"># Reshape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Weighting</span>
    <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
    <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.create_mixture_distribution" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">params</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that creates a mixture distribution.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.create_mixture_distribution--arguments">Arguments</h6>
<p>params: torch.Tensor
    Distributional parameters.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.create_mixture_distribution--returns">Returns</h6>
<p>dist: torch.distributions.Distribution
    Mixture distribution.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">create_mixture_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                <span class="n">params</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
                                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that creates a mixture distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    params: torch.Tensor</span>
<span class="sd">        Distributional parameters.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dist: torch.distributions.Distribution</span>
<span class="sd">        Mixture distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Create Mixture Distribution</span>
    <span class="n">mixture_cat</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">mixture_comp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">mixture_dist</span> <span class="o">=</span> <span class="n">MixtureSameFamily</span><span class="p">(</span><span class="n">mixture_cat</span><span class="p">,</span> <span class="n">mixture_comp</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mixture_dist</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.dist_select" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">dist_select</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">candidate_distributions</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that selects the most suitable distribution among the candidate_distributions for the target variable,
based on the NegLogLikelihood (lower is better).</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.dist_select--parameters">Parameters</h6>
<p>target: np.ndarray
    Response variable.
candidate_distributions: List
    List of candidate distributions.
max_iter: int
    Maximum number of iterations for the optimization.
plot: bool
    If True, a density plot of the actual and fitted distribution is created.
figure_size: tuple
    Figure size of the density plot.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.dist_select--returns">Returns</h6>
<p>fit_df: pd.DataFrame
    Dataframe with the loss values of the fitted candidate distributions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">dist_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                <span class="n">candidate_distributions</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">figure_size</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that selects the most suitable distribution among the candidate_distributions for the target variable,</span>
<span class="sd">    based on the NegLogLikelihood (lower is better).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Response variable.</span>
<span class="sd">    candidate_distributions: List</span>
<span class="sd">        List of candidate distributions.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations for the optimization.</span>
<span class="sd">    plot: bool</span>
<span class="sd">        If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">    figure_size: tuple</span>
<span class="sd">        Figure size of the density plot.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fit_df: pd.DataFrame</span>
<span class="sd">        Dataframe with the loss values of the fitted candidate distributions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dist_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate distributions&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)):</span>
            <span class="n">dist_name</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="n">n_mix</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
            <span class="n">tau</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">temperature</span>
            <span class="n">dist_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Mixture(</span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2">, tau=</span><span class="si">{</span><span class="n">tau</span><span class="si">}</span><span class="s2">, M=</span><span class="si">{</span><span class="n">n_mix</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution&quot;</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span>
                     <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">],</span>
                     <span class="s2">&quot;dist_pos&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                     <span class="s2">&quot;M&quot;</span><span class="p">:</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                     <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span>
                     <span class="s2">&quot;dist_pos&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                     <span class="s2">&quot;M&quot;</span><span class="p">:</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">M</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="n">dist_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate distributions completed&quot;</span><span class="p">)</span>
        <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dist_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;dist_select with plot=True requires &#39;matplotlib&#39; and &#39;seaborn&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

        <span class="c1"># Select best distribution</span>
        <span class="n">best_dist</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">best_dist_pos</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;dist_pos&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">best_dist_pos</span><span class="p">]</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">response_fun</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">fitted_params</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">)</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">500000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">fitted_params</span><span class="p">,</span>
                                                  <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                                  <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

        <span class="c1"># Plot actual and fitted distribution</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure_size</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span><span class="p">)</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_dist</span><span class="p">[</span><span class="s1">&#39;distribution&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">,</span> <span class="s2">&quot;dist_pos&quot;</span><span class="p">,</span> <span class="s2">&quot;M&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.draw_samples" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that draws n_samples from a predicted distribution.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.draw_samples--arguments">Arguments</h6>
<p>predt_params: pd.DataFrame
    pd.DataFrame with predicted distributional parameters.
n_samples: int
    Number of sample to draw from predicted response distribution.
seed: int
    Manual seed.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.draw_samples--returns">Returns</h6>
<p>pred_dist: pd.DataFrame
    DataFrame with n_samples drawn from predicted response distribution.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">predt_params</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that draws n_samples from a predicted distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt_params: pd.DataFrame</span>
<span class="sd">        pd.DataFrame with predicted distributional parameters.</span>
<span class="sd">    n_samples: int</span>
<span class="sd">        Number of sample to draw from predicted response distribution.</span>
<span class="sd">    seed: int</span>
<span class="sd">        Manual seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred_dist: pd.DataFrame</span>
<span class="sd">        DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt_params</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
    <span class="n">pred_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">pred_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">pred_params</span><span class="p">)</span>
    <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>
    <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_samples</span><span class="p">)</span>
    <span class="n">dist_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dist_samples</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.get_params_loss" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns the predicted parameters and the loss.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.get_params_loss--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
target: torch.Tensor
    Target values.
start_values: List
    Starting values for each distributional parameter.
requires_grad: bool
    Whether to add to the computational graph or not.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.get_params_loss--returns">Returns</h6>
<p>predt: List of torch.Tensors
    Predicted parameters.
loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns the predicted parameters and the loss.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>
<span class="sd">    start_values: List</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    requires_grad: bool</span>
<span class="sd">        Whether to add to the computational graph or not.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: List of torch.Tensors</span>
<span class="sd">        Predicted parameters.</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Predicted Parameters</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

    <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
    <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
    <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hessian_mode</span> <span class="o">==</span> <span class="s2">&quot;grouped&quot;</span><span class="p">:</span>
        <span class="c1"># Convert to torch.Tensor: splits the parameters into tensors for each parameter-type</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Transform parameters to response scale</span>
        <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[</span><span class="n">response_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Convert to torch.Tensor: splits the parameters into tensors for each parameter individually</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Transform parameters to response scale</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">max_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span>
        <span class="n">index_ranges</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span> <span class="o">&gt;=</span> <span class="n">max_index</span><span class="p">:</span>
                <span class="n">index_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
                <span class="k">break</span>
            <span class="n">index_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">))</span>

        <span class="n">predt_transformed</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">index_ranges</span><span class="p">):</span>
            <span class="n">predt_transformed</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">[</span><span class="n">key</span><span class="p">](</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>

    <span class="c1"># Specify Distribution and Loss</span>
    <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">predt_transformed</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.loss_fn_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the loss for a given set of distributional parameters. Only used for calculating
the loss for the start values.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.loss_fn_start_values--parameter">Parameter</h6>
<p>params: torch.Tensor
    Distributional parameters.
target: torch.Tensor
    Target values.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.loss_fn_start_values--returns">Returns</h6>
<p>loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">loss_fn_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the loss for a given set of distributional parameters. Only used for calculating</span>
<span class="sd">    the loss for the start values.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    params: torch.Tensor</span>
<span class="sd">        Distributional parameters.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Replace NaNs and infinity values with 0.5</span>
    <span class="n">nan_inf_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_idx</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">params</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Transform parameters to response scale</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">response_fn</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">response_fn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span>

    <span class="c1"># Specify Distribution and Loss</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_mixture_distribution</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.metric_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">metric_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that evaluates the predictions using the specified loss function.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.metric_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: xgb.DMatrix
    Data used for training.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.metric_fn--returns">Returns</h6>
<p>name: str
    Name of the evaluation metric.
loss: float
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that evaluates the predictions using the specified loss function.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: xgb.DMatrix</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    name: str</span>
<span class="sd">        Name of the evaluation metric.</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate loss</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.objective_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">objective_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function to estimate gradients and hessians of distributional parameters.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.objective_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: xgb.DMatrix
    Data used for training.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.objective_fn--returns">Returns</h6>
<p>grad: np.ndarray
    Gradient.
hess: np.ndarray
    Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to estimate gradients and hessians of distributional parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: xgb.DMatrix</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grad: np.ndarray</span>
<span class="sd">        Gradient.</span>
<span class="sd">    hess: np.ndarray</span>
<span class="sd">        Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Weights</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Use 1 as weight if no weights are specified</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate gradients and hessians</span>
    <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.predict_dist" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict_dist</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">pred_type</span><span class="o">=</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that predicts from the trained model.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.predict_dist--arguments">Arguments</h6>
<p>booster : xgb.Booster
    Trained model.
start_values : np.ndarray
    Starting values for each distributional parameter.
data : xgb.DMatrix
    Data to predict from.
pred_type : str
    Type of prediction:
    - "samples" draws n_samples from the predicted distribution.
    - "quantiles" calculates the quantiles from the predicted distribution.
    - "parameters" returns the predicted distributional parameters.
n_samples : int
    Number of samples to draw from the predicted distribution.
quantiles : List[float]
    List of quantiles to calculate from the predicted distribution.
seed : int
    Seed for random number generator used to draw samples from the predicted distribution.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.predict_dist--returns">Returns</h6>
<p>pred : pd.DataFrame
    Predictions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">booster</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                 <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">,</span>
                 <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that predicts from the trained model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    booster : xgb.Booster</span>
<span class="sd">        Trained model.</span>
<span class="sd">    start_values : np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    data : xgb.DMatrix</span>
<span class="sd">        Data to predict from.</span>
<span class="sd">    pred_type : str</span>
<span class="sd">        Type of prediction:</span>
<span class="sd">        - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">        - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">        - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to draw from the predicted distribution.</span>
<span class="sd">    quantiles : List[float]</span>
<span class="sd">        List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred : pd.DataFrame</span>
<span class="sd">        Predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set base_margin as starting point for each distributional parameter. Requires base_score=0 in parameters.</span>
    <span class="n">base_margin_predt</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
    <span class="n">data</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_predt</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

    <span class="n">predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">output_margin</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Transform predicted parameters to response scale</span>
    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">response_fun</span><span class="p">(</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">dist_param</span><span class="p">,</span> <span class="n">response_fun</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="p">],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_params_predt</span><span class="p">)</span>
    <span class="n">dist_params_predt</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span>

    <span class="c1"># Draw samples from predicted response distribution</span>
    <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">predt_params</span><span class="o">=</span><span class="n">dist_params_predt</span><span class="p">,</span>
                                        <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
                                        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">dist_params_predt</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pred_samples_df</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
        <span class="c1"># Calculate quantiles from predicted response distribution</span>
        <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred_quant_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.stabilize_derivative" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;MAD&#39;</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that stabilizes Gradients and Hessians.</p>
<p>As XGBoostLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important
that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,
the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.
Another way to improve convergence might be to standardize the response variable. This is especially useful if the
range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and
the standardization of the response are not always advised but need to be carefully considered.
Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.stabilize_derivative--parameters">Parameters</h6>
<p>input_der : torch.Tensor
    Input derivative, either Gradient or Hessian.
type: str
    Stabilization method. Can be either "None", "MAD" or "L2".</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.MixtureDistributionClass.stabilize_derivative--returns">Returns</h6>
<p>stab_der : torch.Tensor
    Stabilized Gradient or Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that stabilizes Gradients and Hessians.</span>

<span class="sd">    As XGBoostLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important</span>
<span class="sd">    that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,</span>
<span class="sd">    the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.</span>
<span class="sd">    Another way to improve convergence might be to standardize the response variable. This is especially useful if the</span>
<span class="sd">    range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and</span>
<span class="sd">    the standardization of the response are not always advised but need to be carefully considered.</span>
<span class="sd">    Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_der : torch.Tensor</span>
<span class="sd">        Input derivative, either Gradient or Hessian.</span>
<span class="sd">    type: str</span>
<span class="sd">        Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    stab_der : torch.Tensor</span>
<span class="sd">        Stabilized Gradient or Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">stab_der</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h4 id="xgboostlss.distributions.mixture_distribution_utils.get_component_distributions" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_component_distributions</span><span class="p">()</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function that returns component distributions for creating a mixing distribution.</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.get_component_distributions--arguments">Arguments</h6>
<p>None</p>
<h6 id="xgboostlss.distributions.mixture_distribution_utils.get_component_distributions--returns">Returns</h6>
<p>distns: List
    List of all available distributions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/mixture_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_component_distributions</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns component distributions for creating a mixing distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    None</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    distns: List</span>
<span class="sd">        List of all available distributions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Get all distribution names</span>
    <span class="n">mixture_distns</span> <span class="o">=</span> <span class="p">[</span><span class="n">dist</span> <span class="k">for</span> <span class="n">dist</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">distributions</span><span class="p">)</span> <span class="k">if</span> <span class="n">dist</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">isupper</span><span class="p">()]</span>

    <span class="c1"># Remove specific distributions</span>
    <span class="n">distns_remove</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;Dirichlet&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Expectile&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MVN&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MVN_LoRa&quot;</span><span class="p">,</span>
        <span class="s2">&quot;MVT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Mixture&quot;</span><span class="p">,</span>
        <span class="s2">&quot;SplineFlow&quot;</span>
    <span class="p">]</span>

    <span class="n">mixture_distns</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">mixture_distns</span> <span class="k">if</span> <span class="n">item</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">distns_remove</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">mixture_distns</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.multivariate_distribution_utils" class="doc doc-heading">
            <code>multivariate_distribution_utils</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass" class="doc doc-heading">
            <code>Multivariate_DistributionClass</code>


</h4>


    <div class="doc doc-contents ">



        <p>Generic class that contains general functions for multivariate distributions.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass--arguments">Arguments</h6>
<p>distribution: torch.distributions.Distribution
    PyTorch Distribution class.
univariate: bool
    Whether the distribution is univariate or multivariate.
distribution_arg_names: List
    List of distributional parameter names.
n_targets: int
    Number of targets.
rank: Optional[int]
    Rank of the low-rank form of the covariance matrix.
n_dist_param: int
    Number of distributional parameters.
param_dict: Dict[str, Any]
    Dictionary that maps distributional parameters to their response scale.
param_transform: Callable
    Function that transforms the distributional parameters into the required format.
get_dist_params: Callable
    Function that returns the distributional parameters.
discrete: bool
    Whether the support of the distribution is discrete or continuous.
stabilization: str
    Stabilization method.
loss_fn: str
    Loss function. Options are "nll" (negative log-likelihood).
initialize: bool
    Whether to initialize the distributional parameters with unconditional start values. Initialization can help
    to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal
    solutions if the unconditional start values are far from the optimal values.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Multivariate_DistributionClass</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generic class that contains general functions for multivariate distributions.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    distribution: torch.distributions.Distribution</span>
<span class="sd">        PyTorch Distribution class.</span>
<span class="sd">    univariate: bool</span>
<span class="sd">        Whether the distribution is univariate or multivariate.</span>
<span class="sd">    distribution_arg_names: List</span>
<span class="sd">        List of distributional parameter names.</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    rank: Optional[int]</span>
<span class="sd">        Rank of the low-rank form of the covariance matrix.</span>
<span class="sd">    n_dist_param: int</span>
<span class="sd">        Number of distributional parameters.</span>
<span class="sd">    param_dict: Dict[str, Any]</span>
<span class="sd">        Dictionary that maps distributional parameters to their response scale.</span>
<span class="sd">    param_transform: Callable</span>
<span class="sd">        Function that transforms the distributional parameters into the required format.</span>
<span class="sd">    get_dist_params: Callable</span>
<span class="sd">        Function that returns the distributional parameters.</span>
<span class="sd">    discrete: bool</span>
<span class="sd">        Whether the support of the distribution is discrete or continuous.</span>
<span class="sd">    stabilization: str</span>
<span class="sd">        Stabilization method.</span>
<span class="sd">    loss_fn: str</span>
<span class="sd">        Loss function. Options are &quot;nll&quot; (negative log-likelihood).</span>
<span class="sd">    initialize: bool</span>
<span class="sd">        Whether to initialize the distributional parameters with unconditional start values. Initialization can help</span>
<span class="sd">        to improve speed of convergence in some cases. However, it may also lead to early stopping or suboptimal</span>
<span class="sd">        solutions if the unconditional start values are far from the optimal values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">distribution</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">univariate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">distribution_arg_names</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                 <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">n_dist_param</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">param_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">param_transform</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">get_dist_params</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">discrete</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">stabilization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
                 <span class="n">loss_fn</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;nll&quot;</span><span class="p">,</span>
                 <span class="n">initialize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">univariate</span> <span class="o">=</span> <span class="n">univariate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span> <span class="o">=</span> <span class="n">distribution_arg_names</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span> <span class="o">=</span> <span class="n">n_targets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">=</span> <span class="n">n_dist_param</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">param_dict</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_transform</span> <span class="o">=</span> <span class="n">param_transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_dist_params</span> <span class="o">=</span> <span class="n">get_dist_params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span> <span class="o">=</span> <span class="n">discrete</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">=</span> <span class="n">stabilization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize</span> <span class="o">=</span> <span class="n">initialize</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function to estimate gradients and hessians of distributional parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: xgb.DMatrix</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        grad: np.ndarray</span>
<span class="sd">            Gradient.</span>
<span class="sd">        hess: np.ndarray</span>
<span class="sd">            Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">))[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">]</span>

        <span class="c1"># Weights</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Use 1 as weight if no weights are specified</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate gradients and hessians</span>
        <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that evaluates the predictions using the specified loss function.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        data: xgb.DMatrix</span>
<span class="sd">            Data used for training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        name: str</span>
<span class="sd">            Name of the evaluation metric.</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Target</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">))[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">]</span>

        <span class="c1"># Start values (needed to replace NaNs in predt)</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Calculate loss</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">loss_fn_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                             <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                             <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the loss for a given set of distributional parameters. Only used for calculating</span>
<span class="sd">        the loss for the start values.</span>

<span class="sd">        Parameter</span>
<span class="sd">        ---------</span>
<span class="sd">        params: torch.Tensor</span>
<span class="sd">            Distributional parameters.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Replace NaNs and infinity values with 0.5</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">params</span>
        <span class="p">]</span>

        <span class="c1"># Transform parameters to response scale</span>
        <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Specify Distribution and Loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;Dirichlet&quot;</span><span class="p">:</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="p">[</span><span class="n">params</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">params</span><span class="p">))</span>
        <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                               <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                               <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                               <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that calculates the starting values for each distributional parameter.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Data from which starting values are calculated.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        loss: float</span>
<span class="sd">            Loss value.</span>
<span class="sd">        start_values: np.ndarray</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert target to torch.tensor</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">))[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">]</span>

        <span class="c1"># Initialize parameters</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Specify optimizer</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">20</span><span class="p">]),</span> <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

        <span class="c1"># Define learning rate scheduler</span>
        <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

        <span class="c1"># Define closure</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>

        <span class="c1"># Optimize parameters</span>
        <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
            <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="c1"># Get final loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Get start values</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)])</span>

        <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
        <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                        <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                        <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that returns the predicted parameters and the loss.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        predt: np.ndarray</span>
<span class="sd">            Predicted values.</span>
<span class="sd">        target: torch.Tensor</span>
<span class="sd">            Target values.</span>
<span class="sd">        start_values: List</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        requires_grad: bool</span>
<span class="sd">            Whether to add to the computational graph or not.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            Predicted parameters.</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Number of observations</span>
        <span class="n">n_obs</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Predicted Parameters</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

        <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
        <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
        <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># Convert to torch.tensor</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># Predicted Parameters transformed to response scale</span>
        <span class="n">predt_transformed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_transform</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="n">n_obs</span><span class="p">)</span>

        <span class="c1"># Specify Distribution and Loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;Dirichlet&quot;</span><span class="p">:</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="p">[</span><span class="n">predt_transformed</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">predt_transformed</span><span class="p">))</span>
        <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">dist_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that draws n_samples from a predicted distribution.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        dist_pred: torch.distributions.Distribution</span>
<span class="sd">            Predicted distribution.</span>
<span class="sd">        n_samples: int</span>
<span class="sd">            Number of sample to draw from predicted response distribution.</span>
<span class="sd">        seed: int</span>
<span class="sd">            Manual seed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred_dist: pd.DataFrame</span>
<span class="sd">            DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">samples_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">):</span>
            <span class="n">target_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span><span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;y</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]})</span>
            <span class="n">df_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_samples</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
            <span class="n">df_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)]</span>
            <span class="n">samples_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">target_df</span><span class="p">,</span> <span class="n">df_samples</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">samples_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">samples_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">samples_df</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                     <span class="n">booster</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                     <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">,</span>
                     <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                     <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                     <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that predicts from the trained model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        booster : xgb.Booster</span>
<span class="sd">            Trained model.</span>
<span class="sd">        start_values : np.ndarray</span>
<span class="sd">            Starting values for each distributional parameter.</span>
<span class="sd">        data : xgb.DMatrix</span>
<span class="sd">            Data to predict from.</span>
<span class="sd">        pred_type : str</span>
<span class="sd">            Type of prediction:</span>
<span class="sd">            - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">            - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">            - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">            - &quot;expectiles&quot; returns the predicted expectiles.</span>
<span class="sd">        n_samples : int</span>
<span class="sd">            Number of samples to draw from the predicted distribution.</span>
<span class="sd">        quantiles : List[float]</span>
<span class="sd">            List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">        seed : int</span>
<span class="sd">            Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        pred : pd.DataFrame</span>
<span class="sd">            Predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set base_margin as starting point for each distributional parameter. Requires base_score=0 in parameters.</span>
        <span class="n">base_margin_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
        <span class="n">data</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

        <span class="c1"># Predict from model</span>
        <span class="n">n_obs</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">num_row</span><span class="p">()</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">output_margin</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="n">predt</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)]</span>
        <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_transform</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="n">n_obs</span><span class="p">)</span>

        <span class="c1"># Predicted Distributional Parameters</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;Dirichlet&quot;</span><span class="p">:</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="p">[</span><span class="n">dist_params_predt</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">dist_params_predt</span><span class="p">))</span>
        <span class="n">dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>

        <span class="c1"># Draw samples from predicted response distribution</span>
        <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">=</span><span class="n">dist_pred</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># Get predicted distributional parameters</span>
        <span class="n">predt_params_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">dist_pred</span><span class="o">=</span><span class="n">dist_pred</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">predt_params_df</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pred_samples_df</span>

        <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
            <span class="c1"># Calculate quantiles from predicted response distribution</span>
            <span class="n">targets</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">)</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
                <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">targets</span><span class="p">,</span> <span class="n">pred_quant_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">pred_quant_df</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                       <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                       <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates gradients and hessians.</span>

<span class="sd">        Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">        Arguments:</span>
<span class="sd">        ---------</span>
<span class="sd">        loss: torch.Tensor</span>
<span class="sd">            Loss.</span>
<span class="sd">        predt: torch.Tensor</span>
<span class="sd">            List of predicted parameters.</span>
<span class="sd">        weights: np.ndarray</span>
<span class="sd">            Weights.</span>

<span class="sd">        Returns:</span>
<span class="sd">        -------</span>
<span class="sd">        grad: torch.Tensor</span>
<span class="sd">            Gradients.</span>
<span class="sd">        hess: torch.Tensor</span>
<span class="sd">            Hessians.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Calculate gradients and hessians</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

        <span class="c1"># Stabilization of Derivatives</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
            <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

        <span class="c1"># Reshape</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Weighting</span>
        <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
        <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

        <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that stabilizes Gradients and Hessians.</span>

<span class="sd">        As XGBoostLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important</span>
<span class="sd">        that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,</span>
<span class="sd">        the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.</span>
<span class="sd">        Another way to improve convergence might be to standardize the response variable. This is especially useful if the</span>
<span class="sd">        range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and</span>
<span class="sd">        the standardization of the response are not always advised but need to be carefully considered.</span>
<span class="sd">        Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_der : torch.Tensor</span>
<span class="sd">            Input derivative, either Gradient or Hessian.</span>
<span class="sd">        type: str</span>
<span class="sd">            Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        stab_der : torch.Tensor</span>
<span class="sd">            Stabilized Gradient or Hessian.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
            <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

        <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
            <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

        <span class="k">return</span> <span class="n">stab_der</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">dist_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">candidate_distributions</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                    <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                    <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="n">ncol</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                    <span class="n">height</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                    <span class="n">sharex</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                    <span class="n">sharey</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that selects the most suitable distribution among the candidate_distributions for the target variable,</span>
<span class="sd">        based on the NegLogLikelihood (lower is better).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Response variable.</span>
<span class="sd">        candidate_distributions: List</span>
<span class="sd">            List of candidate distributions.</span>
<span class="sd">        max_iter: int</span>
<span class="sd">            Maximum number of iterations for the optimization.</span>
<span class="sd">        plot: bool</span>
<span class="sd">            If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">        ncol: int</span>
<span class="sd">            Number of columns for the facetting of the density plots.</span>
<span class="sd">        height: Float</span>
<span class="sd">            Height (in inches) of each facet.</span>
<span class="sd">        sharex: bool</span>
<span class="sd">            Whether to share the x-axis across the facets.</span>
<span class="sd">        sharey: bool</span>
<span class="sd">            Whether to share the y-axis across the facets.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        fit_df: pd.DataFrame</span>
<span class="sd">            Dataframe with the loss values of the fitted candidate distributions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dist_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate distributions&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)):</span>
                <span class="n">dist_name</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="k">if</span> <span class="n">dist_name</span> <span class="o">==</span> <span class="s2">&quot;MVN_LoRa&quot;</span><span class="p">:</span>
                    <span class="n">dist_name</span> <span class="o">=</span> <span class="n">dist_name</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;(rank=</span><span class="si">{</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution&quot;</span><span class="p">)</span>
                <span class="n">dist_sel</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">target_expand</span> <span class="o">=</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">target_append</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target_expand</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">dist_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span>
                         <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">]</span>
                         <span class="p">}</span>
                    <span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">dist_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                         <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                         <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">n_dist_param</span>
                        <span class="p">}</span>
                    <span class="p">)</span>
                <span class="n">dist_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
                <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate distributions completed&quot;</span><span class="p">)</span>
            <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dist_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">dist_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">dist_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
            <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

            <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">&quot;dist_select with plot=True requires &#39;seaborn&#39; &quot;</span>
                <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
                <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
                <span class="s2">&quot;the required dependencies.&quot;</span>
            <span class="p">)</span>
            <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

            <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

            <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
            <span class="c1"># Select distribution</span>
            <span class="n">best_dist</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">dist</span> <span class="ow">in</span> <span class="n">candidate_distributions</span><span class="p">:</span>
                <span class="n">dist_name</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
                <span class="k">if</span> <span class="n">dist_name</span> <span class="o">==</span> <span class="s2">&quot;MVN_LoRa&quot;</span><span class="p">:</span>
                    <span class="n">dist_name</span> <span class="o">=</span> <span class="n">dist_name</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;(rank=</span><span class="si">{</span><span class="n">dist</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="k">if</span> <span class="n">dist_name</span> <span class="o">==</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="n">dist</span>
                    <span class="k">break</span>

            <span class="c1"># Draw samples from distribution</span>
            <span class="n">dist_params</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="n">dist_params</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_transform</span><span class="p">(</span><span class="n">dist_params</span><span class="p">,</span>
                                                        <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="p">,</span>
                                                        <span class="n">n_targets</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span>
                                                        <span class="n">rank</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
                                                        <span class="n">n_obs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Dirichlet&quot;</span><span class="p">:</span>
                <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="p">[</span><span class="n">dist_params</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">dist_params</span><span class="p">))</span>
            <span class="n">dist_fit</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
            <span class="n">df_samples</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">dist_fit</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

            <span class="c1"># Plot actual and fitted distribution</span>
            <span class="n">df_samples</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_dist</span><span class="p">[</span><span class="s1">&#39;distribution&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">df_samples</span> <span class="o">=</span> <span class="n">df_samples</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&quot;variable&quot;</span><span class="p">)</span>

            <span class="n">df_actual</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
            <span class="n">df_actual</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;y</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">n_targets</span><span class="p">)]</span>
            <span class="n">df_actual</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;Actual&quot;</span>
            <span class="n">df_actual</span> <span class="o">=</span> <span class="n">df_actual</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="s2">&quot;type&quot;</span><span class="p">,</span> <span class="n">var_name</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">)[</span><span class="n">df_samples</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>

            <span class="n">plot_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_actual</span><span class="p">,</span> <span class="n">df_samples</span><span class="p">])</span>

            <span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span>
                <span class="n">plot_df</span><span class="p">,</span>
                <span class="n">col</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">,</span>
                <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;type&quot;</span><span class="p">,</span>
                <span class="n">col_wrap</span><span class="o">=</span><span class="n">ncol</span><span class="p">,</span>
                <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span>
                <span class="n">sharex</span><span class="o">=</span><span class="n">sharex</span><span class="p">,</span>
                <span class="n">sharey</span><span class="o">=</span><span class="n">sharey</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">g</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
            <span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
            <span class="n">g</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper center&#39;</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">))</span>
            <span class="n">g</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
            <span class="n">g</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span>

        <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">fit_df</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">target_append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                      <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                      <span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                      <span class="n">n_dist_param</span><span class="p">:</span> <span class="nb">int</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that appends target to the number of specified parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        target: np.ndarray</span>
<span class="sd">            Target variables.</span>
<span class="sd">        n_targets: int</span>
<span class="sd">            Number of targets.</span>
<span class="sd">        n_dist_param: int</span>
<span class="sd">            Number of distribution parameters.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        label: np.ndarray</span>
<span class="sd">            Array with appended targets.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">)</span>
        <span class="n">n_obs</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_fill</span> <span class="o">=</span> <span class="n">n_dist_param</span> <span class="o">-</span> <span class="n">n_targets</span>
        <span class="n">np_fill</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_obs</span><span class="p">,</span> <span class="n">n_fill</span><span class="p">))</span>
        <span class="n">label_append</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">label</span><span class="p">,</span> <span class="n">np_fill</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_dist_param</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">label_append</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.calculate_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the starting values for each distributional parameter.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.calculate_start_values--arguments">Arguments</h6>
<p>target: np.ndarray
    Data from which starting values are calculated.
max_iter: int
    Maximum number of iterations.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.calculate_start_values--returns">Returns</h6>
<p>loss: float
    Loss value.
start_values: np.ndarray
    Starting values for each distributional parameter.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">calculate_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                           <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                           <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span>
                           <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the starting values for each distributional parameter.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Data from which starting values are calculated.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    start_values: np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Convert target to torch.tensor</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">))[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">]</span>

    <span class="c1"># Initialize parameters</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Specify optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">LBFGS</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">max_iter</span><span class="o">/</span><span class="mi">4</span><span class="p">),</span> <span class="mi">20</span><span class="p">]),</span> <span class="n">line_search_fn</span><span class="o">=</span><span class="s2">&quot;strong_wolfe&quot;</span><span class="p">)</span>

    <span class="c1"># Define learning rate scheduler</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">ReduceLROnPlateau</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Define closure</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">():</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="c1"># Optimize parameters</span>
    <span class="n">loss_vals</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="p">)</span>
        <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">loss_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># Get final loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Get start values</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)])</span>

    <span class="c1"># Replace any remaining NaNs or infinity values with 0.5</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">start_values</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.compute_gradients_and_hessians" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Calculates gradients and hessians.</p>
<p>Output gradients and hessians have shape (n_samples*n_outputs, 1).</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.compute_gradients_and_hessians--arguments">Arguments:</h6>
<p>loss: torch.Tensor
    Loss.
predt: torch.Tensor
    List of predicted parameters.
weights: np.ndarray
    Weights.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.compute_gradients_and_hessians--returns">Returns:</h6>
<p>grad: torch.Tensor
    Gradients.
hess: torch.Tensor
    Hessians.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compute_gradients_and_hessians</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                   <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                                   <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates gradients and hessians.</span>

<span class="sd">    Output gradients and hessians have shape (n_samples*n_outputs, 1).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    ---------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss.</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        List of predicted parameters.</span>
<span class="sd">    weights: np.ndarray</span>
<span class="sd">        Weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">    -------</span>
<span class="sd">    grad: torch.Tensor</span>
<span class="sd">        Gradients.</span>
<span class="sd">    hess: torch.Tensor</span>
<span class="sd">        Hessians.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Calculate gradients and hessians</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">autograd</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="n">autograd</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nansum</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">predt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>

    <span class="c1"># Stabilization of Derivatives</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grad</span><span class="p">))]</span>
        <span class="n">hess</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">hess</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stabilization</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hess</span><span class="p">))]</span>

    <span class="c1"># Reshape</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">hess</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Weighting</span>
    <span class="n">grad</span> <span class="o">*=</span> <span class="n">weights</span>
    <span class="n">hess</span> <span class="o">*=</span> <span class="n">weights</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.dist_select" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">dist_select</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">candidate_distributions</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that selects the most suitable distribution among the candidate_distributions for the target variable,
based on the NegLogLikelihood (lower is better).</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.dist_select--parameters">Parameters</h6>
<p>target: np.ndarray
    Response variable.
candidate_distributions: List
    List of candidate distributions.
max_iter: int
    Maximum number of iterations for the optimization.
plot: bool
    If True, a density plot of the actual and fitted distribution is created.
ncol: int
    Number of columns for the facetting of the density plots.
height: Float
    Height (in inches) of each facet.
sharex: bool
    Whether to share the x-axis across the facets.
sharey: bool
    Whether to share the y-axis across the facets.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.dist_select--returns">Returns</h6>
<p>fit_df: pd.DataFrame
    Dataframe with the loss values of the fitted candidate distributions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">dist_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                <span class="n">candidate_distributions</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
                <span class="n">plot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                <span class="n">ncol</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                <span class="n">height</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                <span class="n">sharex</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                <span class="n">sharey</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that selects the most suitable distribution among the candidate_distributions for the target variable,</span>
<span class="sd">    based on the NegLogLikelihood (lower is better).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Response variable.</span>
<span class="sd">    candidate_distributions: List</span>
<span class="sd">        List of candidate distributions.</span>
<span class="sd">    max_iter: int</span>
<span class="sd">        Maximum number of iterations for the optimization.</span>
<span class="sd">    plot: bool</span>
<span class="sd">        If True, a density plot of the actual and fitted distribution is created.</span>
<span class="sd">    ncol: int</span>
<span class="sd">        Number of columns for the facetting of the density plots.</span>
<span class="sd">    height: Float</span>
<span class="sd">        Height (in inches) of each facet.</span>
<span class="sd">    sharex: bool</span>
<span class="sd">        Whether to share the x-axis across the facets.</span>
<span class="sd">    sharey: bool</span>
<span class="sd">        Whether to share the y-axis across the facets.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    fit_df: pd.DataFrame</span>
<span class="sd">        Dataframe with the loss values of the fitted candidate distributions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dist_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">total_iterations</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Fitting candidate distributions&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">candidate_distributions</span><span class="p">)):</span>
            <span class="n">dist_name</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="k">if</span> <span class="n">dist_name</span> <span class="o">==</span> <span class="s2">&quot;MVN_LoRa&quot;</span><span class="p">:</span>
                <span class="n">dist_name</span> <span class="o">=</span> <span class="n">dist_name</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;(rank=</span><span class="si">{</span><span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution&quot;</span><span class="p">)</span>
            <span class="n">dist_sel</span> <span class="o">=</span> <span class="n">candidate_distributions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">target_expand</span> <span class="o">=</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">target_append</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">loss</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">target_expand</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">dist_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span>
                     <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">params</span><span class="p">]</span>
                     <span class="p">}</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error fitting </span><span class="si">{</span><span class="n">dist_name</span><span class="si">}</span><span class="s2"> distribution: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">dist_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span>
                     <span class="s2">&quot;distribution&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">dist_name</span><span class="p">),</span>
                     <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="n">dist_sel</span><span class="o">.</span><span class="n">n_dist_param</span>
                    <span class="p">}</span>
                <span class="p">)</span>
            <span class="n">dist_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_df</span><span class="p">)</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fitting of candidate distributions completed&quot;</span><span class="p">)</span>
        <span class="n">fit_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">dist_list</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">dist_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">dist_sel</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">]</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">fit_df</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;dist_select with plot=True requires &#39;seaborn&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the packages to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;seaborn&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
        <span class="c1"># Select distribution</span>
        <span class="n">best_dist</span> <span class="o">=</span> <span class="n">fit_df</span><span class="p">[</span><span class="n">fit_df</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">dist</span> <span class="ow">in</span> <span class="n">candidate_distributions</span><span class="p">:</span>
            <span class="n">dist_name</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="k">if</span> <span class="n">dist_name</span> <span class="o">==</span> <span class="s2">&quot;MVN_LoRa&quot;</span><span class="p">:</span>
                <span class="n">dist_name</span> <span class="o">=</span> <span class="n">dist_name</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;(rank=</span><span class="si">{</span><span class="n">dist</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="k">if</span> <span class="n">dist_name</span> <span class="o">==</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">best_dist_sel</span> <span class="o">=</span> <span class="n">dist</span>
                <span class="k">break</span>

        <span class="c1"># Draw samples from distribution</span>
        <span class="n">dist_params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="p">]</span>
        <span class="n">dist_params</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_transform</span><span class="p">(</span><span class="n">dist_params</span><span class="p">,</span>
                                                    <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">param_dict</span><span class="p">,</span>
                                                    <span class="n">n_targets</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span>
                                                    <span class="n">rank</span><span class="o">=</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span>
                                                    <span class="n">n_obs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">best_dist</span><span class="p">[</span><span class="s2">&quot;distribution&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Dirichlet&quot;</span><span class="p">:</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="p">[</span><span class="n">dist_params</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">dist_params</span><span class="p">))</span>
        <span class="n">dist_fit</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">&gt;</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="n">df_samples</span> <span class="o">=</span> <span class="n">best_dist_sel</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">dist_fit</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

        <span class="c1"># Plot actual and fitted distribution</span>
        <span class="n">df_samples</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Best-Fit: </span><span class="si">{</span><span class="n">best_dist</span><span class="p">[</span><span class="s1">&#39;distribution&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">df_samples</span> <span class="o">=</span> <span class="n">df_samples</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&quot;variable&quot;</span><span class="p">)</span>

        <span class="n">df_actual</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
        <span class="n">df_actual</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;y</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">best_dist_sel</span><span class="o">.</span><span class="n">n_targets</span><span class="p">)]</span>
        <span class="n">df_actual</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;Actual&quot;</span>
        <span class="n">df_actual</span> <span class="o">=</span> <span class="n">df_actual</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="s2">&quot;type&quot;</span><span class="p">,</span> <span class="n">var_name</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">)[</span><span class="n">df_samples</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>

        <span class="n">plot_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_actual</span><span class="p">,</span> <span class="n">df_samples</span><span class="p">])</span>

        <span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span>
            <span class="n">plot_df</span><span class="p">,</span>
            <span class="n">col</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">,</span>
            <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;type&quot;</span><span class="p">,</span>
            <span class="n">col_wrap</span><span class="o">=</span><span class="n">ncol</span><span class="p">,</span>
            <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span>
            <span class="n">sharex</span><span class="o">=</span><span class="n">sharex</span><span class="p">,</span>
            <span class="n">sharey</span><span class="o">=</span><span class="n">sharey</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">g</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
        <span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
        <span class="n">g</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper center&#39;</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">))</span>
        <span class="n">g</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Actual vs. Best-Fit Density&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">g</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span>

    <span class="n">fit_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;rank&quot;</span><span class="p">,</span> <span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fit_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.draw_samples" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">draw_samples</span><span class="p">(</span><span class="n">dist_pred</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that draws n_samples from a predicted distribution.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.draw_samples--arguments">Arguments</h6>
<p>dist_pred: torch.distributions.Distribution
    Predicted distribution.
n_samples: int
    Number of sample to draw from predicted response distribution.
seed: int
    Manual seed.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.draw_samples--returns">Returns</h6>
<p>pred_dist: pd.DataFrame
    DataFrame with n_samples drawn from predicted response distribution.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">draw_samples</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">dist_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Distribution</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that draws n_samples from a predicted distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    dist_pred: torch.distributions.Distribution</span>
<span class="sd">        Predicted distribution.</span>
<span class="sd">    n_samples: int</span>
<span class="sd">        Number of sample to draw from predicted response distribution.</span>
<span class="sd">    seed: int</span>
<span class="sd">        Manual seed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred_dist: pd.DataFrame</span>
<span class="sd">        DataFrame with n_samples drawn from predicted response distribution.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_pred</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
        <span class="n">dist_samples</span> <span class="o">=</span> <span class="n">dist_samples</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="n">samples_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">):</span>
        <span class="n">target_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span><span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;y</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]})</span>
        <span class="n">df_samples</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dist_samples</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
        <span class="n">df_samples</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;y_sample&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)]</span>
        <span class="n">samples_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">target_df</span><span class="p">,</span> <span class="n">df_samples</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">samples_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">samples_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">samples_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.get_params_loss" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that returns the predicted parameters and the loss.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.get_params_loss--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
target: torch.Tensor
    Target values.
start_values: List
    Starting values for each distributional parameter.
requires_grad: bool
    Whether to add to the computational graph or not.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.get_params_loss--returns">Returns</h6>
<p>predt: torch.Tensor
    Predicted parameters.
loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_params_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                    <span class="n">start_values</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
                    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that returns the predicted parameters and the loss.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>
<span class="sd">    start_values: List</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    requires_grad: bool</span>
<span class="sd">        Whether to add to the computational graph or not.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.Tensor</span>
<span class="sd">        Predicted parameters.</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Number of observations</span>
    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Predicted Parameters</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">predt</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>

    <span class="c1"># Replace NaNs and infinity values with unconditional start values</span>
    <span class="n">nan_inf_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span>
    <span class="n">predt</span><span class="p">[</span><span class="n">nan_inf_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">start_values</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">nan_inf_mask</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Convert to torch.tensor</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Predicted Parameters transformed to response scale</span>
    <span class="n">predt_transformed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_transform</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="n">n_obs</span><span class="p">)</span>

    <span class="c1"># Specify Distribution and Loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;Dirichlet&quot;</span><span class="p">:</span>
        <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="p">[</span><span class="n">predt_transformed</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">predt_transformed</span><span class="p">))</span>
    <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.loss_fn_start_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">loss_fn_start_values</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that calculates the loss for a given set of distributional parameters. Only used for calculating
the loss for the start values.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.loss_fn_start_values--parameter">Parameter</h6>
<p>params: torch.Tensor
    Distributional parameters.
target: torch.Tensor
    Target values.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.loss_fn_start_values--returns">Returns</h6>
<p>loss: torch.Tensor
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">loss_fn_start_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                         <span class="n">params</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                         <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that calculates the loss for a given set of distributional parameters. Only used for calculating</span>
<span class="sd">    the loss for the start values.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    params: torch.Tensor</span>
<span class="sd">        Distributional parameters.</span>
<span class="sd">    target: torch.Tensor</span>
<span class="sd">        Target values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    loss: torch.Tensor</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Replace NaNs and infinity values with 0.5</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">params</span>
    <span class="p">]</span>

    <span class="c1"># Transform parameters to response scale</span>
    <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_transform</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Specify Distribution and Loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;Dirichlet&quot;</span><span class="p">:</span>
        <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="p">[</span><span class="n">params</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">params</span><span class="p">))</span>
    <span class="n">dist_fit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">dist_fit</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.metric_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">metric_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that evaluates the predictions using the specified loss function.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.metric_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: xgb.DMatrix
    Data used for training.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.metric_fn--returns">Returns</h6>
<p>name: str
    Name of the evaluation metric.
loss: float
    Loss value.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">metric_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that evaluates the predictions using the specified loss function.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: xgb.DMatrix</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    name: str</span>
<span class="sd">        Name of the evaluation metric.</span>
<span class="sd">    loss: float</span>
<span class="sd">        Loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">))[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">]</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate loss</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.objective_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">objective_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function to estimate gradients and hessians of distributional parameters.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.objective_fn--arguments">Arguments</h6>
<p>predt: np.ndarray
    Predicted values.
data: xgb.DMatrix
    Data used for training.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.objective_fn--returns">Returns</h6>
<p>grad: np.ndarray
    Gradient.
hess: np.ndarray
    Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">objective_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predt</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to estimate gradients and hessians of distributional parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: np.ndarray</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    data: xgb.DMatrix</span>
<span class="sd">        Data used for training.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    grad: np.ndarray</span>
<span class="sd">        Gradient.</span>
<span class="sd">    hess: np.ndarray</span>
<span class="sd">        Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Target</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get_label</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">))[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">]</span>

    <span class="c1"># Weights</span>
    <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Use 1 as weight if no weights are specified</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_weight</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Start values (needed to replace NaNs in predt)</span>
    <span class="n">start_values</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_base_margin</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="c1"># Calculate gradients and hessians</span>
    <span class="n">predt</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params_loss</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_gradients_and_hessians</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">predt</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hess</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.predict_dist" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict_dist</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span> <span class="n">start_values</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">pred_type</span><span class="o">=</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that predicts from the trained model.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.predict_dist--arguments">Arguments</h6>
<p>booster : xgb.Booster
    Trained model.
start_values : np.ndarray
    Starting values for each distributional parameter.
data : xgb.DMatrix
    Data to predict from.
pred_type : str
    Type of prediction:
    - "samples" draws n_samples from the predicted distribution.
    - "quantiles" calculates the quantiles from the predicted distribution.
    - "parameters" returns the predicted distributional parameters.
    - "expectiles" returns the predicted expectiles.
n_samples : int
    Number of samples to draw from the predicted distribution.
quantiles : List[float]
    List of quantiles to calculate from the predicted distribution.
seed : int
    Seed for random number generator used to draw samples from the predicted distribution.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.predict_dist--returns">Returns</h6>
<p>pred : pd.DataFrame
    Predictions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict_dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">booster</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">Booster</span><span class="p">,</span>
                 <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">,</span>
                 <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
                 <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                 <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                 <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that predicts from the trained model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    booster : xgb.Booster</span>
<span class="sd">        Trained model.</span>
<span class="sd">    start_values : np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    data : xgb.DMatrix</span>
<span class="sd">        Data to predict from.</span>
<span class="sd">    pred_type : str</span>
<span class="sd">        Type of prediction:</span>
<span class="sd">        - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">        - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">        - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">        - &quot;expectiles&quot; returns the predicted expectiles.</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to draw from the predicted distribution.</span>
<span class="sd">    quantiles : List[float]</span>
<span class="sd">        List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Seed for random number generator used to draw samples from the predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred : pd.DataFrame</span>
<span class="sd">        Predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Set base_margin as starting point for each distributional parameter. Requires base_score=0 in parameters.</span>
    <span class="n">base_margin_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
    <span class="n">data</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_pred</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

    <span class="c1"># Predict from model</span>
    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">num_row</span><span class="p">()</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">booster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">output_margin</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">predt</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)]</span>
    <span class="n">dist_params_predt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_transform</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_dict</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="n">n_obs</span><span class="p">)</span>

    <span class="c1"># Predicted Distributional Parameters</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;Dirichlet&quot;</span><span class="p">:</span>
        <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="p">[</span><span class="n">dist_params_predt</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dist_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="p">,</span> <span class="n">dist_params_predt</span><span class="p">))</span>
    <span class="n">dist_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="p">(</span><span class="o">**</span><span class="n">dist_kwargs</span><span class="p">)</span>

    <span class="c1"># Draw samples from predicted response distribution</span>
    <span class="n">pred_samples_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">draw_samples</span><span class="p">(</span><span class="n">dist_pred</span><span class="o">=</span><span class="n">dist_pred</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Get predicted distributional parameters</span>
    <span class="n">predt_params_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_dist_params</span><span class="p">(</span><span class="n">n_targets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="n">dist_pred</span><span class="o">=</span><span class="n">dist_pred</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;parameters&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">predt_params_df</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;samples&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pred_samples_df</span>

    <span class="k">elif</span> <span class="n">pred_type</span> <span class="o">==</span> <span class="s2">&quot;quantiles&quot;</span><span class="p">:</span>
        <span class="c1"># Calculate quantiles from predicted response distribution</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
        <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_samples_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">)</span>
        <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="s2">&quot;quant_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">quantiles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">quantiles</span><span class="p">))]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">discrete</span><span class="p">:</span>
            <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pred_quant_df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">pred_quant_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">targets</span><span class="p">,</span> <span class="n">pred_quant_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">pred_quant_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.stabilize_derivative" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">stabilize_derivative</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;MAD&#39;</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that stabilizes Gradients and Hessians.</p>
<p>As XGBoostLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important
that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,
the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.
Another way to improve convergence might be to standardize the response variable. This is especially useful if the
range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and
the standardization of the response are not always advised but need to be carefully considered.
Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.stabilize_derivative--parameters">Parameters</h6>
<p>input_der : torch.Tensor
    Input derivative, either Gradient or Hessian.
type: str
    Stabilization method. Can be either "None", "MAD" or "L2".</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.stabilize_derivative--returns">Returns</h6>
<p>stab_der : torch.Tensor
    Stabilized Gradient or Hessian.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">stabilize_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_der</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;MAD&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that stabilizes Gradients and Hessians.</span>

<span class="sd">    As XGBoostLSS updates the parameter estimates by optimizing Gradients and Hessians, it is important</span>
<span class="sd">    that these are comparable in magnitude for all distributional parameters. Due to imbalances regarding the ranges,</span>
<span class="sd">    the estimation might become unstable so that it does not converge (or converge very slowly) to the optimal solution.</span>
<span class="sd">    Another way to improve convergence might be to standardize the response variable. This is especially useful if the</span>
<span class="sd">    range of the response differs strongly from the range of the Gradients and Hessians. Both, the stabilization and</span>
<span class="sd">    the standardization of the response are not always advised but need to be carefully considered.</span>
<span class="sd">    Source: https://github.com/boost-R/gamboostLSS/blob/7792951d2984f289ed7e530befa42a2a4cb04d1d/R/helpers.R#L173</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_der : torch.Tensor</span>
<span class="sd">        Input derivative, either Gradient or Hessian.</span>
<span class="sd">    type: str</span>
<span class="sd">        Stabilization method. Can be either &quot;None&quot;, &quot;MAD&quot; or &quot;L2&quot;.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    stab_der : torch.Tensor</span>
<span class="sd">        Stabilized Gradient or Hessian.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;MAD&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">input_der</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;L2&quot;</span><span class="p">:</span>
        <span class="n">input_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">input_der</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-04</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">),</span> <span class="n">div</span><span class="p">)</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">input_der</span> <span class="o">/</span> <span class="n">div</span>

    <span class="k">if</span> <span class="nb">type</span> <span class="o">==</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="n">stab_der</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">input_der</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nansum</span><span class="p">(</span><span class="n">input_der</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">stab_der</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h5 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.target_append" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">target_append</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">,</span> <span class="n">n_dist_param</span><span class="p">)</span></code>

</h5>


    <div class="doc doc-contents ">

        <p>Function that appends target to the number of specified parameters.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.target_append--arguments">Arguments</h6>
<p>target: np.ndarray
    Target variables.
n_targets: int
    Number of targets.
n_dist_param: int
    Number of distribution parameters.</p>
<h6 id="xgboostlss.distributions.multivariate_distribution_utils.Multivariate_DistributionClass.target_append--returns">Returns</h6>
<p>label: np.ndarray
    Array with appended targets.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/distributions/multivariate_distribution_utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">target_append</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">target</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                  <span class="n">n_targets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                  <span class="n">n_dist_param</span><span class="p">:</span> <span class="nb">int</span>
                  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that appends target to the number of specified parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    target: np.ndarray</span>
<span class="sd">        Target variables.</span>
<span class="sd">    n_targets: int</span>
<span class="sd">        Number of targets.</span>
<span class="sd">    n_dist_param: int</span>
<span class="sd">        Number of distribution parameters.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    label: np.ndarray</span>
<span class="sd">        Array with appended targets.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_targets</span><span class="p">)</span>
    <span class="n">n_obs</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_fill</span> <span class="o">=</span> <span class="n">n_dist_param</span> <span class="o">-</span> <span class="n">n_targets</span>
    <span class="n">np_fill</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_obs</span><span class="p">,</span> <span class="n">n_fill</span><span class="p">))</span>
    <span class="n">label_append</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">label</span><span class="p">,</span> <span class="n">np_fill</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_dist_param</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">label_append</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h3 id="xgboostlss.distributions.zero_inflated" class="doc doc-heading">
            <code>zero_inflated</code>


</h3>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.zero_inflated.ZeroAdjustedBeta" class="doc doc-heading">
            <code>ZeroAdjustedBeta</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="ZeroInflatedDistribution (xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution)" href="#xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a></code></p>



        <p>A Zero-Adjusted Beta distribution.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroAdjustedBeta--parameter">Parameter</h6>
<p>concentration1: torch.Tensor
    1st concentration parameter of the distribution (often referred to as alpha).
concentration0: torch.Tensor
    2nd concentration parameter of the distribution (often referred to as beta).
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroAdjustedBeta--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroAdjustedBeta</span><span class="p">(</span><span class="n">ZeroInflatedDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Zero-Adjusted Beta distribution.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    concentration1: torch.Tensor</span>
<span class="sd">        1st concentration parameter of the distribution (often referred to as alpha).</span>
<span class="sd">    concentration0: torch.Tensor</span>
<span class="sd">        2nd concentration parameter of the distribution (often referred to as beta).</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;concentration1&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;concentration0&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">support</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">concentration1</span><span class="p">,</span> <span class="n">concentration0</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="n">concentration1</span><span class="o">=</span><span class="n">concentration1</span><span class="p">,</span> <span class="n">concentration0</span><span class="o">=</span><span class="n">concentration0</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">base_dist</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="n">validate_args</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">concentration1</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">concentration1</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">concentration0</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">concentration0</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.zero_inflated.ZeroAdjustedGamma" class="doc doc-heading">
            <code>ZeroAdjustedGamma</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="ZeroInflatedDistribution (xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution)" href="#xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a></code></p>



        <p>A Zero-Adjusted Gamma distribution.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroAdjustedGamma--parameter">Parameter</h6>
<p>concentration: torch.Tensor
    shape parameter of the distribution (often referred to as alpha)
rate: torch.Tensor
    rate = 1 / scale of the distribution (often referred to as beta)
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroAdjustedGamma--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroAdjustedGamma</span><span class="p">(</span><span class="n">ZeroInflatedDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Zero-Adjusted Gamma distribution.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    concentration: torch.Tensor</span>
<span class="sd">        shape parameter of the distribution (often referred to as alpha)</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        rate = 1 / scale of the distribution (often referred to as beta)</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;concentration&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">support</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">nonnegative</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">concentration</span><span class="o">=</span><span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">base_dist</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="n">validate_args</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">concentration</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">concentration</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">rate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">rate</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.zero_inflated.ZeroAdjustedLogNormal" class="doc doc-heading">
            <code>ZeroAdjustedLogNormal</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="ZeroInflatedDistribution (xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution)" href="#xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a></code></p>



        <p>A Zero-Adjusted Log-Normal distribution.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroAdjustedLogNormal--parameter">Parameter</h6>
<p>loc: torch.Tensor
    Mean of log of distribution.
scale: torch.Tensor
    Standard deviation of log of the distribution.
gate: torch.Tensor
    Probability of zeros given via a Bernoulli distribution.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroAdjustedLogNormal--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroAdjustedLogNormal</span><span class="p">(</span><span class="n">ZeroInflatedDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Zero-Adjusted Log-Normal distribution.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    loc: torch.Tensor</span>
<span class="sd">        Mean of log of distribution.</span>
<span class="sd">    scale: torch.Tensor</span>
<span class="sd">        Standard deviation of log of the distribution.</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;loc&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">real</span><span class="p">,</span>
        <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">support</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">nonnegative</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="n">LogNormal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">base_dist</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="n">validate_args</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">loc</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">loc</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">scale</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">scale</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution" class="doc doc-heading">
            <code>ZeroInflatedDistribution</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="pyro.distributions.TorchDistribution">TorchDistribution</span></code></p>



        <p>Generic Zero Inflated distribution.</p>
<p>This can be used directly or can be used as a base class as e.g. for
:class:<code>ZeroInflatedPoisson</code> and :class:<code>ZeroInflatedNegativeBinomial</code>.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution--parameters">Parameters</h6>
<p>gate : torch.Tensor
    Probability of extra zeros given via a Bernoulli distribution.
base_dist : torch.distributions.Distribution
    The base distribution.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L18</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroInflatedDistribution</span><span class="p">(</span><span class="n">TorchDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generic Zero Inflated distribution.</span>

<span class="sd">    This can be used directly or can be used as a base class as e.g. for</span>
<span class="sd">    :class:`ZeroInflatedPoisson` and :class:`ZeroInflatedNegativeBinomial`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    gate : torch.Tensor</span>
<span class="sd">        Probability of extra zeros given via a Bernoulli distribution.</span>
<span class="sd">    base_dist : torch.distributions.Distribution</span>
<span class="sd">        The base distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L18</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
        <span class="s2">&quot;gate_logits&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">real</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_dist</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gate_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">gate</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">gate_logits</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Either `gate` or `gate_logits` must be specified, but not both.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">gate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">broadcast_shape</span><span class="p">(</span><span class="n">gate</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">base_dist</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gate</span> <span class="o">=</span> <span class="n">gate</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">broadcast_shape</span><span class="p">(</span><span class="n">gate_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">base_dist</span><span class="o">.</span><span class="n">batch_shape</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gate_logits</span> <span class="o">=</span> <span class="n">gate_logits</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">base_dist</span><span class="o">.</span><span class="n">event_shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;ZeroInflatedDistribution expected empty &quot;</span>
                <span class="s2">&quot;base_dist.event_shape but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">base_dist</span><span class="o">.</span><span class="n">event_shape</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span> <span class="o">=</span> <span class="n">base_dist</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="n">event_shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">()</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">,</span> <span class="n">event_shape</span><span class="p">,</span> <span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@constraints</span><span class="o">.</span><span class="n">dependent_property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">support</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">support</span>

    <span class="nd">@lazy_property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">gate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">logits_to_probs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_logits</span><span class="p">)</span>

    <span class="nd">@lazy_property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">gate_logits</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">probs_to_logits</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_args</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_validate_sample</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">zero_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">value</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">support</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">support</span>
        <span class="n">epsilon</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="s2">&quot;lower_bound&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">is_identically_zero</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="s2">&quot;lower_bound&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)):</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="n">epsilon</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="s2">&quot;upper_bound&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">is_identically_one</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">support</span><span class="p">,</span> <span class="s2">&quot;upper_bound&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">):</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;gate&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">:</span>
            <span class="n">gate</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">broadcast_all</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">gate</span><span class="p">)</span><span class="o">.</span><span class="n">log1p</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">zero_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">gate</span> <span class="o">+</span> <span class="n">log_prob</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span><span class="o">.</span><span class="n">log</span><span class="p">(),</span> <span class="n">log_prob</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gate_logits</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">broadcast_all</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_logits</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="n">log_prob_minus_log_gate</span> <span class="o">=</span> <span class="o">-</span><span class="n">gate_logits</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="n">log_gate</span> <span class="o">=</span> <span class="o">-</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">gate_logits</span><span class="p">)</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">log_prob_minus_log_gate</span> <span class="o">+</span> <span class="n">log_gate</span>
            <span class="n">zero_log_prob</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">log_prob_minus_log_gate</span><span class="p">)</span> <span class="o">+</span> <span class="n">log_gate</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">zero_idx</span><span class="p">,</span> <span class="n">zero_log_prob</span><span class="p">,</span> <span class="n">log_prob</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">log_prob</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">()):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extended_shape</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">samples</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(()),</span> <span class="n">samples</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">samples</span>

    <span class="nd">@lazy_property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">mean</span>

    <span class="nd">@lazy_property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">mean</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">variance</span>
        <span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="o">**</span><span class="mi">2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">expand</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_shape</span><span class="p">,</span> <span class="n">_instance</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">new</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_checked_instance</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span> <span class="n">_instance</span><span class="p">)</span>
        <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="n">gate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span> <span class="k">if</span> <span class="s2">&quot;gate&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">gate_logits</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gate_logits</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;gate_logits&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_shape</span><span class="p">)</span>
        <span class="n">ZeroInflatedDistribution</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">new</span><span class="p">,</span> <span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">gate_logits</span><span class="o">=</span><span class="n">gate_logits</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="n">new</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_args</span>
        <span class="k">return</span> <span class="n">new</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.zero_inflated.ZeroInflatedNegativeBinomial" class="doc doc-heading">
            <code>ZeroInflatedNegativeBinomial</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="ZeroInflatedDistribution (xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution)" href="#xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a></code></p>



        <p>A Zero Inflated Negative Binomial distribution.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroInflatedNegativeBinomial--parameter">Parameter</h6>
<p>total_count: torch.Tensor
    Non-negative number of negative Bernoulli trial.
probs: torch.Tensor
    Event probabilities of success in the half open interval [0, 1).
logits: torch.Tensor
    Event log-odds of success (log(p/(1-p))).
gate: torch.Tensor
    Probability of extra zeros given via a Bernoulli distribution.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroInflatedNegativeBinomial--source">Source</h6>
<ul>
<li>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</li>
</ul>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroInflatedNegativeBinomial</span><span class="p">(</span><span class="n">ZeroInflatedDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Zero Inflated Negative Binomial distribution.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    total_count: torch.Tensor</span>
<span class="sd">        Non-negative number of negative Bernoulli trial.</span>
<span class="sd">    probs: torch.Tensor</span>
<span class="sd">        Event probabilities of success in the half open interval [0, 1).</span>
<span class="sd">    logits: torch.Tensor</span>
<span class="sd">        Event log-odds of success (log(p/(1-p))).</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of extra zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    - https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L150</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;total_count&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">greater_than_eq</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
        <span class="s2">&quot;probs&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">half_open_interval</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
        <span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">real</span><span class="p">,</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">support</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">nonnegative_integer</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">total_count</span><span class="p">,</span> <span class="n">probs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="n">NegativeBinomial</span><span class="p">(</span><span class="n">total_count</span><span class="o">=</span><span class="n">total_count</span><span class="p">,</span> <span class="n">probs</span><span class="o">=</span><span class="n">probs</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">base_dist</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="n">validate_args</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">total_count</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">total_count</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">probs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">probs</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">logits</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">logits</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h4 id="xgboostlss.distributions.zero_inflated.ZeroInflatedPoisson" class="doc doc-heading">
            <code>ZeroInflatedPoisson</code>


</h4>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="ZeroInflatedDistribution (xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution)" href="#xgboostlss.distributions.zero_inflated.ZeroInflatedDistribution">ZeroInflatedDistribution</a></code></p>



        <p>A Zero-Inflated Poisson distribution.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroInflatedPoisson--parameter">Parameter</h6>
<p>rate: torch.Tensor
    The rate of the Poisson distribution.
gate: torch.Tensor
    Probability of extra zeros given via a Bernoulli distribution.</p>
<h6 id="xgboostlss.distributions.zero_inflated.ZeroInflatedPoisson--source">Source</h6>
<p>https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L121</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/distributions/zero_inflated.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">ZeroInflatedPoisson</span><span class="p">(</span><span class="n">ZeroInflatedDistribution</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Zero-Inflated Poisson distribution.</span>

<span class="sd">    Parameter</span>
<span class="sd">    ---------</span>
<span class="sd">    rate: torch.Tensor</span>
<span class="sd">        The rate of the Poisson distribution.</span>
<span class="sd">    gate: torch.Tensor</span>
<span class="sd">        Probability of extra zeros given via a Bernoulli distribution.</span>

<span class="sd">    Source</span>
<span class="sd">    ------</span>
<span class="sd">    https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/zero_inflated.py#L121</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arg_constraints</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;rate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span>
        <span class="s2">&quot;gate&quot;</span><span class="p">:</span> <span class="n">constraints</span><span class="o">.</span><span class="n">unit_interval</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">support</span> <span class="o">=</span> <span class="n">constraints</span><span class="o">.</span><span class="n">nonnegative_integer</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">base_dist</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">rate</span><span class="o">=</span><span class="n">rate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">base_dist</span><span class="o">.</span><span class="n">_validate_args</span> <span class="o">=</span> <span class="n">validate_args</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">base_dist</span><span class="p">,</span> <span class="n">gate</span><span class="o">=</span><span class="n">gate</span><span class="p">,</span> <span class="n">validate_args</span><span class="o">=</span><span class="n">validate_args</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">rate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_dist</span><span class="o">.</span><span class="n">rate</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">












  </div>

    </div>


</div>




  </div>

    </div>

</div>


  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="xgboostlss.model" class="doc doc-heading">
            <code>model</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="xgboostlss.model.XGBoostLSS" class="doc doc-heading">
            <code>XGBoostLSS</code>


</h3>


    <div class="doc doc-contents ">



        <p>XGBoostLSS model class</p>
<h5 id="xgboostlss.model.XGBoostLSS--parameters">Parameters</h5>
<p>dist : Distribution
    DistributionClass object.
start_values : np.ndarray
    Starting values for each distributional parameter.</p>







              <details class="quote">
                <summary>Source code in <code>xgboostlss/model.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">XGBoostLSS</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    XGBoostLSS model class</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dist : Distribution</span>
<span class="sd">        DistributionClass object.</span>
<span class="sd">    start_values : np.ndarray</span>
<span class="sd">        Starting values for each distributional parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dist</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">dist</span>             <span class="c1"># Distribution object</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="o">=</span> <span class="kc">None</span>     <span class="c1"># Starting values for distributional parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multivariate_label_expand</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multivariate_eval_label_expand</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_params_adj</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set parameters for distributional model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        params : Dict[str, Any]</span>
<span class="sd">            Parameters for model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        params : Dict[str, Any]</span>
<span class="sd">            Updated Parameters for model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params_adj</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s2">&quot;base_score&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="s2">&quot;num_target&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span>
            <span class="s2">&quot;disable_default_eval_metric&quot;</span><span class="p">:</span> <span class="kc">True</span>
        <span class="p">}</span>
        <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params_adj</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">adjust_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dmatrix</span><span class="p">:</span> <span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adjust labels for multivariate distributions.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        dmatrix : DMatrix</span>
<span class="sd">            DMatrix object.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">univariate</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">multivariate_label_expand</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">multivariate_label_expand</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">target_append</span><span class="p">(</span>
                <span class="n">dmatrix</span><span class="o">.</span><span class="n">get_label</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span>
            <span class="p">)</span>
            <span class="n">dmatrix</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_base_margin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dmatrix</span><span class="p">:</span> <span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set base margin for distributions.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        dmatrix : DMatrix</span>
<span class="sd">            DMatrix object.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">initialize</span><span class="p">:</span>
                <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">dmatrix</span><span class="o">.</span><span class="n">get_label</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span>
        <span class="n">base_margin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">dmatrix</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span>
        <span class="n">dmatrix</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">dtrain</span><span class="p">:</span> <span class="n">DMatrix</span><span class="p">,</span>
        <span class="n">num_boost_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">evals</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">DMatrix</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">early_stopping_rounds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">evals_result</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TrainingCallback</span><span class="o">.</span><span class="n">EvalsLog</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose_eval</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">xgb_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">,</span> <span class="n">Booster</span><span class="p">,</span> <span class="nb">bytearray</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">TrainingCallback</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Booster</span><span class="p">:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Train a booster with given parameters.</span>

<span class="sd">            Arguments</span>
<span class="sd">            ---------</span>
<span class="sd">            params :</span>
<span class="sd">                Booster params.</span>
<span class="sd">            dtrain :</span>
<span class="sd">                Data to be trained.</span>
<span class="sd">            num_boost_round :</span>
<span class="sd">                Number of boosting iterations.</span>
<span class="sd">            evals :</span>
<span class="sd">                List of validation sets for which metrics will evaluated during training.</span>
<span class="sd">                Validation metrics will help us track the performance of the model.</span>
<span class="sd">            early_stopping_rounds :</span>
<span class="sd">                Activates early stopping. Validation metric needs to improve at least once in</span>
<span class="sd">                every **early_stopping_rounds** round(s) to continue training.</span>
<span class="sd">                Requires at least one item in **evals**.</span>
<span class="sd">                The method returns the model from the last iteration (not the best one).  Use</span>
<span class="sd">                custom callback or model slicing if the best model is desired.</span>
<span class="sd">                If there&#39;s more than one item in **evals**, the last entry will be used for early</span>
<span class="sd">                stopping.</span>
<span class="sd">                If there&#39;s more than one metric in the **eval_metric** parameter given in</span>
<span class="sd">                **params**, the last metric will be used for early stopping.</span>
<span class="sd">                If early stopping occurs, the model will have two additional fields:</span>
<span class="sd">                ``bst.best_score``, ``bst.best_iteration``.</span>
<span class="sd">            evals_result :</span>
<span class="sd">                This dictionary stores the evaluation results of all the items in watchlist.</span>
<span class="sd">                Example: with a watchlist containing</span>
<span class="sd">                ``[(dtest,&#39;eval&#39;), (dtrain,&#39;train&#39;)]`` and</span>
<span class="sd">                a parameter containing ``(&#39;eval_metric&#39;: &#39;logloss&#39;)``,</span>
<span class="sd">                the **evals_result** returns</span>
<span class="sd">                .. code-block:: python</span>
<span class="sd">                    {&#39;train&#39;: {&#39;logloss&#39;: [&#39;0.48253&#39;, &#39;0.35953&#39;]},</span>
<span class="sd">                     &#39;eval&#39;: {&#39;logloss&#39;: [&#39;0.480385&#39;, &#39;0.357756&#39;]}}</span>
<span class="sd">            verbose_eval :</span>
<span class="sd">                Requires at least one item in **evals**.</span>
<span class="sd">                If **verbose_eval** is True then the evaluation metric on the validation set is</span>
<span class="sd">                printed at each boosting stage.</span>
<span class="sd">                If **verbose_eval** is an integer then the evaluation metric on the validation set</span>
<span class="sd">                is printed at every given **verbose_eval** boosting stage. The last boosting stage</span>
<span class="sd">                / the boosting stage found by using **early_stopping_rounds** is also printed.</span>
<span class="sd">                Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric</span>
<span class="sd">                is printed every 4 boosting stages, instead of every boosting stage.</span>
<span class="sd">            xgb_model :</span>
<span class="sd">                Xgb model to be loaded before training (allows training continuation).</span>
<span class="sd">            callbacks :</span>
<span class="sd">                List of callback functions that are applied at end of each iteration.</span>
<span class="sd">                It is possible to use predefined callbacks by using</span>
<span class="sd">                :ref:`Callback API &lt;callback_api&gt;`.</span>
<span class="sd">                .. note::</span>
<span class="sd">                   States in callback are not preserved during training, which means callback</span>
<span class="sd">                   objects can not be reused for multiple training sessions without</span>
<span class="sd">                   reinitialization or deepcopy.</span>
<span class="sd">                .. code-block:: python</span>
<span class="sd">                    for params in parameters_grid:</span>
<span class="sd">                        # be sure to (re)initialize the callbacks before each run</span>
<span class="sd">                        callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]</span>
<span class="sd">                        xgboost.train(params, Xy, callbacks=callbacks)</span>

<span class="sd">            Returns</span>
<span class="sd">            -------</span>
<span class="sd">            Booster:</span>
<span class="sd">                The trained booster model.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_params_adj</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">adjust_labels</span><span class="p">(</span><span class="n">dtrain</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">dtrain</span><span class="p">)</span>

            <span class="c1"># Set base_margin for evals</span>
            <span class="k">if</span> <span class="n">evals</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">evals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_eval_margin</span><span class="p">(</span><span class="n">evals</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">booster</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
                <span class="n">params</span><span class="p">,</span>
                <span class="n">dtrain</span><span class="p">,</span>
                <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
                <span class="n">evals</span><span class="o">=</span><span class="n">evals</span><span class="p">,</span>
                <span class="n">obj</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">objective_fn</span><span class="p">,</span>
                <span class="n">custom_metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">metric_fn</span><span class="p">,</span>
                <span class="n">xgb_model</span><span class="o">=</span><span class="n">xgb_model</span><span class="p">,</span>
                <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
                <span class="n">verbose_eval</span><span class="o">=</span><span class="n">verbose_eval</span><span class="p">,</span>
                <span class="n">evals_result</span><span class="o">=</span><span class="n">evals_result</span><span class="p">,</span>
                <span class="n">maximize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="n">early_stopping_rounds</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">cv</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">dtrain</span><span class="p">:</span> <span class="n">DMatrix</span><span class="p">,</span>
        <span class="n">num_boost_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">nfold</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">stratified</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">folds</span><span class="p">:</span> <span class="n">XGBStratifiedKFold</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">early_stopping_rounds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fpreproc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FPreProcCallable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">as_pandas</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">verbose_eval</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">show_stdv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">TrainingCallback</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># pylint: disable = invalid-name</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Cross-validation with given parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ----------</span>
<span class="sd">        params : dict</span>
<span class="sd">            Booster params.</span>
<span class="sd">        dtrain : DMatrix</span>
<span class="sd">            Data to be trained.</span>
<span class="sd">        num_boost_round : int</span>
<span class="sd">            Number of boosting iterations.</span>
<span class="sd">        nfold : int</span>
<span class="sd">            Number of folds in CV.</span>
<span class="sd">        stratified : bool</span>
<span class="sd">            Perform stratified sampling.</span>
<span class="sd">        folds : a KFold or StratifiedKFold instance or list of fold indices</span>
<span class="sd">            Sklearn KFolds or StratifiedKFolds object.</span>
<span class="sd">            Alternatively may explicitly pass sample indices for each fold.</span>
<span class="sd">            For ``n`` folds, **folds** should be a length ``n`` list of tuples.</span>
<span class="sd">            Each tuple is ``(in,out)`` where ``in`` is a list of indices to be used</span>
<span class="sd">            as the training samples for the ``n`` th fold and ``out`` is a list of</span>
<span class="sd">            indices to be used as the testing samples for the ``n`` th fold.</span>
<span class="sd">        early_stopping_rounds: int</span>
<span class="sd">            Activates early stopping. Cross-Validation metric (average of validation</span>
<span class="sd">            metric computed over CV folds) needs to improve at least once in</span>
<span class="sd">            every **early_stopping_rounds** round(s) to continue training.</span>
<span class="sd">            The last entry in the evaluation history will represent the best iteration.</span>
<span class="sd">            If there&#39;s more than one metric in the **eval_metric** parameter given in</span>
<span class="sd">            **params**, the last metric will be used for early stopping.</span>
<span class="sd">        fpreproc : function</span>
<span class="sd">            Preprocessing function that takes (dtrain, dtest, param) and returns</span>
<span class="sd">            transformed versions of those.</span>
<span class="sd">        as_pandas : bool, default True</span>
<span class="sd">            Return pd.DataFrame when pandas is installed.</span>
<span class="sd">            If False or pandas is not installed, return np.ndarray</span>
<span class="sd">        verbose_eval : bool, int, or None, default None</span>
<span class="sd">            Whether to display the progress. If None, progress will be displayed</span>
<span class="sd">            when np.ndarray is returned. If True, progress will be displayed at</span>
<span class="sd">            boosting stage. If an integer is given, progress will be displayed</span>
<span class="sd">            at every given `verbose_eval` boosting stage.</span>
<span class="sd">        show_stdv : bool, default True</span>
<span class="sd">            Whether to display the standard deviation in progress.</span>
<span class="sd">            Results are not affected, and always contains std.</span>
<span class="sd">        seed : int</span>
<span class="sd">            Seed used to generate the folds (passed to numpy.random.seed).</span>
<span class="sd">        callbacks :</span>
<span class="sd">            List of callback functions that are applied at end of each iteration.</span>
<span class="sd">            It is possible to use predefined callbacks by using</span>
<span class="sd">            :ref:`Callback API &lt;callback_api&gt;`.</span>
<span class="sd">            .. note::</span>
<span class="sd">               States in callback are not preserved during training, which means callback</span>
<span class="sd">               objects can not be reused for multiple training sessions without</span>
<span class="sd">               reinitialization or deepcopy.</span>
<span class="sd">            .. code-block:: python</span>
<span class="sd">                for params in parameters_grid:</span>
<span class="sd">                    # be sure to (re)initialize the callbacks before each run</span>
<span class="sd">                    callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]</span>
<span class="sd">                    xgboost.train(params, Xy, callbacks=callbacks)</span>
<span class="sd">        shuffle : bool</span>
<span class="sd">            Shuffle data before creating folds.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        evaluation history : list(string)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_params_adj</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adjust_labels</span><span class="p">(</span><span class="n">dtrain</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">dtrain</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cv_booster</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
            <span class="n">params</span><span class="p">,</span>
            <span class="n">dtrain</span><span class="p">,</span>
            <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
            <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
            <span class="n">stratified</span><span class="o">=</span><span class="n">stratified</span><span class="p">,</span>
            <span class="n">folds</span><span class="o">=</span><span class="n">folds</span><span class="p">,</span>
            <span class="n">obj</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">objective_fn</span><span class="p">,</span>
            <span class="n">custom_metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">metric_fn</span><span class="p">,</span>
            <span class="n">maximize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="n">early_stopping_rounds</span><span class="p">,</span>
            <span class="n">fpreproc</span><span class="o">=</span><span class="n">fpreproc</span><span class="p">,</span>
            <span class="n">as_pandas</span><span class="o">=</span><span class="n">as_pandas</span><span class="p">,</span>
            <span class="n">verbose_eval</span><span class="o">=</span><span class="n">verbose_eval</span><span class="p">,</span>
            <span class="n">show_stdv</span><span class="o">=</span><span class="n">show_stdv</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv_booster</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">hyper_opt</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hp_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
        <span class="n">dtrain</span><span class="p">:</span> <span class="n">DMatrix</span><span class="p">,</span>
        <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
        <span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">max_minutes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">n_trials</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">study_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">silence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">hp_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function to tune hyperparameters using optuna.</span>

<span class="sd">        Requires optuna and optuna-integration to be installed.</span>
<span class="sd">        Install via pip install ``xgboostlss[all_extras]``.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ----------</span>
<span class="sd">        hp_dict: dict</span>
<span class="sd">            Dictionary of hyperparameters to tune.</span>
<span class="sd">        dtrain: xgb.DMatrix</span>
<span class="sd">            Training data.</span>
<span class="sd">        num_boost_round: int</span>
<span class="sd">            Number of boosting iterations.</span>
<span class="sd">        nfold: int</span>
<span class="sd">            Number of folds in CV.</span>
<span class="sd">        early_stopping_rounds: int</span>
<span class="sd">            Activates early stopping. Cross-Validation metric (average of validation</span>
<span class="sd">            metric computed over CV folds) needs to improve at least once in</span>
<span class="sd">            every **early_stopping_rounds** round(s) to continue training.</span>
<span class="sd">            The last entry in the evaluation history will represent the best iteration.</span>
<span class="sd">            If there&#39;s more than one metric in the **eval_metric** parameter given in</span>
<span class="sd">            **params**, the last metric will be used for early stopping.</span>
<span class="sd">        max_minutes: int</span>
<span class="sd">            Time budget in minutes, i.e., stop study after the given number of minutes.</span>
<span class="sd">        n_trials: int</span>
<span class="sd">            The number of trials. If this argument is set to None, there is no limitation on the number of trials.</span>
<span class="sd">        study_name: str</span>
<span class="sd">            Name of the hyperparameter study.</span>
<span class="sd">        silence: bool</span>
<span class="sd">            Controls the verbosity of the trail, i.e., user can silence the outputs of the trail.</span>
<span class="sd">        seed: int</span>
<span class="sd">            Seed used to generate the folds (passed to numpy.random.seed).</span>
<span class="sd">        hp_seed: int</span>
<span class="sd">            Seed for random number generator used in the Bayesian hyper-parameter search.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        opt_params : dict</span>
<span class="sd">            Optimal hyper-parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;XGBoostLSS.hyper_opt requires &#39;optuna&#39; and &#39;optuna-integration&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;optuna&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">optuna.samplers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TPESampler</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

            <span class="n">hyper_params</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param_value</span> <span class="ow">in</span> <span class="n">hp_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>

                <span class="n">param_type</span> <span class="o">=</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;categorical&quot;</span> <span class="ow">or</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
                    <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                        <span class="p">{</span><span class="n">param_name</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">1</span><span class="p">])}</span>
                    <span class="p">)</span>

                <span class="k">elif</span> <span class="n">param_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;float&quot;</span><span class="p">,</span> <span class="s2">&quot;int&quot;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;float&quot;</span><span class="p">:</span>
                        <span class="n">suggester</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span>
                    <span class="k">else</span><span class="p">:</span>  <span class="c1"># param_type == &quot;int&quot;</span>
                        <span class="n">suggester</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span>

                    <span class="n">param_constraints</span> <span class="o">=</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">param_low</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;low&quot;</span><span class="p">]</span>
                    <span class="n">param_high</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;high&quot;</span><span class="p">]</span>
                    <span class="n">param_log</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span>
                    <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                        <span class="p">{</span>
                            <span class="n">param_name</span><span class="p">:</span> <span class="n">suggester</span><span class="p">(</span>
                                <span class="n">param_name</span><span class="p">,</span>
                                <span class="n">low</span><span class="o">=</span><span class="n">param_low</span><span class="p">,</span>
                                <span class="n">high</span><span class="o">=</span><span class="n">param_high</span><span class="p">,</span>
                                <span class="n">log</span><span class="o">=</span><span class="n">param_log</span><span class="p">,</span>
                            <span class="p">)</span>
                        <span class="p">}</span>
                    <span class="p">)</span>

            <span class="c1"># Add booster if not included in dictionary</span>
            <span class="k">if</span> <span class="s2">&quot;booster&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">hyper_params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="p">{</span><span class="s2">&quot;booster&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s2">&quot;booster&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;gbtree&quot;</span><span class="p">])}</span>
                <span class="p">)</span>

            <span class="c1"># Add pruning</span>
            <span class="n">pruning_callback</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">integration</span><span class="o">.</span><span class="n">XGBoostPruningCallback</span><span class="p">(</span>
                <span class="n">trial</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;test-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

            <span class="n">xgblss_param_tuning</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
                <span class="n">params</span><span class="o">=</span><span class="n">hyper_params</span><span class="p">,</span>
                <span class="n">dtrain</span><span class="o">=</span><span class="n">dtrain</span><span class="p">,</span>
                <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
                <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
                <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="n">early_stopping_rounds</span><span class="p">,</span>
                <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">pruning_callback</span><span class="p">],</span>
                <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
                <span class="n">verbose_eval</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>

            <span class="c1"># Add the optimal number of rounds</span>
            <span class="n">opt_rounds</span> <span class="o">=</span> <span class="n">xgblss_param_tuning</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;test-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="si">}</span><span class="s2">-mean&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">trial</span><span class="o">.</span><span class="n">set_user_attr</span><span class="p">(</span><span class="s2">&quot;opt_round&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">opt_rounds</span><span class="p">))</span>

            <span class="c1"># Extract the best score</span>
            <span class="n">best_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">xgblss_param_tuning</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;test-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="si">}</span><span class="s2">-mean&quot;</span><span class="p">])</span>
            <span class="c1"># Replace -inf with 1e8 (to avoid -inf in the log)</span>
            <span class="n">best_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">best_score</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="mf">1e8</span><span class="p">),</span> <span class="n">best_score</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">best_score</span>

        <span class="k">if</span> <span class="n">study_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">study_name</span> <span class="o">=</span> <span class="s2">&quot;XGBoostLSS Hyper-Parameter Optimization&quot;</span>

        <span class="k">if</span> <span class="n">silence</span><span class="p">:</span>
            <span class="n">optuna</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">optuna</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">hp_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sampler</span> <span class="o">=</span> <span class="n">TPESampler</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">hp_seed</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sampler</span> <span class="o">=</span> <span class="n">TPESampler</span><span class="p">()</span>

        <span class="n">pruner</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">pruners</span><span class="o">.</span><span class="n">MedianPruner</span><span class="p">(</span><span class="n">n_startup_trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_warmup_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
        <span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">pruner</span><span class="o">=</span><span class="n">pruner</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;minimize&quot;</span><span class="p">,</span> <span class="n">study_name</span><span class="o">=</span><span class="n">study_name</span><span class="p">)</span>
        <span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span> <span class="o">*</span> <span class="n">max_minutes</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Hyper-Parameter Optimization successfully finished.&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Number of finished trials: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">study</span><span class="o">.</span><span class="n">trials</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Best trial:&quot;</span><span class="p">)</span>
        <span class="n">opt_param</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span>

        <span class="c1"># Add optimal stopping round</span>
        <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">trials_dataframe</span><span class="p">()[</span><span class="s2">&quot;user_attrs_opt_round&quot;</span><span class="p">][</span>
            <span class="n">study</span><span class="o">.</span><span class="n">trials_dataframe</span><span class="p">()[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>
        <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">])</span>

        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">opt_param</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Params: &quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    </span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">,</span>
        <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
        <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that predicts from the trained model.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        data : xgb.DMatrix</span>
<span class="sd">            Data to predict from.</span>
<span class="sd">        pred_type : str</span>
<span class="sd">            Type of prediction:</span>

<span class="sd">            - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">            - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">            - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">            - &quot;expectiles&quot; returns the predicted expectiles.</span>

<span class="sd">        n_samples : int</span>
<span class="sd">            Number of samples to draw from the predicted distribution.</span>
<span class="sd">        quantiles : List[float]</span>
<span class="sd">            List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">        seed : int</span>
<span class="sd">            Seed for random number generator used to draw samples</span>
<span class="sd">            from the predicted distribution.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        predt_df : pd.DataFrame</span>
<span class="sd">            Predictions.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Predict</span>
        <span class="n">predt_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">predict_dist</span><span class="p">(</span>
            <span class="n">booster</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">,</span>
            <span class="n">start_values</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">start_values</span><span class="p">,</span>
            <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
            <span class="n">pred_type</span><span class="o">=</span><span class="n">pred_type</span><span class="p">,</span>
            <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
            <span class="n">quantiles</span><span class="o">=</span><span class="n">quantiles</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">predt_df</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">X</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
             <span class="n">feature</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span>
             <span class="n">parameter</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;loc&quot;</span><span class="p">,</span>
             <span class="n">max_display</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
             <span class="n">plot_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">,</span>
            <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        XGBoostLSS SHap plotting function.</span>

<span class="sd">        Requires ``shap`` to be installed.</span>
<span class="sd">        Install via pip install ``xgboostlss[all_extras]``.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        X: pd.DataFrame</span>
<span class="sd">            Train/Test Data</span>
<span class="sd">        feature: str</span>
<span class="sd">            Specifies which feature is to be plotted.</span>
<span class="sd">        parameter: str</span>
<span class="sd">            Specifies which distributional parameter is to be plotted.</span>
<span class="sd">        max_display: int</span>
<span class="sd">            Specifies the maximum number of features to be displayed.</span>
<span class="sd">        plot_type: str</span>
<span class="sd">            Specifies the type of plot:</span>
<span class="sd">                &quot;Partial_Dependence&quot; plots the partial dependence of the parameter on the feature.</span>
<span class="sd">                &quot;Feature_Importance&quot; plots the feature importance of the parameter.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;XGBoostLSS.plot requires &#39;shap&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;shap&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">shap</span>

        <span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
        <span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">)</span>
        <span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">param_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">parameter</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">param_pos</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">param_pos</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Feature_Importance&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">max_display</span><span class="o">=</span><span class="n">max_display</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_display</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span>
                    <span class="n">shap_values</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">param_pos</span><span class="p">],</span> <span class="n">max_display</span><span class="o">=</span><span class="n">max_display</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_display</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">expectile_plot</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">X</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
            <span class="n">feature</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span>
            <span class="n">expectile</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;0.05&quot;</span><span class="p">,</span>
            <span class="n">plot_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">,</span>
        <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        XGBoostLSS function for plotting expectile SHapley values.</span>

<span class="sd">        Requires ``shap`` to be installed.</span>
<span class="sd">        Install via pip install ``xgboostlss[all_extras]``.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        X: pd.DataFrame</span>
<span class="sd">            Train/Test Data</span>
<span class="sd">        feature: str</span>
<span class="sd">            Specifies which feature to use for plotting Partial_Dependence plot.</span>
<span class="sd">        expectile: str</span>
<span class="sd">            Specifies which expectile to plot.</span>
<span class="sd">        plot_type: str</span>
<span class="sd">            Specifies which SHapley-plot to visualize. Currently, &quot;Partial_Dependence&quot; and &quot;Feature_Importance&quot;</span>
<span class="sd">            are supported.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;XGBoostLSS.expectile_plot requires &#39;shap&#39; &quot;</span>
            <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
            <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
            <span class="s2">&quot;the required dependencies.&quot;</span>
        <span class="p">)</span>
        <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;shap&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">shap</span>

        <span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
        <span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">)</span>
        <span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">expect_pos</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">expectile</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">expect_pos</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">expect_pos</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Feature_Importance&quot;</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">expect_pos</span><span class="p">],</span> <span class="n">max_display</span><span class="o">=</span><span class="mi">15</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">15</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_eval_margin</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">eval_set</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
        <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function that sets the base margin for the evaluation set.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        eval_set : list</span>
<span class="sd">            List of tuples containing the train and evaluation set.</span>
<span class="sd">        start_values : np.ndarray</span>
<span class="sd">            Array containing the start values for each distributional parameter.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        eval_set : list</span>
<span class="sd">            List of tuples containing the train and evaluation set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sets</span> <span class="o">=</span> <span class="p">[(</span><span class="n">item</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">eval_set</span><span class="p">]</span>

        <span class="n">eval_set1</span><span class="p">,</span> <span class="n">label1</span> <span class="o">=</span> <span class="n">sets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">eval_set2</span><span class="p">,</span> <span class="n">label2</span> <span class="o">=</span> <span class="n">sets</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Adjust labels to number of distributional parameters</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">univariate</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">multivariate_eval_label_expand</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">multivariate_eval_label_expand</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">eval_set2_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">target_append</span><span class="p">(</span><span class="n">eval_set2</span><span class="o">.</span><span class="n">get_label</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
            <span class="n">eval_set2</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">eval_set2_label</span><span class="p">)</span>

        <span class="c1"># Set base margins</span>
        <span class="n">base_margin_set1</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">eval_set1</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
        <span class="n">eval_set1</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_set1</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
        <span class="n">base_margin_set2</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">eval_set2</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
        <span class="n">eval_set2</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_set2</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

        <span class="n">eval_set</span> <span class="o">=</span> <span class="p">[(</span><span class="n">eval_set1</span><span class="p">,</span> <span class="n">label1</span><span class="p">),</span> <span class="p">(</span><span class="n">eval_set2</span><span class="p">,</span> <span class="n">label2</span><span class="p">)]</span>

        <span class="k">return</span> <span class="n">eval_set</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the model to a file.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model_path : str</span>
<span class="sd">            The path to save the model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load the model from a file.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model_path : str</span>
<span class="sd">            The path to the saved model.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        The loaded model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.adjust_labels" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">adjust_labels</span><span class="p">(</span><span class="n">dmatrix</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Adjust labels for multivariate distributions.</p>
<h6 id="xgboostlss.model.XGBoostLSS.adjust_labels--arguments">Arguments</h6>
<p>dmatrix : DMatrix
    DMatrix object.</p>
<h6 id="xgboostlss.model.XGBoostLSS.adjust_labels--returns">Returns</h6>
<p>None</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">adjust_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dmatrix</span><span class="p">:</span> <span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adjust labels for multivariate distributions.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    dmatrix : DMatrix</span>
<span class="sd">        DMatrix object.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">univariate</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">multivariate_label_expand</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multivariate_label_expand</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">target_append</span><span class="p">(</span>
            <span class="n">dmatrix</span><span class="o">.</span><span class="n">get_label</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span>
        <span class="p">)</span>
        <span class="n">dmatrix</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.cv" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">cv</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dtrain</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">nfold</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stratified</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">folds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fpreproc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">as_pandas</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose_eval</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show_stdv</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Cross-validation with given parameters.</p>
<h6 id="xgboostlss.model.XGBoostLSS.cv--arguments">Arguments</h6>
<p>params : dict
    Booster params.
dtrain : DMatrix
    Data to be trained.
num_boost_round : int
    Number of boosting iterations.
nfold : int
    Number of folds in CV.
stratified : bool
    Perform stratified sampling.
folds : a KFold or StratifiedKFold instance or list of fold indices
    Sklearn KFolds or StratifiedKFolds object.
    Alternatively may explicitly pass sample indices for each fold.
    For <code>n</code> folds, <strong>folds</strong> should be a length <code>n</code> list of tuples.
    Each tuple is <code>(in,out)</code> where <code>in</code> is a list of indices to be used
    as the training samples for the <code>n</code> th fold and <code>out</code> is a list of
    indices to be used as the testing samples for the <code>n</code> th fold.
early_stopping_rounds: int
    Activates early stopping. Cross-Validation metric (average of validation
    metric computed over CV folds) needs to improve at least once in
    every <strong>early_stopping_rounds</strong> round(s) to continue training.
    The last entry in the evaluation history will represent the best iteration.
    If there's more than one metric in the <strong>eval_metric</strong> parameter given in
    <strong>params</strong>, the last metric will be used for early stopping.
fpreproc : function
    Preprocessing function that takes (dtrain, dtest, param) and returns
    transformed versions of those.
as_pandas : bool, default True
    Return pd.DataFrame when pandas is installed.
    If False or pandas is not installed, return np.ndarray
verbose_eval : bool, int, or None, default None
    Whether to display the progress. If None, progress will be displayed
    when np.ndarray is returned. If True, progress will be displayed at
    boosting stage. If an integer is given, progress will be displayed
    at every given <code>verbose_eval</code> boosting stage.
show_stdv : bool, default True
    Whether to display the standard deviation in progress.
    Results are not affected, and always contains std.
seed : int
    Seed used to generate the folds (passed to numpy.random.seed).
callbacks :
    List of callback functions that are applied at end of each iteration.
    It is possible to use predefined callbacks by using
    :ref:<code>Callback API &lt;callback_api&gt;</code>.
    .. note::
       States in callback are not preserved during training, which means callback
       objects can not be reused for multiple training sessions without
       reinitialization or deepcopy.
    .. code-block:: python
        for params in parameters_grid:
            # be sure to (re)initialize the callbacks before each run
            callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]
            xgboost.train(params, Xy, callbacks=callbacks)
shuffle : bool
    Shuffle data before creating folds.</p>
<h6 id="xgboostlss.model.XGBoostLSS.cv--returns">Returns</h6>
<p>evaluation history : list(string)</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">cv</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">dtrain</span><span class="p">:</span> <span class="n">DMatrix</span><span class="p">,</span>
    <span class="n">num_boost_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">nfold</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">stratified</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">folds</span><span class="p">:</span> <span class="n">XGBStratifiedKFold</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">early_stopping_rounds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fpreproc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">FPreProcCallable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">as_pandas</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose_eval</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">show_stdv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">TrainingCallback</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># pylint: disable = invalid-name</span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cross-validation with given parameters.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ----------</span>
<span class="sd">    params : dict</span>
<span class="sd">        Booster params.</span>
<span class="sd">    dtrain : DMatrix</span>
<span class="sd">        Data to be trained.</span>
<span class="sd">    num_boost_round : int</span>
<span class="sd">        Number of boosting iterations.</span>
<span class="sd">    nfold : int</span>
<span class="sd">        Number of folds in CV.</span>
<span class="sd">    stratified : bool</span>
<span class="sd">        Perform stratified sampling.</span>
<span class="sd">    folds : a KFold or StratifiedKFold instance or list of fold indices</span>
<span class="sd">        Sklearn KFolds or StratifiedKFolds object.</span>
<span class="sd">        Alternatively may explicitly pass sample indices for each fold.</span>
<span class="sd">        For ``n`` folds, **folds** should be a length ``n`` list of tuples.</span>
<span class="sd">        Each tuple is ``(in,out)`` where ``in`` is a list of indices to be used</span>
<span class="sd">        as the training samples for the ``n`` th fold and ``out`` is a list of</span>
<span class="sd">        indices to be used as the testing samples for the ``n`` th fold.</span>
<span class="sd">    early_stopping_rounds: int</span>
<span class="sd">        Activates early stopping. Cross-Validation metric (average of validation</span>
<span class="sd">        metric computed over CV folds) needs to improve at least once in</span>
<span class="sd">        every **early_stopping_rounds** round(s) to continue training.</span>
<span class="sd">        The last entry in the evaluation history will represent the best iteration.</span>
<span class="sd">        If there&#39;s more than one metric in the **eval_metric** parameter given in</span>
<span class="sd">        **params**, the last metric will be used for early stopping.</span>
<span class="sd">    fpreproc : function</span>
<span class="sd">        Preprocessing function that takes (dtrain, dtest, param) and returns</span>
<span class="sd">        transformed versions of those.</span>
<span class="sd">    as_pandas : bool, default True</span>
<span class="sd">        Return pd.DataFrame when pandas is installed.</span>
<span class="sd">        If False or pandas is not installed, return np.ndarray</span>
<span class="sd">    verbose_eval : bool, int, or None, default None</span>
<span class="sd">        Whether to display the progress. If None, progress will be displayed</span>
<span class="sd">        when np.ndarray is returned. If True, progress will be displayed at</span>
<span class="sd">        boosting stage. If an integer is given, progress will be displayed</span>
<span class="sd">        at every given `verbose_eval` boosting stage.</span>
<span class="sd">    show_stdv : bool, default True</span>
<span class="sd">        Whether to display the standard deviation in progress.</span>
<span class="sd">        Results are not affected, and always contains std.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Seed used to generate the folds (passed to numpy.random.seed).</span>
<span class="sd">    callbacks :</span>
<span class="sd">        List of callback functions that are applied at end of each iteration.</span>
<span class="sd">        It is possible to use predefined callbacks by using</span>
<span class="sd">        :ref:`Callback API &lt;callback_api&gt;`.</span>
<span class="sd">        .. note::</span>
<span class="sd">           States in callback are not preserved during training, which means callback</span>
<span class="sd">           objects can not be reused for multiple training sessions without</span>
<span class="sd">           reinitialization or deepcopy.</span>
<span class="sd">        .. code-block:: python</span>
<span class="sd">            for params in parameters_grid:</span>
<span class="sd">                # be sure to (re)initialize the callbacks before each run</span>
<span class="sd">                callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]</span>
<span class="sd">                xgboost.train(params, Xy, callbacks=callbacks)</span>
<span class="sd">    shuffle : bool</span>
<span class="sd">        Shuffle data before creating folds.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    evaluation history : list(string)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_params_adj</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">adjust_labels</span><span class="p">(</span><span class="n">dtrain</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">dtrain</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">cv_booster</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
        <span class="n">params</span><span class="p">,</span>
        <span class="n">dtrain</span><span class="p">,</span>
        <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
        <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
        <span class="n">stratified</span><span class="o">=</span><span class="n">stratified</span><span class="p">,</span>
        <span class="n">folds</span><span class="o">=</span><span class="n">folds</span><span class="p">,</span>
        <span class="n">obj</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">objective_fn</span><span class="p">,</span>
        <span class="n">custom_metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">metric_fn</span><span class="p">,</span>
        <span class="n">maximize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="n">early_stopping_rounds</span><span class="p">,</span>
        <span class="n">fpreproc</span><span class="o">=</span><span class="n">fpreproc</span><span class="p">,</span>
        <span class="n">as_pandas</span><span class="o">=</span><span class="n">as_pandas</span><span class="p">,</span>
        <span class="n">verbose_eval</span><span class="o">=</span><span class="n">verbose_eval</span><span class="p">,</span>
        <span class="n">show_stdv</span><span class="o">=</span><span class="n">show_stdv</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv_booster</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.expectile_plot" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">expectile_plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">expectile</span><span class="o">=</span><span class="s1">&#39;0.05&#39;</span><span class="p">,</span> <span class="n">plot_type</span><span class="o">=</span><span class="s1">&#39;Partial_Dependence&#39;</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>XGBoostLSS function for plotting expectile SHapley values.</p>
<p>Requires <code>shap</code> to be installed.
Install via pip install <code>xgboostlss[all_extras]</code>.</p>
<h6 id="xgboostlss.model.XGBoostLSS.expectile_plot--arguments">Arguments</h6>
<p>X: pd.DataFrame
    Train/Test Data
feature: str
    Specifies which feature to use for plotting Partial_Dependence plot.
expectile: str
    Specifies which expectile to plot.
plot_type: str
    Specifies which SHapley-plot to visualize. Currently, "Partial_Dependence" and "Feature_Importance"
    are supported.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">expectile_plot</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">X</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
        <span class="n">feature</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span>
        <span class="n">expectile</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;0.05&quot;</span><span class="p">,</span>
        <span class="n">plot_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    XGBoostLSS function for plotting expectile SHapley values.</span>

<span class="sd">    Requires ``shap`` to be installed.</span>
<span class="sd">    Install via pip install ``xgboostlss[all_extras]``.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    X: pd.DataFrame</span>
<span class="sd">        Train/Test Data</span>
<span class="sd">    feature: str</span>
<span class="sd">        Specifies which feature to use for plotting Partial_Dependence plot.</span>
<span class="sd">    expectile: str</span>
<span class="sd">        Specifies which expectile to plot.</span>
<span class="sd">    plot_type: str</span>
<span class="sd">        Specifies which SHapley-plot to visualize. Currently, &quot;Partial_Dependence&quot; and &quot;Feature_Importance&quot;</span>
<span class="sd">        are supported.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;XGBoostLSS.expectile_plot requires &#39;shap&#39; &quot;</span>
        <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
        <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
        <span class="s2">&quot;the required dependencies.&quot;</span>
    <span class="p">)</span>
    <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;shap&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

    <span class="kn">import</span><span class="w"> </span><span class="nn">shap</span>

    <span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
    <span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">)</span>
    <span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">expect_pos</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">param_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">expectile</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">:</span>
        <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">expect_pos</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">expect_pos</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Feature_Importance&quot;</span><span class="p">:</span>
        <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">expect_pos</span><span class="p">],</span> <span class="n">max_display</span><span class="o">=</span><span class="mi">15</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">15</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.hyper_opt" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">hyper_opt</span><span class="p">(</span><span class="n">hp_dict</span><span class="p">,</span> <span class="n">dtrain</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_minutes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">study_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">silence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hp_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function to tune hyperparameters using optuna.</p>
<p>Requires optuna and optuna-integration to be installed.
Install via pip install <code>xgboostlss[all_extras]</code>.</p>
<h6 id="xgboostlss.model.XGBoostLSS.hyper_opt--arguments">Arguments</h6>
<p>hp_dict: dict
    Dictionary of hyperparameters to tune.
dtrain: xgb.DMatrix
    Training data.
num_boost_round: int
    Number of boosting iterations.
nfold: int
    Number of folds in CV.
early_stopping_rounds: int
    Activates early stopping. Cross-Validation metric (average of validation
    metric computed over CV folds) needs to improve at least once in
    every <strong>early_stopping_rounds</strong> round(s) to continue training.
    The last entry in the evaluation history will represent the best iteration.
    If there's more than one metric in the <strong>eval_metric</strong> parameter given in
    <strong>params</strong>, the last metric will be used for early stopping.
max_minutes: int
    Time budget in minutes, i.e., stop study after the given number of minutes.
n_trials: int
    The number of trials. If this argument is set to None, there is no limitation on the number of trials.
study_name: str
    Name of the hyperparameter study.
silence: bool
    Controls the verbosity of the trail, i.e., user can silence the outputs of the trail.
seed: int
    Seed used to generate the folds (passed to numpy.random.seed).
hp_seed: int
    Seed for random number generator used in the Bayesian hyper-parameter search.</p>
<h6 id="xgboostlss.model.XGBoostLSS.hyper_opt--returns">Returns</h6>
<p>opt_params : dict
    Optimal hyper-parameters.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">hyper_opt</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hp_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="n">dtrain</span><span class="p">:</span> <span class="n">DMatrix</span><span class="p">,</span>
    <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">max_minutes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">n_trials</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">study_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">silence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">hp_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function to tune hyperparameters using optuna.</span>

<span class="sd">    Requires optuna and optuna-integration to be installed.</span>
<span class="sd">    Install via pip install ``xgboostlss[all_extras]``.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ----------</span>
<span class="sd">    hp_dict: dict</span>
<span class="sd">        Dictionary of hyperparameters to tune.</span>
<span class="sd">    dtrain: xgb.DMatrix</span>
<span class="sd">        Training data.</span>
<span class="sd">    num_boost_round: int</span>
<span class="sd">        Number of boosting iterations.</span>
<span class="sd">    nfold: int</span>
<span class="sd">        Number of folds in CV.</span>
<span class="sd">    early_stopping_rounds: int</span>
<span class="sd">        Activates early stopping. Cross-Validation metric (average of validation</span>
<span class="sd">        metric computed over CV folds) needs to improve at least once in</span>
<span class="sd">        every **early_stopping_rounds** round(s) to continue training.</span>
<span class="sd">        The last entry in the evaluation history will represent the best iteration.</span>
<span class="sd">        If there&#39;s more than one metric in the **eval_metric** parameter given in</span>
<span class="sd">        **params**, the last metric will be used for early stopping.</span>
<span class="sd">    max_minutes: int</span>
<span class="sd">        Time budget in minutes, i.e., stop study after the given number of minutes.</span>
<span class="sd">    n_trials: int</span>
<span class="sd">        The number of trials. If this argument is set to None, there is no limitation on the number of trials.</span>
<span class="sd">    study_name: str</span>
<span class="sd">        Name of the hyperparameter study.</span>
<span class="sd">    silence: bool</span>
<span class="sd">        Controls the verbosity of the trail, i.e., user can silence the outputs of the trail.</span>
<span class="sd">    seed: int</span>
<span class="sd">        Seed used to generate the folds (passed to numpy.random.seed).</span>
<span class="sd">    hp_seed: int</span>
<span class="sd">        Seed for random number generator used in the Bayesian hyper-parameter search.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    opt_params : dict</span>
<span class="sd">        Optimal hyper-parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;XGBoostLSS.hyper_opt requires &#39;optuna&#39; and &#39;optuna-integration&#39; &quot;</span>
        <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
        <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
        <span class="s2">&quot;the required dependencies.&quot;</span>
    <span class="p">)</span>
    <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;optuna&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

    <span class="kn">import</span><span class="w"> </span><span class="nn">optuna</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">optuna.samplers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TPESampler</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>

        <span class="n">hyper_params</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param_value</span> <span class="ow">in</span> <span class="n">hp_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>

            <span class="n">param_type</span> <span class="o">=</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;categorical&quot;</span> <span class="ow">or</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
                <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="p">{</span><span class="n">param_name</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">1</span><span class="p">])}</span>
                <span class="p">)</span>

            <span class="k">elif</span> <span class="n">param_type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;float&quot;</span><span class="p">,</span> <span class="s2">&quot;int&quot;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;float&quot;</span><span class="p">:</span>
                    <span class="n">suggester</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span>
                <span class="k">else</span><span class="p">:</span>  <span class="c1"># param_type == &quot;int&quot;</span>
                    <span class="n">suggester</span> <span class="o">=</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span>

                <span class="n">param_constraints</span> <span class="o">=</span> <span class="n">param_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">param_low</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;low&quot;</span><span class="p">]</span>
                <span class="n">param_high</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;high&quot;</span><span class="p">]</span>
                <span class="n">param_log</span> <span class="o">=</span> <span class="n">param_constraints</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span>
                <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                    <span class="p">{</span>
                        <span class="n">param_name</span><span class="p">:</span> <span class="n">suggester</span><span class="p">(</span>
                            <span class="n">param_name</span><span class="p">,</span>
                            <span class="n">low</span><span class="o">=</span><span class="n">param_low</span><span class="p">,</span>
                            <span class="n">high</span><span class="o">=</span><span class="n">param_high</span><span class="p">,</span>
                            <span class="n">log</span><span class="o">=</span><span class="n">param_log</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">}</span>
                <span class="p">)</span>

        <span class="c1"># Add booster if not included in dictionary</span>
        <span class="k">if</span> <span class="s2">&quot;booster&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">hyper_params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">hyper_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span><span class="s2">&quot;booster&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s2">&quot;booster&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;gbtree&quot;</span><span class="p">])}</span>
            <span class="p">)</span>

        <span class="c1"># Add pruning</span>
        <span class="n">pruning_callback</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">integration</span><span class="o">.</span><span class="n">XGBoostPruningCallback</span><span class="p">(</span>
            <span class="n">trial</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;test-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="n">xgblss_param_tuning</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cv</span><span class="p">(</span>
            <span class="n">params</span><span class="o">=</span><span class="n">hyper_params</span><span class="p">,</span>
            <span class="n">dtrain</span><span class="o">=</span><span class="n">dtrain</span><span class="p">,</span>
            <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
            <span class="n">nfold</span><span class="o">=</span><span class="n">nfold</span><span class="p">,</span>
            <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="n">early_stopping_rounds</span><span class="p">,</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">pruning_callback</span><span class="p">],</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
            <span class="n">verbose_eval</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

        <span class="c1"># Add the optimal number of rounds</span>
        <span class="n">opt_rounds</span> <span class="o">=</span> <span class="n">xgblss_param_tuning</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;test-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="si">}</span><span class="s2">-mean&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">trial</span><span class="o">.</span><span class="n">set_user_attr</span><span class="p">(</span><span class="s2">&quot;opt_round&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">opt_rounds</span><span class="p">))</span>

        <span class="c1"># Extract the best score</span>
        <span class="n">best_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">xgblss_param_tuning</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;test-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">loss_fn</span><span class="si">}</span><span class="s2">-mean&quot;</span><span class="p">])</span>
        <span class="c1"># Replace -inf with 1e8 (to avoid -inf in the log)</span>
        <span class="n">best_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">best_score</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="mf">1e8</span><span class="p">),</span> <span class="n">best_score</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">best_score</span>

    <span class="k">if</span> <span class="n">study_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">study_name</span> <span class="o">=</span> <span class="s2">&quot;XGBoostLSS Hyper-Parameter Optimization&quot;</span>

    <span class="k">if</span> <span class="n">silence</span><span class="p">:</span>
        <span class="n">optuna</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">optuna</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">hp_seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">TPESampler</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">hp_seed</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">TPESampler</span><span class="p">()</span>

    <span class="n">pruner</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">pruners</span><span class="o">.</span><span class="n">MedianPruner</span><span class="p">(</span><span class="n">n_startup_trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_warmup_steps</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span> <span class="n">pruner</span><span class="o">=</span><span class="n">pruner</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;minimize&quot;</span><span class="p">,</span> <span class="n">study_name</span><span class="o">=</span><span class="n">study_name</span><span class="p">)</span>
    <span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="n">n_trials</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span> <span class="o">*</span> <span class="n">max_minutes</span><span class="p">,</span> <span class="n">show_progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Hyper-Parameter Optimization successfully finished.&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Number of finished trials: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">study</span><span class="o">.</span><span class="n">trials</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Best trial:&quot;</span><span class="p">)</span>
    <span class="n">opt_param</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span>

    <span class="c1"># Add optimal stopping round</span>
    <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">trials_dataframe</span><span class="p">()[</span><span class="s2">&quot;user_attrs_opt_round&quot;</span><span class="p">][</span>
        <span class="n">study</span><span class="o">.</span><span class="n">trials_dataframe</span><span class="p">()[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>
    <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;opt_rounds&quot;</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">opt_param</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    Params: &quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;    </span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">opt_param</span><span class="o">.</span><span class="n">params</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.load_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>Load the model from a file.</p>
<h6 id="xgboostlss.model.XGBoostLSS.load_model--parameters">Parameters</h6>
<p>model_path : str
    The path to the saved model.</p>
<h6 id="xgboostlss.model.XGBoostLSS.load_model--returns">Returns</h6>
<p>The loaded model.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">load_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load the model from a file.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model_path : str</span>
<span class="sd">        The path to the saved model.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    The loaded model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.plot" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">parameter</span><span class="o">=</span><span class="s1">&#39;loc&#39;</span><span class="p">,</span> <span class="n">max_display</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">plot_type</span><span class="o">=</span><span class="s1">&#39;Partial_Dependence&#39;</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>XGBoostLSS SHap plotting function.</p>
<p>Requires <code>shap</code> to be installed.
Install via pip install <code>xgboostlss[all_extras]</code>.</p>
<h6 id="xgboostlss.model.XGBoostLSS.plot--arguments">Arguments</h6>
<p>X: pd.DataFrame
    Train/Test Data
feature: str
    Specifies which feature is to be plotted.
parameter: str
    Specifies which distributional parameter is to be plotted.
max_display: int
    Specifies the maximum number of features to be displayed.
plot_type: str
    Specifies the type of plot:
        "Partial_Dependence" plots the partial dependence of the parameter on the feature.
        "Feature_Importance" plots the feature importance of the parameter.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
         <span class="n">X</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
         <span class="n">feature</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span>
         <span class="n">parameter</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;loc&quot;</span><span class="p">,</span>
         <span class="n">max_display</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
         <span class="n">plot_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">,</span>
        <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    XGBoostLSS SHap plotting function.</span>

<span class="sd">    Requires ``shap`` to be installed.</span>
<span class="sd">    Install via pip install ``xgboostlss[all_extras]``.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    X: pd.DataFrame</span>
<span class="sd">        Train/Test Data</span>
<span class="sd">    feature: str</span>
<span class="sd">        Specifies which feature is to be plotted.</span>
<span class="sd">    parameter: str</span>
<span class="sd">        Specifies which distributional parameter is to be plotted.</span>
<span class="sd">    max_display: int</span>
<span class="sd">        Specifies the maximum number of features to be displayed.</span>
<span class="sd">    plot_type: str</span>
<span class="sd">        Specifies the type of plot:</span>
<span class="sd">            &quot;Partial_Dependence&quot; plots the partial dependence of the parameter on the feature.</span>
<span class="sd">            &quot;Feature_Importance&quot; plots the feature importance of the parameter.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">skbase.utils.dependencies</span><span class="w"> </span><span class="kn">import</span> <span class="n">_check_soft_dependencies</span>

    <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;XGBoostLSS.plot requires &#39;shap&#39; &quot;</span>
        <span class="s2">&quot;to be installed. Please install the package to use this feature. &quot;</span>
        <span class="s2">&quot;Installing via pip install xgboostlss[all_extras] also installs &quot;</span>
        <span class="s2">&quot;the required dependencies.&quot;</span>
    <span class="p">)</span>
    <span class="n">_check_soft_dependencies</span><span class="p">([</span><span class="s2">&quot;shap&quot;</span><span class="p">],</span> <span class="n">msg</span><span class="o">=</span><span class="n">msg</span><span class="p">)</span>

    <span class="kn">import</span><span class="w"> </span><span class="nn">shap</span>

    <span class="n">shap</span><span class="o">.</span><span class="n">initjs</span><span class="p">()</span>
    <span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">)</span>
    <span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="n">param_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">distribution_arg_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">parameter</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Partial_Dependence&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">param_pos</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">shap_values</span><span class="p">[:,</span> <span class="n">feature</span><span class="p">][:,</span> <span class="n">param_pos</span><span class="p">])</span>
    <span class="k">elif</span> <span class="n">plot_type</span> <span class="o">==</span> <span class="s2">&quot;Feature_Importance&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">max_display</span><span class="o">=</span><span class="n">max_display</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_display</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span>
                <span class="n">shap_values</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">param_pos</span><span class="p">],</span> <span class="n">max_display</span><span class="o">=</span><span class="n">max_display</span> <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_display</span> <span class="k">else</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.predict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pred_type</span><span class="o">=</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function that predicts from the trained model.</p>
<h6 id="xgboostlss.model.XGBoostLSS.predict--arguments">Arguments</h6>
<p>data : xgb.DMatrix
    Data to predict from.
pred_type : str
    Type of prediction:</p>
<pre><code>- "samples" draws n_samples from the predicted distribution.
- "quantiles" calculates the quantiles from the predicted distribution.
- "parameters" returns the predicted distributional parameters.
- "expectiles" returns the predicted expectiles.
</code></pre>


<details class="n_samples-" open>
  <summary>int</summary>
  <p>Number of samples to draw from the predicted distribution.</p>
</details>        <p>quantiles : List[float]
    List of quantiles to calculate from the predicted distribution.
seed : int
    Seed for random number generator used to draw samples
    from the predicted distribution.</p>
<h6 id="xgboostlss.model.XGBoostLSS.predict--returns">Returns</h6>
<p>predt_df : pd.DataFrame
    Predictions.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">,</span>
    <span class="n">pred_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;parameters&quot;</span><span class="p">,</span>
    <span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">quantiles</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="mi">123</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that predicts from the trained model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    data : xgb.DMatrix</span>
<span class="sd">        Data to predict from.</span>
<span class="sd">    pred_type : str</span>
<span class="sd">        Type of prediction:</span>

<span class="sd">        - &quot;samples&quot; draws n_samples from the predicted distribution.</span>
<span class="sd">        - &quot;quantiles&quot; calculates the quantiles from the predicted distribution.</span>
<span class="sd">        - &quot;parameters&quot; returns the predicted distributional parameters.</span>
<span class="sd">        - &quot;expectiles&quot; returns the predicted expectiles.</span>

<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples to draw from the predicted distribution.</span>
<span class="sd">    quantiles : List[float]</span>
<span class="sd">        List of quantiles to calculate from the predicted distribution.</span>
<span class="sd">    seed : int</span>
<span class="sd">        Seed for random number generator used to draw samples</span>
<span class="sd">        from the predicted distribution.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt_df : pd.DataFrame</span>
<span class="sd">        Predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Predict</span>
    <span class="n">predt_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">predict_dist</span><span class="p">(</span>
        <span class="n">booster</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">booster</span><span class="p">,</span>
        <span class="n">start_values</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">start_values</span><span class="p">,</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
        <span class="n">pred_type</span><span class="o">=</span><span class="n">pred_type</span><span class="p">,</span>
        <span class="n">n_samples</span><span class="o">=</span><span class="n">n_samples</span><span class="p">,</span>
        <span class="n">quantiles</span><span class="o">=</span><span class="n">quantiles</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt_df</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.save_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">save_model</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Save the model to a file.</p>
<h6 id="xgboostlss.model.XGBoostLSS.save_model--parameters">Parameters</h6>
<p>model_path : str
    The path to save the model.</p>
<h6 id="xgboostlss.model.XGBoostLSS.save_model--returns">Returns</h6>
<p>None</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Save the model to a file.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model_path : str</span>
<span class="sd">        The path to save the model.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.set_base_margin" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">set_base_margin</span><span class="p">(</span><span class="n">dmatrix</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Set base margin for distributions.</p>
<h6 id="xgboostlss.model.XGBoostLSS.set_base_margin--arguments">Arguments</h6>
<p>dmatrix : DMatrix
    DMatrix object.</p>
<h6 id="xgboostlss.model.XGBoostLSS.set_base_margin--returns">Returns</h6>
<p>None</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">set_base_margin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dmatrix</span><span class="p">:</span> <span class="n">DMatrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set base margin for distributions.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    dmatrix : DMatrix</span>
<span class="sd">        DMatrix object.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">initialize</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">calculate_start_values</span><span class="p">(</span><span class="n">dmatrix</span><span class="o">.</span><span class="n">get_label</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span>
    <span class="n">base_margin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">dmatrix</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span>
    <span class="n">dmatrix</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.set_eval_margin" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">set_eval_margin</span><span class="p">(</span><span class="n">eval_set</span><span class="p">,</span> <span class="n">start_values</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Function that sets the base margin for the evaluation set.</p>
<h6 id="xgboostlss.model.XGBoostLSS.set_eval_margin--arguments">Arguments</h6>
<p>eval_set : list
    List of tuples containing the train and evaluation set.
start_values : np.ndarray
    Array containing the start values for each distributional parameter.</p>
<h6 id="xgboostlss.model.XGBoostLSS.set_eval_margin--returns">Returns</h6>
<p>eval_set : list
    List of tuples containing the train and evaluation set.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">set_eval_margin</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">eval_set</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
    <span class="n">start_values</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function that sets the base margin for the evaluation set.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    eval_set : list</span>
<span class="sd">        List of tuples containing the train and evaluation set.</span>
<span class="sd">    start_values : np.ndarray</span>
<span class="sd">        Array containing the start values for each distributional parameter.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    eval_set : list</span>
<span class="sd">        List of tuples containing the train and evaluation set.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sets</span> <span class="o">=</span> <span class="p">[(</span><span class="n">item</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">eval_set</span><span class="p">]</span>

    <span class="n">eval_set1</span><span class="p">,</span> <span class="n">label1</span> <span class="o">=</span> <span class="n">sets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">eval_set2</span><span class="p">,</span> <span class="n">label2</span> <span class="o">=</span> <span class="n">sets</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Adjust labels to number of distributional parameters</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">univariate</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">multivariate_eval_label_expand</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multivariate_eval_label_expand</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">eval_set2_label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">target_append</span><span class="p">(</span><span class="n">eval_set2</span><span class="o">.</span><span class="n">get_label</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_targets</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">)</span>
        <span class="n">eval_set2</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="n">eval_set2_label</span><span class="p">)</span>

    <span class="c1"># Set base margins</span>
    <span class="n">base_margin_set1</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">eval_set1</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
    <span class="n">eval_set1</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_set1</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
    <span class="n">base_margin_set2</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">eval_set2</span><span class="o">.</span><span class="n">num_row</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)))</span> <span class="o">*</span> <span class="n">start_values</span>
    <span class="n">eval_set2</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">base_margin_set2</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

    <span class="n">eval_set</span> <span class="o">=</span> <span class="p">[(</span><span class="n">eval_set1</span><span class="p">,</span> <span class="n">label1</span><span class="p">),</span> <span class="p">(</span><span class="n">eval_set2</span><span class="p">,</span> <span class="n">label2</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">eval_set</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.set_params_adj" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">set_params_adj</span><span class="p">(</span><span class="n">params</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Set parameters for distributional model.</p>
<h6 id="xgboostlss.model.XGBoostLSS.set_params_adj--arguments">Arguments</h6>
<p>params : Dict[str, Any]
    Parameters for model.</p>
<h6 id="xgboostlss.model.XGBoostLSS.set_params_adj--returns">Returns</h6>
<p>params : Dict[str, Any]
    Updated Parameters for model.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">set_params_adj</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set parameters for distributional model.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    params : Dict[str, Any]</span>
<span class="sd">        Parameters for model.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params : Dict[str, Any]</span>
<span class="sd">        Updated Parameters for model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params_adj</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;objective&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;base_score&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;num_target&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">n_dist_param</span><span class="p">,</span>
        <span class="s2">&quot;disable_default_eval_metric&quot;</span><span class="p">:</span> <span class="kc">True</span>
    <span class="p">}</span>
    <span class="n">params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params_adj</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">params</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="xgboostlss.model.XGBoostLSS.train" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dtrain</span><span class="p">,</span> <span class="n">num_boost_round</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">evals</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">evals_result</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose_eval</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">xgb_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Train a booster with given parameters.</p>
<h6 id="xgboostlss.model.XGBoostLSS.train--arguments">Arguments</h6>
<p>params :
    Booster params.
dtrain :
    Data to be trained.
num_boost_round :
    Number of boosting iterations.
evals :
    List of validation sets for which metrics will evaluated during training.
    Validation metrics will help us track the performance of the model.
early_stopping_rounds :
    Activates early stopping. Validation metric needs to improve at least once in
    every <strong>early_stopping_rounds</strong> round(s) to continue training.
    Requires at least one item in <strong>evals</strong>.
    The method returns the model from the last iteration (not the best one).  Use
    custom callback or model slicing if the best model is desired.
    If there's more than one item in <strong>evals</strong>, the last entry will be used for early
    stopping.
    If there's more than one metric in the <strong>eval_metric</strong> parameter given in
    <strong>params</strong>, the last metric will be used for early stopping.
    If early stopping occurs, the model will have two additional fields:
    <code>bst.best_score</code>, <code>bst.best_iteration</code>.
evals_result :
    This dictionary stores the evaluation results of all the items in watchlist.
    Example: with a watchlist containing
    <code>[(dtest,'eval'), (dtrain,'train')]</code> and
    a parameter containing <code>('eval_metric': 'logloss')</code>,
    the <strong>evals_result</strong> returns
    .. code-block:: python
        {'train': {'logloss': ['0.48253', '0.35953']},
         'eval': {'logloss': ['0.480385', '0.357756']}}
verbose_eval :
    Requires at least one item in <strong>evals</strong>.
    If <strong>verbose_eval</strong> is True then the evaluation metric on the validation set is
    printed at each boosting stage.
    If <strong>verbose_eval</strong> is an integer then the evaluation metric on the validation set
    is printed at every given <strong>verbose_eval</strong> boosting stage. The last boosting stage
    / the boosting stage found by using <strong>early_stopping_rounds</strong> is also printed.
    Example: with <code>verbose_eval=4</code> and at least one item in <strong>evals</strong>, an evaluation metric
    is printed every 4 boosting stages, instead of every boosting stage.
xgb_model :
    Xgb model to be loaded before training (allows training continuation).
callbacks :
    List of callback functions that are applied at end of each iteration.
    It is possible to use predefined callbacks by using
    :ref:<code>Callback API &lt;callback_api&gt;</code>.
    .. note::
       States in callback are not preserved during training, which means callback
       objects can not be reused for multiple training sessions without
       reinitialization or deepcopy.
    .. code-block:: python
        for params in parameters_grid:
            # be sure to (re)initialize the callbacks before each run
            callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]
            xgboost.train(params, Xy, callbacks=callbacks)</p>
<h6 id="xgboostlss.model.XGBoostLSS.train--returns">Returns</h6>
<p>Booster:
    The trained booster model.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">dtrain</span><span class="p">:</span> <span class="n">DMatrix</span><span class="p">,</span>
    <span class="n">num_boost_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">evals</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">DMatrix</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">early_stopping_rounds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">evals_result</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TrainingCallback</span><span class="o">.</span><span class="n">EvalsLog</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose_eval</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">xgb_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">,</span> <span class="n">Booster</span><span class="p">,</span> <span class="nb">bytearray</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">TrainingCallback</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Booster</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Train a booster with given parameters.</span>

<span class="sd">        Arguments</span>
<span class="sd">        ---------</span>
<span class="sd">        params :</span>
<span class="sd">            Booster params.</span>
<span class="sd">        dtrain :</span>
<span class="sd">            Data to be trained.</span>
<span class="sd">        num_boost_round :</span>
<span class="sd">            Number of boosting iterations.</span>
<span class="sd">        evals :</span>
<span class="sd">            List of validation sets for which metrics will evaluated during training.</span>
<span class="sd">            Validation metrics will help us track the performance of the model.</span>
<span class="sd">        early_stopping_rounds :</span>
<span class="sd">            Activates early stopping. Validation metric needs to improve at least once in</span>
<span class="sd">            every **early_stopping_rounds** round(s) to continue training.</span>
<span class="sd">            Requires at least one item in **evals**.</span>
<span class="sd">            The method returns the model from the last iteration (not the best one).  Use</span>
<span class="sd">            custom callback or model slicing if the best model is desired.</span>
<span class="sd">            If there&#39;s more than one item in **evals**, the last entry will be used for early</span>
<span class="sd">            stopping.</span>
<span class="sd">            If there&#39;s more than one metric in the **eval_metric** parameter given in</span>
<span class="sd">            **params**, the last metric will be used for early stopping.</span>
<span class="sd">            If early stopping occurs, the model will have two additional fields:</span>
<span class="sd">            ``bst.best_score``, ``bst.best_iteration``.</span>
<span class="sd">        evals_result :</span>
<span class="sd">            This dictionary stores the evaluation results of all the items in watchlist.</span>
<span class="sd">            Example: with a watchlist containing</span>
<span class="sd">            ``[(dtest,&#39;eval&#39;), (dtrain,&#39;train&#39;)]`` and</span>
<span class="sd">            a parameter containing ``(&#39;eval_metric&#39;: &#39;logloss&#39;)``,</span>
<span class="sd">            the **evals_result** returns</span>
<span class="sd">            .. code-block:: python</span>
<span class="sd">                {&#39;train&#39;: {&#39;logloss&#39;: [&#39;0.48253&#39;, &#39;0.35953&#39;]},</span>
<span class="sd">                 &#39;eval&#39;: {&#39;logloss&#39;: [&#39;0.480385&#39;, &#39;0.357756&#39;]}}</span>
<span class="sd">        verbose_eval :</span>
<span class="sd">            Requires at least one item in **evals**.</span>
<span class="sd">            If **verbose_eval** is True then the evaluation metric on the validation set is</span>
<span class="sd">            printed at each boosting stage.</span>
<span class="sd">            If **verbose_eval** is an integer then the evaluation metric on the validation set</span>
<span class="sd">            is printed at every given **verbose_eval** boosting stage. The last boosting stage</span>
<span class="sd">            / the boosting stage found by using **early_stopping_rounds** is also printed.</span>
<span class="sd">            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric</span>
<span class="sd">            is printed every 4 boosting stages, instead of every boosting stage.</span>
<span class="sd">        xgb_model :</span>
<span class="sd">            Xgb model to be loaded before training (allows training continuation).</span>
<span class="sd">        callbacks :</span>
<span class="sd">            List of callback functions that are applied at end of each iteration.</span>
<span class="sd">            It is possible to use predefined callbacks by using</span>
<span class="sd">            :ref:`Callback API &lt;callback_api&gt;`.</span>
<span class="sd">            .. note::</span>
<span class="sd">               States in callback are not preserved during training, which means callback</span>
<span class="sd">               objects can not be reused for multiple training sessions without</span>
<span class="sd">               reinitialization or deepcopy.</span>
<span class="sd">            .. code-block:: python</span>
<span class="sd">                for params in parameters_grid:</span>
<span class="sd">                    # be sure to (re)initialize the callbacks before each run</span>
<span class="sd">                    callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]</span>
<span class="sd">                    xgboost.train(params, Xy, callbacks=callbacks)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Booster:</span>
<span class="sd">            The trained booster model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_params_adj</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adjust_labels</span><span class="p">(</span><span class="n">dtrain</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_base_margin</span><span class="p">(</span><span class="n">dtrain</span><span class="p">)</span>

        <span class="c1"># Set base_margin for evals</span>
        <span class="k">if</span> <span class="n">evals</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">evals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_eval_margin</span><span class="p">(</span><span class="n">evals</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_values</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">booster</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
            <span class="n">params</span><span class="p">,</span>
            <span class="n">dtrain</span><span class="p">,</span>
            <span class="n">num_boost_round</span><span class="o">=</span><span class="n">num_boost_round</span><span class="p">,</span>
            <span class="n">evals</span><span class="o">=</span><span class="n">evals</span><span class="p">,</span>
            <span class="n">obj</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">objective_fn</span><span class="p">,</span>
            <span class="n">custom_metric</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">metric_fn</span><span class="p">,</span>
            <span class="n">xgb_model</span><span class="o">=</span><span class="n">xgb_model</span><span class="p">,</span>
            <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
            <span class="n">verbose_eval</span><span class="o">=</span><span class="n">verbose_eval</span><span class="p">,</span>
            <span class="n">evals_result</span><span class="o">=</span><span class="n">evals_result</span><span class="p">,</span>
            <span class="n">maximize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="n">early_stopping_rounds</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h5 id="xgboostlss.model.exp_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.exp_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h5 id="xgboostlss.model.exp_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.exp_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h5 id="xgboostlss.model.gumbel_softmax_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h5 id="xgboostlss.model.gumbel_softmax_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h5 id="xgboostlss.model.identity_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.identity_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h5 id="xgboostlss.model.nan_to_num--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.nan_to_num--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h5 id="xgboostlss.model.relu_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.relu_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h5 id="xgboostlss.model.relu_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.relu_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h5 id="xgboostlss.model.sigmoid_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.sigmoid_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h5 id="xgboostlss.model.softmax_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.softmax_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h5 id="xgboostlss.model.softplus_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.softplus_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h5 id="xgboostlss.model.softplus_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.softplus_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h5 id="xgboostlss.model.squareplus_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.squareplus_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.model.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h5 id="xgboostlss.model.squareplus_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.model.squareplus_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="xgboostlss.utils" class="doc doc-heading">
            <code>utils</code>


</h2>

    <div class="doc doc-contents ">










<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.exp_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Exponential function used to ensure predt is strictly positive.</p>
<h5 id="xgboostlss.utils.exp_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.exp_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.exp_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Exponential function used for Student-T distribution.</p>
<h5 id="xgboostlss.utils.exp_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.exp_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">exp_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.gumbel_softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Gumbel-softmax function used to ensure predt is adding to one.</p>
<p>The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a "soft"
version of a categorical distribution. Its a way to draw samples from a categorical distribution in a
differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of
categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a
Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.
Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is
defined as:</p>
<pre><code>s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}
</code></pre>
<p>where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness
of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau
approaches infty, the mixing probabilities become more uniform. For more information we refer to</p>
<pre><code>Jang, E., Gu, Shixiang and Poole, B. "Categorical Reparameterization with Gumbel-Softmax", ICLR, 2017.
</code></pre>
<h5 id="xgboostlss.utils.gumbel_softmax_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.
tau: float, non-negative scalar temperature.
    Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as
    tau -&gt; inf, the output becomes more uniform.</p>
<h5 id="xgboostlss.utils.gumbel_softmax_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">gumbel_softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span>
                      <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
                      <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gumbel-softmax function used to ensure predt is adding to one.</span>

<span class="sd">    The Gumbel-softmax distribution is a continuous distribution over the simplex, which can be thought of as a &quot;soft&quot;</span>
<span class="sd">    version of a categorical distribution. Its a way to draw samples from a categorical distribution in a</span>
<span class="sd">    differentiable way. The motivation behind using the Gumbel-Softmax is to make the discrete sampling process of</span>
<span class="sd">    categorical variables differentiable, which is useful in gradient-based optimization problems. To sample from a</span>
<span class="sd">    Gumbel-Softmax distribution, one would use the Gumbel-max trick: add a Gumbel noise to logits and apply the softmax.</span>
<span class="sd">    Formally, given a vector z, the Gumbel-softmax function s(z,tau)_i for a component i at temperature tau is</span>
<span class="sd">    defined as:</span>

<span class="sd">        s(z,tau)_i = frac{e^{(z_i + g_i) / tau}}{sum_{j=1}^M e^{(z_j + g_j) / tau}}</span>

<span class="sd">    where g_i is a sample from the Gumbel(0, 1) distribution. The parameter tau (temperature) controls the sharpness</span>
<span class="sd">    of the output distribution. As tau approaches 0, the mixing probabilities become more discrete, and as tau</span>
<span class="sd">    approaches infty, the mixing probabilities become more uniform. For more information we refer to</span>

<span class="sd">        Jang, E., Gu, Shixiang and Poole, B. &quot;Categorical Reparameterization with Gumbel-Softmax&quot;, ICLR, 2017.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    tau: float, non-negative scalar temperature.</span>
<span class="sd">        Temperature parameter for the Gumbel-softmax distribution. As tau -&gt; 0, the output becomes more discrete, and as</span>
<span class="sd">        tau -&gt; inf, the output becomes more uniform.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">gumbel_softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.identity_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Identity mapping of predt.</p>
<h5 id="xgboostlss.utils.identity_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.identity_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">identity_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Identity mapping of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.nan_to_num" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Replace nan, inf and -inf with the mean of predt.</p>
<h5 id="xgboostlss.utils.nan_to_num--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.nan_to_num--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace nan, inf and -inf with the mean of predt.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span>
                             <span class="n">nan</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">posinf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">)),</span>
                             <span class="n">neginf</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nanmean</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span>
                             <span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.relu_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h5 id="xgboostlss.utils.relu_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.relu_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.relu_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to max(0, predt).</p>
<h5 id="xgboostlss.utils.relu_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.relu_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">relu_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to max(0, predt).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.sigmoid_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Function used to ensure predt are scaled to (0,1).</p>
<h5 id="xgboostlss.utils.sigmoid_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.sigmoid_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Function used to ensure predt are scaled to (0,1).</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">predt</span><span class="p">,</span> <span class="mf">1e-03</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-03</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.softmax_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softmax function used to ensure predt is adding to one.</p>
<h5 id="xgboostlss.utils.softmax_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.softmax_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softmax_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax function used to ensure predt is adding to one.</span>


<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.softplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softplus function used to ensure predt is strictly positive.</p>
<h5 id="xgboostlss.utils.softplus_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.softplus_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.softplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Softplus function used for Student-T distribution.</p>
<h5 id="xgboostlss.utils.softplus_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.softplus_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">softplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus function used for Student-T distribution.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="n">softplus</span><span class="p">(</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">predt</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.squareplus_fn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h5 id="xgboostlss.utils.squareplus_fn--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.squareplus_fn--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="xgboostlss.utils.squareplus_fn_df" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Square-Plus function used to ensure predt is strictly positive.</p>
<h5 id="xgboostlss.utils.squareplus_fn_df--arguments">Arguments</h5>
<p>predt: torch.tensor
    Predicted values.</p>
<h5 id="xgboostlss.utils.squareplus_fn_df--returns">Returns</h5>
<p>predt: torch.tensor
    Predicted values.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>xgboostlss/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">squareplus_fn_df</span><span class="p">(</span><span class="n">predt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Square-Plus function used to ensure predt is strictly positive.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    predt: torch.tensor</span>
<span class="sd">        Predicted values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">4.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">predt</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">predt</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">predt</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">predt</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


  </div>

    </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../examples/ZAGamma_Regression/" class="btn btn-neutral float-left" title="Zero-Adjusted Gamma Regression"><span class="icon icon-circle-arrow-left"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/StatMixedML/XGBoostLSS" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../examples/ZAGamma_Regression/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../javascripts/mathjax.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
